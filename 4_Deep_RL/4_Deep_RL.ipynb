{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA 4, Interactive Learning, Fall 2024\n",
    "- **Name**: Majid Faridfar\n",
    "- **Student ID**: 810199569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Deep RL algorithms are either based on value learning or direct policy learning. Compare these two categories. In general, why is there a need for direct policy learning as long as value can be learned? Is there a third category that uses both approaches? If so, explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "One algorithm that uses the DQN idea is Dueling DQN. Research this algorithm and explain its main differences from DQN. It is important to explain how the advantage function works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "Examine the A2C algorithm. How is the idea of ​​advantage used in this algorithm? How is this method different from the Dueling DQN method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementation Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "Explore the Cart Pole v1 environment in the Gym library. Provide complete information about the Action Space, Observation Space, and Rewards of this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space:\n",
    "The action can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "0: Push cart to the left\n",
    "\n",
    "1: Push cart to the right\n",
    "\n",
    "The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it.\n",
    "\n",
    "Observation space:\n",
    "The observation is the values corresponding to the following positions and velocities:\n",
    "\n",
    "| Num | Observation | Min | Max |\n",
    "| 0 | Cart Position | -4.8 | 4.8 |\n",
    "\n",
    "1\n",
    "\n",
    "Cart Velocity\n",
    "\n",
    "-Inf\n",
    "\n",
    "Inf\n",
    "\n",
    "2\n",
    "\n",
    "Pole Angle\n",
    "\n",
    "~ -0.418 rad (-24°)\n",
    "\n",
    "~ 0.418 rad (24°)\n",
    "\n",
    "3\n",
    "\n",
    "Pole Angular Velocity\n",
    "\n",
    "-Inf\n",
    "\n",
    "Inf\n",
    "\n",
    "Note: While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "\n",
    "The cart x-position (index 0) can be take values between (-4.8, 4.8), but the episode terminates if the cart leaves the (-2.4, 2.4) range.\n",
    "\n",
    "The pole angle can be observed between (-.418, .418) radians (or ±24°), but the episode terminates if the pole angle is not in the range (-.2095, .2095) (or ±12°)\n",
    "\n",
    "Rewards:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "Why is it not good to use classical RL algorithms in this environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "MEMORY_SIZE = 10000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 500\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards),\n",
    "                np.array(next_states), np.array(dones))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, q_network, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_dim - 1)\n",
    "    else:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "def train():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    q_network = DQN(state_dim, action_dim)\n",
    "    target_network = DQN(state_dim, action_dim)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    target_network.eval()\n",
    "\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=LEARNING_RATE)\n",
    "    memory = ReplayBuffer(MEMORY_SIZE)\n",
    "    \n",
    "    epsilon = EPSILON_START\n",
    "    step_count = 0\n",
    "    \n",
    "    for episode in range(300):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(200):\n",
    "            action = epsilon_greedy_policy(state, epsilon, q_network, action_dim)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                states, actions, rewards, next_states, dones = memory.sample(BATCH_SIZE)\n",
    "                states = torch.FloatTensor(states)\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards = torch.FloatTensor(rewards)\n",
    "                next_states = torch.FloatTensor(next_states)\n",
    "                dones = torch.FloatTensor(dones)\n",
    "\n",
    "                q_values = q_network(states).gather(1, actions).squeeze()\n",
    "                next_q_values = target_network(next_states).max(1)[0]\n",
    "                target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, target_q_values.detach())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "            \n",
    "            step_count += 1\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        epsilon = max(EPSILON_END, EPSILON_START - episode / EPSILON_DECAY)\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Using Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
