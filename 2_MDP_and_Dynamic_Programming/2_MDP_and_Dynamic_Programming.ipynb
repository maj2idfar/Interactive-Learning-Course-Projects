{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA 2, Interactive Learning, Fall 2024\n",
    "- **Name**: Majid Faridfar\n",
    "- **Student ID**: 810199569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "What is the difference between reinforcement learning and supervised learning? Explain by providing two similar problems: one that requires reinforcement learning to solve, and another that can be solved with supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences:\n",
    "- How to learn? \n",
    "  - RL is based on an agent interacting with an environment. The agent takes actions, receives feedback in the form of rewards or penalties, and aims to maximize the cumulative reward over time. The feedback is sparse and often delayed, and there’s no explicit label or correct answer for each action.\n",
    "  - SL involves learning from labeled data where each input comes with a corresponding correct output (label). The model is trained to map inputs to outputs based on this labeled dataset.\n",
    "\n",
    "- What is the goal?\n",
    "  - The goal of RL is to learn a strategy (or policy) that will maximize the total reward over time. The agent learns through trial and error, exploring and exploiting the environment to improve its decisions.\n",
    "  - The goal of SL is to minimize the error between the predicted output and the true label, thereby learning a mapping function from inputs to outputs.\n",
    "\n",
    "Illustration (Maze):\n",
    "- A robot (agent) is placed in a maze (environment) and has to navigate from the start to the goal. The robot takes actions such as moving left, right, up, or down. Initially, it doesn’t know the best path to take and receives feedback in the form of rewards (e.g., +1 for reaching the goal, -1 for hitting a wall or making a wrong turn). The goal is to maximize the cumulative reward by learning which actions lead to the goal. The robot learns over time by exploring different paths, receiving feedback, and adjusting its strategy based on the rewards it receives.\n",
    "- Instead of having the robot learn by exploring the maze, you collect a dataset of maze configurations (states) and the correct sequence of moves (labels) that leads to the goal. For each configuration, you have a label indicating the correct next action (e.g., move left, move right, etc.). You train a model on this labeled dataset to predict the correct action for a given maze state. Once trained, the model can predict the correct move for any new maze configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "In an MDP problem, if the reward function undergoes a linear transformation, does the optimal policy change? (Provide a mathematical proof or a counterexample, and ignore the trivial case of multiplying by zero.) Does the answer to this question depend on whether the task is continuing or episodic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general linear transformation of a reward function $R(s, a)$ is:\n",
    "\n",
    "$$R'(s, a) = \\alpha R(s, a) + \\beta$$\n",
    "where $\\alpha > 0$ and $\\beta$ is a scalar.\n",
    "\n",
    "Here, we will show that when the reward function undergoes a linear transformation like this, the optimal policy does **not** change. This conclusion holds for both continuing and episodic tasks.\n",
    "\n",
    "### Understanding Optimal Policies:\n",
    "\n",
    "The goal in an MDP is to find an optimal policy $\\pi^*$ that maximizes the **expected cumulative reward**. Depending on whether the problem is episodic or continuing, this objective is either the total expected reward (episodic task) or the expected discounted cumulative reward (continuing task). We’ll focus on the discounted scenario, as the principles generalize well.\n",
    "\n",
    "In a discounted setting, the objective is to maximize the expected return,\n",
    "\n",
    "$$G_t = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) | s_t = s, a_t = a \\right]$$\n",
    "where $\\gamma$ is the discount factor such that $0 \\leq \\gamma \\leq 1$ (if $\\gamma$ is equal to $1$, then we are calculating the expected cumulative reward).\n",
    "\n",
    "We define the value function $V^\\pi(s)$ under a policy $\\pi$ as:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) \\middle| s_t = s \\right]$$\n",
    "\n",
    "Similarly, the action-value function or Q-value function $Q^\\pi(s, a)$ is:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) \\middle| s_t = s, a_t = a \\right]$$\n",
    "\n",
    "The optimal policy is the one that maximizes the value or Q-value function, satisfying:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n",
    "\n",
    "#### Effect on the Value Function:\n",
    "\n",
    "With the new transformed reward function $R'(s, a) = \\alpha R(s, a) + \\beta$, the new value function $V'^\\pi(s)$ under policy $\\pi$ becomes:\n",
    "\n",
    "$$V'^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R'(s_{t+k}, a_{t+k}) \\middle| s_t = s \\right]\n",
    "         = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k (\\alpha R(s_{t+k}, a_{t+k}) + \\beta) \\middle| s_t = s \\right]$$\n",
    "\n",
    "This expands into:\n",
    "\n",
    "$$V'^\\pi(s) = \\alpha \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) \\middle| s_t = s \\right]\n",
    "         + \\beta \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k \\middle| s_t = s \\right]$$\n",
    "\n",
    "Shortening the notation, we have:\n",
    "\n",
    "$$V'^\\pi(s) = \\alpha V^\\pi(s) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "where the sum $\\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}$ because it is a geometric series.\n",
    "\n",
    "#### Effect on the Q-Value Function:\n",
    "\n",
    "Similarly, the transformed Q-value function would be:\n",
    "\n",
    "$$Q'^\\pi(s, a) = \\alpha Q^\\pi(s, a) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "### Does the Optimal Policy Change?\n",
    "\n",
    "Answer is no. The optimal policy $\\pi^*(s)$ is determined by selecting the action $a$ that maximizes the Q-value function:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n",
    "\n",
    "When applying the linear transformation, the transformed Q-value function satisfies:\n",
    "\n",
    "$$Q'^\\pi(s, a) = \\alpha Q^\\pi(s, a) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "Since $\\alpha > 0$, this transformation preserves the order of the Q-values. That is, if action $a_1$ was better than action $a_2$ before the transformation (i.e., $Q^*(s, a_1) > Q^*(s, a_2)$), it will remain better after the transformation because the transformation is a positive linear scaling and a constant shift, both of which do not affect the relative ordering of numbers:\n",
    "\n",
    "$$Q'^\\pi(s, a_1) = \\alpha Q^\\pi(s, a_1) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "$$Q'^\\pi(s, a_2) = \\alpha Q^\\pi(s, a_2) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "Thus, $Q^\\pi(s, a_1) > Q^\\pi(s, a_2) \\Rightarrow Q'^\\pi(s, a_1) > Q'^\\pi(s, a_2)$.\n",
    "\n",
    "Since the relative rankings of the Q-values are preserved, the optimal action $\\arg\\max_a Q^*(s, a)$ does not change. Therefore, the optimal policy remains the same.\n",
    "\n",
    "The answer does not depend on whether the task is continuing or episodic. In an episodic task, you would also be maximizing the expected return, but limited to a certain number of steps before the episode ends. The reasoning around the transformation of the reward function and its impact on the Q-value functions still holds because the transformation is applied uniformly across the rewards, and the policy that maximizes value remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Assume a robot operates in a grid environment as follows. In each episode, the robot starts in one of the cells in the bottom row (with an equal probability of starting in each cell). The robot can move left, right, or up. If the robot chooses to move in a direction where there is a wall, it remains in place. If the robot enters one of the green cells, the episode ends.\n",
    "\n",
    "![pics/P3.png](pics/P3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1\n",
    "\n",
    "The robot knows the grid environment completely and is aware of its current location at any moment, using this information to make decisions. The goal is for the robot to reach the second row and enter one of the green cells. If the robot enters a green cell, it receives a reward of +1 (and the episode ends), and if it moves from one blue cell to another blue cell, it receives a reward of 0. For this task, $\\lambda = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "Can the defined task be represented by an MDP? If not, explain why; if yes, fully specify the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this task can be defined by an MDP. To fully specify the Markov Decision Process (MDP) for this task, we need to define **States (S)**, **Actions (A)**, **Transition probabilities (P(s'|s,a))**, **Rewards (R(s,a))**, and **Discount Factor (λ)**.\n",
    "\n",
    "##### States (S):\n",
    "The states are the cells in the grid:\n",
    "- S1, S2, S3, S4, S5, S6, S7 represent the blue cells.\n",
    "- T1, T3, and T5 are the green terminal cells (absorbing states).\n",
    "\n",
    "So the state space is:\n",
    "$$S = \\{S1, S2, S3, S4, S5, S6, S7, T1, T3, T5\\}$$\n",
    "\n",
    "##### Actions (A):\n",
    "The robot has three possible actions in the bottom row: **Up (U)**, **Left (L)**, **Right (R)**. However, when the robot is in the terminal green cells T1, T3, T5, the episode ends, and no further actions are available.\n",
    "\n",
    "Thus, the action set is:\n",
    "$$A = \\{\\text{Up}, \\text{Left}, \\text{Right}\\}$$\n",
    "\n",
    "##### Transition Probabilities (P(s' | s, a)):\n",
    "The robot transitions deterministically (we assume that there is no stochastic behavior, since it is not mentioned in the problem), so the probability of transitioning from one state to another depends on whether the robot encounters a wall.\n",
    "\n",
    "- Moving \"Left\" in state S1 will not change the state.\n",
    "- Moving \"Up\" in states S2, S4, S6 will move the robot into the green cells (and stop the episode), while in other states will not change the state.\n",
    "- Moving \"Right\" in state S7 will not change the state.\n",
    "\n",
    "Here are the key deterministic transitions:\n",
    "\n",
    "- **Up movement**:\n",
    "  - $P(T1 | S2, U) = 1$\n",
    "  - $P(T3 | S4, U) = 1$\n",
    "  - $P(T5 | S6, U) = 1$\n",
    "  - Other cases: The robot stays in the current state if \"Up\" is attempted from a cell without a green terminal cell above it.\n",
    "\n",
    "- **Left movement**:\n",
    "  - $P(S1 | S2, L) = 1$\n",
    "  - $P(S2 | S3, L) = 1$\n",
    "  - $P(S3 | S4, L) = 1$\n",
    "  - $P(S4 | S5, L) = 1$\n",
    "  - $P(S5 | S6, L) = 1$\n",
    "  - $P(S6 | S7, L) = 1$\n",
    "  - Other cases: The robot stays in the current state if \"Left\" is attempted from  S1.\n",
    "\n",
    "- **Right movement**:\n",
    "  - $P(S2 | S1, R) = 1$\n",
    "  - $P(S3 | S2, R) = 1$\n",
    "  - $P(S4 | S3, R) = 1$\n",
    "  - $P(S5 | S4, R) = 1$\n",
    "  - $P(S6 | S5, R) = 1$\n",
    "  - $P(S7 | S6, R) = 1$\n",
    "  - Other cases: The robot stays in the current state if \"Right\" is attempted from  S7.\n",
    "\n",
    "##### Rewards (R(s,a)):\n",
    "The robot receives:\n",
    "- A reward of **+1** upon entering a terminal green cell (T1, T3, T5).\n",
    "- A reward of **0** for any movement from one blue cell to another blue cell.\n",
    "\n",
    "Thus, the rewards are:\n",
    "- $R(s' | s, U) = 1$ if the robot moves from S2 to T1, S4 to T3, or S6 to T5.\n",
    "- $R(s' | s, a) = 0$ for any other movement.\n",
    "\n",
    "For example:\n",
    "- $R(T1 | S2, U) = 1$,\n",
    "- $R(T3 | S4, U) = 1$,\n",
    "- $R(T5 | S6, U) = 1$,\n",
    "- $R(S2 | S3, L) = 0$,\n",
    "- $R(S6 | S5, R) = 0$, etc.\n",
    "\n",
    "##### Discount Factor $λ$:\n",
    "The **discount factor** is $λ = 0.9$. This means that rewards received in future states are weighted by a factor of 0.9 for every step into the future. It reflects the agent's preference for immediate rewards over delayed ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "How many optimal deterministic policies exist for solving this task? Appropriately express $\\pi(s)$ for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the optimal policy, we go state by state and analyze the optimal actions:\n",
    "\n",
    "1. **S1**: The only possible optimal action here is to move **right**, since other actions are blocked by a wall. Thus, for $\\pi(S1)$:\n",
    "$$\\pi(S1) = \\text{R}$$\n",
    "- Hence, there’s only **one optimal action** in S1.\n",
    "\n",
    "2. **S2**: The optimal way to immediately end the episode and earn a reward is to move **Up** into the green terminal state T1. Thus:\n",
    "$$\\pi(S2) = \\text{U}$$\n",
    "- Hence, there’s only **one optimal action** in S2.\n",
    "\n",
    "3. **S3**: Moving **Up** is not blocked by the above wall. From S3, the robot can either move **left** to S2, letting it access the green cell T1, or move **right** to S4, potentially heading toward T3. Since rewards are the same (all terminal cells give +1 and actions cost 0), **both left and right are equally optimal** choices. Therefore:\n",
    "$$\\pi(S3) = \\text{L or R}$$\n",
    "- Hence, there are **two optimal actions** in S3.\n",
    "\n",
    "4. **S4**: The optimal action in state S4 is to move **Up** into T3 immediately to end the episode. Thus:\n",
    "$$\\pi(S4) = \\text{U}$$\n",
    "- Hence, there’s only **one optimal action** in S4.\n",
    "\n",
    "5. **S5**: Robot can move **right** toward S6 to reach the green terminal T5, or **left** to S4 and reach T3. Since both terminal cells T3 and T5 give the same reward, either direction is optimal. Therefore:\n",
    "$$\\pi(S5) = \\text{L or R}$$\n",
    "- Hence, there are **two optimal choices** in S5.\n",
    "\n",
    "6. **S6**: The optimal action is to go **Up** to T5 immediately. Thus:\n",
    "$$\\pi(S6) = \\text{U}$$\n",
    "- Hence, there’s only **one optimal action** in S6.\n",
    "\n",
    "7. **S7**: Since S7 has no option but to move **left** (right and up are blocked by a wall), the optimal policy is:\n",
    "$$\\pi(S7) = \\text{L}$$\n",
    "- Hence, there’s only **one optimal action** in S7.\n",
    "\n",
    "---\n",
    "\n",
    "- In **S1, S2, S4, S6, S7**, the robot has only **one optimal action**.\n",
    "- In **S3, S5**, the robot has **two optimal choices**: move left or right.\n",
    "\n",
    "Therefore, the total number of **optimal deterministic policies** is:\n",
    "\n",
    "$$\\text{Total optimal policies} = 2 \\times 2 = 4$$\n",
    "\n",
    "---\n",
    "\n",
    "![pics/P3S1b.png](pics/P3S1b.png)\n",
    "\n",
    "1. **Policy 1**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{L}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{L}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$\n",
    "   \n",
    "2. **Policy 2**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{L}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{R}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$\n",
    "\n",
    "3. **Policy 3**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{R}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{L}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$\n",
    "\n",
    "4. **Policy 4**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{R}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{R}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2\n",
    "\n",
    "The robot has no knowledge of the environment it's in. It has distance sensors on all four sides that tell it whether or not there is a wall next to it. For example, consider the images below.\n",
    "\n",
    "![pics/P3S2.png](pics/P3S2.png)\n",
    "\n",
    "Assume the objective is similar to the first scenario. In terms of key elements of an MDP, what has changed here? Can this problem still be represented by an MDP in a way that ultimately fulfills our objective? (Is the optimal policy what we want it to be?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the problem can still be represented by a **Markov Decision Process (MDP)**, as it fundamentally satisfies the requirements for an MDP. But there are key changes because the robot no longer knows exactly where it is within the environment.\n",
    "\n",
    "#### State Space (S)\n",
    "In general, states in this MDP are based on the robot’s sensory input: `{right, up, left, down}`. Each direction can either be blocked (0) or unblocked (1), thus defining the state space. Each state represents the robot’s local sensory information and position within the context of the grid. To simplify, we can define states $S_1, S_2, S_3, ...$ based on these sensory inputs.\n",
    "\n",
    "- **$S_1$**: Sensor reading `{right: 1, up: 0, left: 0, down: 0}`.\n",
    "- **$S_2$**: Sensor reading `{right: 0, up: 0, left: 1, down: 0}`.\n",
    "- **$S_3$**: Sensor reading `{right: 1, up: 0, left: 1, down: 0}`.\n",
    "- **$S_4$** (Ambiguous state): Sensor reading `{right: 1, up: 1, left: 1, down: 0}`.\n",
    "- **$S_5$** (terminal state): Sensor reading `{right: 0, up: 0, left: 0, down: 1}`. In can be either green or red.\n",
    "\n",
    "Thus, the **finite state space $S$** is:\n",
    "$$S = \\{ S_1, S_2, S_3, S_4, S_5 \\}$$\n",
    "\n",
    "#### Action Space (A)\n",
    "We define actions based on directional movements:\n",
    "- **$A_1$**: **Move Up**. The robot tries to move to a cell directly above in this action.\n",
    "- **$A_2$**: **Move Right**. The robot moves to the neighboring cell to the right.\n",
    "- **$A_3$**: **Move Left**. The robot moves to the neighboring cell to the left.\n",
    "  \n",
    "Thus, the **action space $A$** is:\n",
    "$$A = \\{ A_1, A_2, A_3 \\} = \\{ \\text{Up}, \\text{Right}, \\text{Left} \\}$$\n",
    "\n",
    "#### Transition Function (P(s', s, a))\n",
    "Using states $S_1 \\ldots S_5$ and actions $A_1, A_2, A_3$, consider each transition:\n",
    "\n",
    "| State Transition        | Action: Up | Action: Right | Action: Left |\n",
    "|-------------------------|------------------------|-----------------------------|----------------------------|\n",
    "| **$S_1$ → $S_1$** | 1 (blocked)            | 0                            | 1 (blocked)                  |\n",
    "| **$S_1$ → $S_4$** | 0                      | 1                            | 0                            |\n",
    "| **$S_2$ → $S_2$** | 1 (blocked)            | 1 (blocked)                  | 0                            |\n",
    "| **$S_2$ → $S_4$** | 0                      | 0                            | 1                            |\n",
    "| **$S_3$ → $S_3$** | 1 (blocked)            | 0                            | 0                            |\n",
    "| **$S_3$ → $S_4$** | 0                      | 1                            | 1                            |\n",
    "| **$S_4$ → $S_3$** | 0                      | $\\frac{2}{3}$ (probabilistic)| $\\frac{2}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_2$** | 0                      | $\\frac{1}{3}$ (probabilistic)| 0                            |\n",
    "| **$S_4$ → $S_1$** | 0                      | 0                            | $\\frac{1}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_5$** | 1 (terminal)           | 0                            | 0                            |\n",
    "\n",
    "Above:\n",
    "- **Blocked transitions** (e.g., \"Up\" when no upward movement is possible) have a **probability of 1** for staying in the same state.\n",
    "- Probabilistic transitions from $S_4$ to $S_1$, $S_2$, or $S_3$ when moving left or right.\n",
    "- Moving **up** from $S_4$ always leads to the terminal state $S_5$ with probability **1**.\n",
    "\n",
    "#### Reward Function (R(s, a))\n",
    "The reward structure also changes slightly. Instead of rewards depending on grid positions, the **reward is now a function of the sensor state** and the action taken:\n",
    "\n",
    "- If the robot takes the **Up** action in a sensor state where **Up is allowed** (i.e., there's no wall above), and this move leads to a terminal green cell, the robot gets a reward of **+1**.\n",
    "- For all other valid moves (where no immediate green state is reached), the robot receives **0 reward**.\n",
    "- If the robot tries an invalid action (e.g., moving into a wall), it stays in the same state and still receives a reward of **0**.\n",
    "  \n",
    "So, the reward function can be expressed as:\n",
    "\n",
    "$$R(s' | s, a) = \n",
    "     \\begin{cases}\n",
    "       +1 & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}, and a is \\text{Up}} \\\\\n",
    "       0 & \\text{Otherwise}\n",
    "     \\end{cases}$$\n",
    "\n",
    "#### Discount Factor (λ)\n",
    "The problem didn't change the discount factor, so $\\lambda$ remains at 0.9, meaning that future rewards are discounted slightly but still carry weight.\n",
    "\n",
    "---\n",
    "\n",
    "The optimal policy in the original problem was to **quickly reach the green terminal cells** by moving up when possible (if there was no wall above) and moving horizontally if necessary.\n",
    "\n",
    "In this new formulation, the **optimal policy would still aim to achieve the same goal**—the robot must use its sensor readings to figure out:\n",
    "- When it's best to move **Up** into a terminal cell,\n",
    "- How to move **Left** or **Right** to position itself below a terminal (green) cell if it's not already there.\n",
    "\n",
    "Thus, **in the optimal policy**, the robot will:\n",
    "- Move **Up** when its sensors indicate that moving up is allowed (i.e., in a sensor state like `{right: 1, up: 1, left: 1, down: 0}`).\n",
    "- Move **Left** or **Right** when needed to position itself for optimal transitions.\n",
    "\n",
    "Thus, the optimal policy will likely involve:\n",
    "$$\n",
    "\\pi(s) = \\begin{cases} \n",
    "\\text{Up} & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}} \\\\\n",
    "\\text{Right/Left} & \\text{If s = \\{right: 1, up: 0, left: 1, down: 0\\}} \\\\\n",
    "\\text{Right} & \\text{If s = \\{right: 1, up: 0, left: 0, down: 0\\}} \\\\\n",
    "\\text{Left} & \\text{If s = \\{right: 0, up: 0, left: 1, down: 0\\}} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So, the **optimal policy** still fulfills the ultimate goal by appropriately guiding the robot to the green terminal cells using sensor-guided movements. Therefore, under this formulation, the robot will still eventually learn the optimal way to achieve the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3\n",
    "\n",
    "Assume everything is the same as in Scenario 2, but our objective has changed, and we want the robot to enter only the two green cells on the left and right. If the robot enters the red cell in the figure below, it will receive a reward of -1, and if it enters one of the green cells, it will receive a reward of +1 (all other rewards are zero). For this task, $\\lambda = 0.9$.\n",
    "\n",
    "![pics/P3S3.png](pics/P3S3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "Can this problem still be represented by an MDP in a way that ultimately fulfills our objective? (Is the optimal policy what we want it to be?) If it can be represented, provide the desired optimal policy. If it cannot, explain what should be done, given the robot's input data, so that the robot can find an optimal policy that meets our objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can indeed be represented by a **Markov Decision Process (MDP)**. In MDPs, the environment's dynamics are expressed through states, actions, transitions, and rewards (the MDP is fully provided in the next section of question). The robot’s states can be represented as its position in the grid, its actions are left, right, and up, the rewards are received when entering specific \"trial\" cells (green +1 and the red -1), and the transitions outline how the robot moves based on its actions.\n",
    "\n",
    "However, **the optimal policy derived from this MDP may not be the one we desire** due to the robot's sensory limitation. From the robot’s perspective, when it starts in the bottom row, each of the bottom cells under the green or red cells looks the same until it receives feedback from the current state or the environment (i.e., reaching a terminal state). The robot does not know which upper cell will end up being a red or green cell because, from its starting position, all adjacent walls or sensors are identical. As a result, the optimal policy might make it behave similarly in cells beneath the red and green cells, which is not aligned with the desired behavior — the robot should avoid the cell under the red cell and prefer moving toward the cells under the green cells.\n",
    "\n",
    "##### Improved Distance Sensors as a Solution:\n",
    "To solve this issue, one method would be to modify the sensing capabilities of the robot. Currently, the robot’s sensors only tell it whether there’s a wall directly adjacent to it. If we **extend the sensor range** to allow the robot to detect walls that are a bit farther away (say, one extra cell ahead), the robot can then detect the presence of walls that define the boundary of the grid. This information can be crucial because the robot could infer whether it is in a corner (leftmost or rightmost cell) and indirectly deduce it is situated under one of the green target cells. When the robot detects no side walls, it knows it must be under the red cell, and thus the correct action would be to avoid moving up.\n",
    "\n",
    "With this extension to the robot’s sensors, the robot is able to recognize its position as either closer to the green terminal cells (left and right sides) or closer to the red terminal cell in the center. This extra sensory information helps the robot find an **optimal policy** that adheres to the task requirement of maximizing the number of times it reaches the green cells and minimizing the number of times it reaches the red cell.\n",
    "\n",
    "##### Alternative Approaches:\n",
    "\n",
    "There are alternative ways to resolve this problem without increasing the sensor range:\n",
    "\n",
    "1. **Learning from Experience via Reinforcement Learning**:\n",
    "    Instead of adjusting the robot's sensors, you can allow the robot to learn the environment through experience in the form of **Reinforcement Learning (RL)** algorithms such as Q-learning or SARSA. By using feedback (positive and negative rewards), the robot can gradually learn which states lead to favorable outcomes (green cells) and which states lead to undesirable outcomes (red cell) without needing upfront modifications to its sensors. Through exploration, it can distinguish the three bottom cells indirectly by learning the rewards that result from moving up from those cells.\n",
    "   \n",
    "   After a sufficient amount of exploration, the robot will learn that moving up from the cells below the green ones gives rewards, while moving up from the one below the red gives a penalty. As RL focuses on maximizing cumulative reward, the optimal policy will be to favor actions that lead to the green cells and avoid actions that lead to the red cell.\n",
    "\n",
    "2. **Pre-training the Robot**:\n",
    "   If time or reliability is a concern with the RL approach, you could **pre-train the robot** in a simulated environment to have it discover the favorable policy before deployment. By running multiple simulations in diverse grid configurations beforehand, the robot can learn to recognize patterns related to corner cells (green cells) and mid-grid cells (red cell). Once trained with a policy that favors green cells and avoids the red cell, it can be deployed with that knowledge.\n",
    "\n",
    "3. **Adding State Identifiers**:\n",
    "   Another solution is to introduce **state-based identifiers** (labels or distinct features) in the cells themselves. If each cell in the lower row had some distinguishing feature, such as a slight variation (visual cue, color, or numeric label), the robot’s input data could contain enough information to differentiate between the different regions under the green and red cells. This could be more practical than adjusting the robot’s sensory hardware for some real-world applications.\n",
    "\n",
    "##### Final Thoughts:\n",
    "\n",
    "1. **Distance Sensors Enhancement**: Extending the sensors to perceive walls that are farther away allowing the robot to infer its position with better accuracy.\n",
    "\n",
    "2. **Reinforcement Learning**: Allowing the robot to learn over time what the rewards of different states and actions are will naturally lead it to avoid the red cell over many episodes.\n",
    "\n",
    "3. **Pre-training**: Using simulations to help the robot develop an appropriate policy before it encounters the actual environment.\n",
    "\n",
    "4. **State Identifiers**: Each bottom row cell can have distinguishing features that make differentiation possible with existing sensors.\n",
    "\n",
    "In summary, while this problem is representable as an MDP, the optimal policy won't directly reflect the task's requirements because of the limitations of the robot’s input data (sensors). Enhancing the sensors, investing in RL, or simply marking the states differently are viable ways to tune the robot's policy toward the task objective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "If you solve Scenario 3 using dynamic programming methods, what MDP does the optimal solution correspond to? Represent the states of this MDP with $s_j$ (for $j = 0, 1, \\dots$), and indicate which grid cells correspond to these states. Represent the actions with $a_i$ (for $i = 0, 1, 2, \\dots$) and specify which robot actions they correspond to. Define the probability matrix $P$ accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pics/P3S3b](pics/P3S3b.png)\n",
    "\n",
    "##### States $s_j$\n",
    "In general, states in this MDP are based on the robot’s sensory input: `{right, up, left, down}`. Each direction can either be blocked (0) or unblocked (1), thus defining the state space. Each state represents the robot’s local sensory information and position within the context of the grid. To simplify, we can define states $S_1, S_2, S_3, ...$ based on these sensory inputs.\n",
    "\n",
    "- **$S_1$**: Sensor reading `{right: 1, up: 0, left: 0, down: 0}`.\n",
    "- **$S_2$**: Sensor reading `{right: 0, up: 0, left: 1, down: 0}`.\n",
    "- **$S_3$**: Sensor reading `{right: 1, up: 0, left: 1, down: 0}`.\n",
    "- **$S_4$** (Ambiguous state): Sensor reading `{right: 1, up: 1, left: 1, down: 0}`.\n",
    "- **$S_5$** (terminal state): Sensor reading `{right: 0, up: 0, left: 0, down: 1}`. In can be either green or red.\n",
    "\n",
    "Thus, the **finite state space $S$** is:\n",
    "$$S = \\{ S_1, S_2, S_3, S_4, S_5 \\}$$\n",
    "\n",
    "##### Actions $a_i$\n",
    "We define actions based on directional movements:\n",
    "- **$A_1$**: **Move Up**. The robot tries to move to a cell directly above in this action.\n",
    "- **$A_2$**: **Move Right**. The robot moves to the neighboring cell to the right.\n",
    "- **$A_3$**: **Move Left**. The robot moves to the neighboring cell to the left.\n",
    "  \n",
    "Thus, the **action space $A$** is:\n",
    "$$A = \\{ A_1, A_2, A_3 \\} = \\{ \\text{Up}, \\text{Right}, \\text{Left} \\}$$\n",
    "\n",
    "##### Transition Function\n",
    "This part describes how the robot transitions between states given specific actions. We assume transitions are **deterministic** and depend on whether the action is allowable based on the current sensory readings.\n",
    "\n",
    "**Transition for Action $A_1$ (Up)**\n",
    "- **$S_1$ through $S_3$ → $S_1, S_2, S_3$** (No change)\n",
    "  - If the robot tries to move **Up** in states $S_1, S_2, S_3$ where **moving up is blocked**, the robot stays in the same state.\n",
    "  \n",
    "- **$S_4$ → $S_5$**\n",
    "  - If the robot moves **Up** from $S_4$, it reaches one of the terminal cells (green or red), hence transitioning to the terminal state $S_5$.\n",
    "\n",
    "**Transition for Action $A_2$ (Right)**\n",
    "- **$S_1$ → $S_4$**: Move from the far left $S_1$ into the ambiguous central position $S_4$.\n",
    "  \n",
    "- **$S_3$ → $S_4$**: Move within the middle. From $S_3$, moving right brings the robot to the ambiguous central position $S_4$.\n",
    "\n",
    "- **$S_4$ → $S_3$ or $S_2$**: Moving **right** from $S_4$ potentially takes the robot to $S_2$ (For $\\frac{1}{3}$ times-S6) or $S_3$ (For $\\frac{2}{3}$ times-S2 and S4).\n",
    "\n",
    "**Transition for Action $A_3$ (Left)**\n",
    "- **$S_2$ → $S_4$**: Move from the far right $S_2$ to the central ambiguous state $S_4$.\n",
    "  \n",
    "- **$S_3$ → $S_4$**: Move from a middle position to the ambiguous $S_4$.\n",
    "\n",
    "- **$S_4$ → $S_3$ or $S_1$**: Moving **left** from $S_4$ potentially takes the robot to $S_1$ (For $\\frac{1}{3}$ times-S2) or $S_3$ (For $\\frac{2}{3}$ times-S4 and S6).\n",
    "\n",
    "**Probability Matrix $P$**\n",
    "\n",
    "Using states $S_1 \\ldots S_5$ and actions $A_1, A_2, A_3$, consider each transition:\n",
    "\n",
    "| State Transition        | Action: Up ($A_1$) | Action: Right ($A_2$) | Action: Left ($A_3$) |\n",
    "|-------------------------|------------------------|-----------------------------|----------------------------|\n",
    "| **$S_1$ → $S_1$** | 1 (blocked)            | 0                            | 1 (blocked)                  |\n",
    "| **$S_1$ → $S_4$** | 0                      | 1                            | 0                            |\n",
    "| **$S_2$ → $S_2$** | 1 (blocked)            | 1 (blocked)                  | 0                            |\n",
    "| **$S_2$ → $S_4$** | 0                      | 0                            | 1                            |\n",
    "| **$S_3$ → $S_3$** | 1 (blocked)            | 0                            | 0                            |\n",
    "| **$S_3$ → $S_4$** | 0                      | 1                            | 1                            |\n",
    "| **$S_4$ → $S_3$** | 0                      | $\\frac{2}{3}$ (probabilistic)| $\\frac{2}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_2$** | 0                      | $\\frac{1}{3}$ (probabilistic)| 0                            |\n",
    "| **$S_4$ → $S_1$** | 0                      | 0                            | $\\frac{1}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_5$** | 1 (terminal)           | 0                            | 0                            |\n",
    "  \n",
    "Above:\n",
    "- **Blocked transitions** (e.g., \"Up\" when no upward movement is possible) have a **probability of 1** for staying in the same state.\n",
    "- Probabilistic transitions from $S_4$ to $S_1$, $S_2$, or $S_3$ when moving left or right.\n",
    "- Moving **up** from $S_4$ always leads to the terminal state $S_5$ with probability **1**.\n",
    "\n",
    "**Reward Function $R$**\n",
    "\n",
    "- If the robot takes the **Up** action in a sensor state where **Up is allowed** (i.e., there's no wall above), in $\\frac{2}{3}$ times it receives **+1** (entering green cells), and in $\\frac{1}{3}$ times, it receives **-1** (red cell).\n",
    "- For all other valid moves (where no immediate green state is reached), the robot receives **0 reward**.\n",
    "- If the robot tries an invalid action (e.g., moving into a wall), it stays in the same state and still receives a reward of **0**.\n",
    "  \n",
    "So, the reward function can be expressed as:\n",
    "\n",
    "$$R(s' | s, a) = \n",
    "     \\begin{cases}\n",
    "       +1 & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}, and a is \\text{Up}, with probability of } \\frac{2}{3} \\\\\n",
    "       -1 & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}, and a is \\text{Up}, with probability of } \\frac{1}{3} \\\\\n",
    "       0 & \\text{Otherwise}\n",
    "     \\end{cases}$$\n",
    "\n",
    "This MDP can indeed be efficiently solved using **dynamic programming** methods or value iteration once set up this way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "\n",
    "In scenario 3, implement **MDP** in the Python environment and determine the optimal policy using **value iteration** and **policy iteration**. If the obtained optimal policy is applied to the robot, what decision will it make in each part of the grid environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from hands_on.src.value_iteration import ValueIteration\n",
    "from hands_on.src.tabular_value_function import TabularValueFunction\n",
    "from hands_on.src.value_policy import ValuePolicy\n",
    "\n",
    "from hands_on.src.policy_iteration import PolicyIteration\n",
    "from hands_on.src.tabular_policy import TabularPolicy\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the mapping of actions to directions\n",
    "action_to_vector = {\n",
    "    \"Right\": (1, 0),  # (dx, dy) for right\n",
    "    \"Left\":  (-1, 0), # (dx, dy) for left\n",
    "    \"Up\":    (0, 1),  # (dx, dy) for up\n",
    "}\n",
    "\n",
    "# Define grid positions for states\n",
    "state_positions = {\n",
    "    \"S1\": (0, 0), \"S2\": (6, 0), \"S3\": (2, 0), \"S4_1\": (1, 0), \"S4_2\": (3, 0), \"S4_3\": (5, 0),\n",
    "    \"S5_1\": (1, 1), \"S5_2\": (3, 1), \"S5_3\": (5, 1)\n",
    "}\n",
    "\n",
    "# Create a function to plot the custom grid with actions\n",
    "def plot_custom_grid(policy: callable, state_positions, states, actions):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    # Set bounds of the plot (limits)\n",
    "    ax.set_xlim(-1, 7)\n",
    "    ax.set_ylim(-1, 2)\n",
    "\n",
    "    # Plot the top row terminal S5T states\n",
    "    for state, position in state_positions.items():\n",
    "        if \"S5\" in state:\n",
    "            # Color the middle terminal state red, others green\n",
    "            color = 'red' if state == \"S5_2\" else 'green'\n",
    "            ax.add_patch(plt.Rectangle(position, 1, 1, facecolor=color, edgecolor='black'))\n",
    "            ax.text(position[0] + 0.5, position[1] + 0.5, \"S5\", color='white', fontsize=14, ha='center', va='center')\n",
    "\n",
    "    # Plot the bottom row regular states\n",
    "    bottom_states = {\"S1\": (0, 0), \"S4_1\": (1, 0), \"S3\": (2, 0), \"S4_2\": (3, 0), \"S3_2\": (4, 0), \"S4_3\": (5, 0), \"S2\": (6, 0)}\n",
    "    \n",
    "    state_actions = {}\n",
    "\n",
    "    for state in states:\n",
    "        state_actions[state] = policy(state, actions)\n",
    "\n",
    "    for state_key, position in bottom_states.items():\n",
    "        state_name = state_key[:-2] if \"_\" in state_key else state_key   # Handle state names (like \"S4_2\" -> \"S4\")\n",
    "        ax.add_patch(plt.Rectangle(position, 1, 1, facecolor='skyblue', edgecolor='black'))\n",
    "        ax.text(position[0] + 0.5, position[1] + 0.5, state_name, color='black', fontsize=14, ha='center', va='center')\n",
    "\n",
    "        # Plot the action arrows according to the policy\n",
    "        action = state_actions[state_name]\n",
    "        dx, dy = action_to_vector[action]\n",
    "        \n",
    "        # Plot the action arrow for the state\n",
    "        ax.arrow(position[0] + 0.5, position[1] + 0.5, 0.25 * dx, 0.25 * dy, head_width=0.1, head_length=0.1, fc='white', ec='white')\n",
    "\n",
    "    # Remove ticks and format plot\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.title(\"Policy Visualization\")\n",
    "    plt.grid(False)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()\n",
    "\n",
    "    return state_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, transition_func, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_func = transition_func\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transition_func[state][action]\n",
    "    \n",
    "    def get_reward(self, state, action, new_state):\n",
    "        if state == \"S4\" and action == \"Up\":\n",
    "            return random.choices([1, -1], weights=[2, 1], k=1)[0]\n",
    "        return 0\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDP components\n",
    "states = ['S1', 'S2', 'S3', 'S4', 'S5']  # S5 is the terminal state\n",
    "actions = ['Up', 'Right', 'Left']\n",
    "discount_factor = 0.9   # Lambda = 0.9\n",
    "\n",
    "# Set the transition probability matrix P(s' | s, a)\n",
    "# Transitions represented as P[s_current][a] = list of possible (s_next, prob).\n",
    "transition_func = {\n",
    "    'S1': {'Up': [('S1', 1)], 'Right': [('S4', 1)], 'Left': [('S1', 1)]},       # S1 -> right leads to S4\n",
    "    'S2': {'Up': [('S2', 1)], 'Right': [('S2', 1)], 'Left': [('S4', 1)]},       # S2 -> left leads to S4\n",
    "    'S3': {'Up': [('S3', 1)], 'Right': [('S4', 1)], 'Left': [('S4', 1)]},       # S3 -> left/right leads to S4\n",
    "    'S4': {'Up': [('S5', 1)], 'Right': [('S2', 0.33), ('S3', 0.67)],            # S4 -> Up to terminal\n",
    "                              'Left': [('S1', 0.33), ('S3', 0.67)]},\n",
    "    'S5': {'Up': [('S5', 1)], 'Right': [('S5', 1)], 'Left': [('S5', 1)]}        # S5 is terminal (absorbing)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(states, actions, transition_func, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9, 0.9, 0.9, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "vi_max_iter = 100\n",
    "\n",
    "vi_values = TabularValueFunction()\n",
    "\n",
    "time_start = time()\n",
    "vi_iter = ValueIteration(mdp, vi_values).value_iteration(max_iterations=vi_max_iter)\n",
    "time_end = time()\n",
    "\n",
    "vi_time = time_end - time_start\n",
    "\n",
    "print(vi_values.get_values(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_policy = ValuePolicy(mdp, vi_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vi_policy = plot_custom_grid(vi_policy.select_action, state_positions, states, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_policy = TabularPolicy(default_action=\"Left\")\n",
    "\n",
    "pi_max_iter = 1000\n",
    "\n",
    "time_start = time()\n",
    "pi_iter = PolicyIteration(mdp, pi_policy).policy_iteration(max_iterations=pi_max_iter)\n",
    "time_end = time()\n",
    "\n",
    "pi_time = time_end - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi_policy = plot_custom_grid(pi_policy.select_action, state_positions, states, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison of VI and PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "| State   | Pi Policy Action   | Vi Policy Action   |\n",
      "+=========+====================+====================+\n",
      "| S1      | Right              | Right              |\n",
      "+---------+--------------------+--------------------+\n",
      "| S2      | Left               | Left               |\n",
      "+---------+--------------------+--------------------+\n",
      "| S3      | Right              | Left               |\n",
      "+---------+--------------------+--------------------+\n",
      "| S4      | Up                 | Up                 |\n",
      "+---------+--------------------+--------------------+\n",
      "| S5      | Right              | Up                 |\n",
      "+---------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming `states` is a list of all states and \n",
    "# `actions` is a list of possible actions.\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    pi_action = pi_policy.get(state, 'N/A')\n",
    "    vi_action = vi_policy.get(state, 'N/A')\n",
    "    table_data.append([state, pi_action, vi_action])\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"State\", \"Pi Policy Action\", \"Vi Policy Action\"]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration: 21, Time: 0.006002902984619141\n",
      "Value Iteration: 5, Time: 0.0010058879852294922\n"
     ]
    }
   ],
   "source": [
    "print(f\"Policy Iteration: {pi_iter}, Time: {pi_time}\")\n",
    "print(f\"Value Iteration: {vi_iter}, Time: {vi_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "\n",
    "Consider scenario 3 in a situation where the robot is aware of its location on the map. Once again, determine the optimal policy using the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the mapping of actions to directions\n",
    "action_to_vector = {\n",
    "    \"Right\": (1, 0),  # (dx, dy) for right\n",
    "    \"Left\":  (-1, 0), # (dx, dy) for left\n",
    "    \"Up\":    (0, 1),  # (dx, dy) for up\n",
    "}\n",
    "\n",
    "# Define grid positions for states\n",
    "state_positions = {\n",
    "    \"S1\": (0, 0), \"S2\": (1, 0), \"S3\": (2, 0), \"S4\": (3, 0), \"S5\": (4, 0), \"S6\": (5, 0), \"S7\": (6, 0),\n",
    "    \"T1\": (1, 1), \"T3\": (3, 1), \"T5\": (5, 1)\n",
    "}\n",
    "\n",
    "# Create a function to plot the custom grid with actions\n",
    "def plot_custom_grid(policy: callable, state_positions, states, actions):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    # Set bounds of the plot (limits)\n",
    "    ax.set_xlim(-1, 7)\n",
    "    ax.set_ylim(-1, 2)\n",
    "\n",
    "    # Plot the top row terminal S5T states\n",
    "    for state, position in state_positions.items():\n",
    "        if \"T\" in state:\n",
    "            # Color the middle terminal state red, others green\n",
    "            color = 'red' if state == \"T3\" else 'green'\n",
    "            ax.add_patch(plt.Rectangle(position, 1, 1, facecolor=color, edgecolor='black'))\n",
    "            ax.text(position[0] + 0.5, position[1] + 0.5, \"T\", color='white', fontsize=14, ha='center', va='center')\n",
    "\n",
    "    # Plot the bottom row regular states\n",
    "    bottom_states = {\"S1\": (0, 0), \"S2\": (1, 0), \"S3\": (2, 0), \"S4\": (3, 0), \"S5\": (4, 0), \"S6\": (5, 0), \"S7\": (6, 0)}\n",
    "    \n",
    "    state_actions = {}\n",
    "\n",
    "    for state in states:\n",
    "        state_actions[state] = policy(state, actions)\n",
    "\n",
    "    for state_key, position in bottom_states.items():\n",
    "        state_name = state_key   # Handle state names (like \"S4_2\" -> \"S4\")\n",
    "        ax.add_patch(plt.Rectangle(position, 1, 1, facecolor='skyblue', edgecolor='black'))\n",
    "        ax.text(position[0] + 0.5, position[1] + 0.5, state_name, color='black', fontsize=14, ha='center', va='center')\n",
    "\n",
    "        # Plot the action arrows according to the policy\n",
    "        action = state_actions[state_name]\n",
    "        dx, dy = action_to_vector[action]\n",
    "        \n",
    "        # Plot the action arrow for the state\n",
    "        ax.arrow(position[0] + 0.5, position[1] + 0.5, 0.25 * dx, 0.25 * dy, head_width=0.1, head_length=0.1, fc='white', ec='white')\n",
    "\n",
    "    # Remove ticks and format plot\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.title(\"Policy Visualization\")\n",
    "    plt.grid(False)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()\n",
    "\n",
    "    return state_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, reward_func, transition_func, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.reward_func = reward_func\n",
    "        self.transition_func = transition_func\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transition_func[state][action]\n",
    "    \n",
    "    def get_reward(self, state, action, new_state):\n",
    "        return self.reward_func[state][action][new_state]\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDP components\n",
    "states = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'T1', 'T3', 'T5'] \n",
    "actions = ['Up', 'Right', 'Left']\n",
    "discount_factor = 0.9   # Lambda = 0.9\n",
    "\n",
    "# Reward function R(s, a) -> `terminal (+1/-1)` and `0` elsewhere (unused)\n",
    "reward_func = {\n",
    "    'S1': {'Up': {'S1': 0}, 'Right': {'S2': 0}, 'Left': {'S1': 0}},       \n",
    "    'S2': {'Up': {'T1': +1}, 'Right': {'S3': 0}, 'Left': {'S1': 0}},       \n",
    "    'S3': {'Up': {'S3': 0}, 'Right': {'S4': 0}, 'Left': {'S2': 0}},       \n",
    "    'S4': {'Up': {'T3': -1}, 'Right': {'S5': 0}, 'Left': {'S3': 0}},\n",
    "    'S5': {'Up': {'S5': 0}, 'Right': {'S6': 0}, 'Left': {'S4': 0}},        \n",
    "    'S6': {'Up': {'T5': +1}, 'Right': {'S7': 0}, 'Left': {'S5': 0}},\n",
    "    'S7': {'Up': {'S7': 0}, 'Right': {'S7': 0}, 'Left': {'S6': 0}},\n",
    "    'T1': {'Up': {'T1': 0}, 'Right': {'T1': 0}, 'Left': {'T1': 0}},\n",
    "    'T3': {'Up': {'T3': 0}, 'Right': {'T3': 0}, 'Left': {'T3': 0}},\n",
    "    'T5': {'Up': {'T5': 0}, 'Right': {'T5': 0}, 'Left': {'T5': 0}}\n",
    "}\n",
    "\n",
    "# Set the transition probability matrix P(s' | s, a)\n",
    "# Transitions represented as P[s_current][a] = list of possible (s_next, prob).\n",
    "transition_func = {\n",
    "    'S1': {'Up': [('S1', 1)], 'Right': [('S2', 1)], 'Left': [('S1', 1)]},       \n",
    "    'S2': {'Up': [('T1', 1)], 'Right': [('S3', 1)], 'Left': [('S1', 1)]},       \n",
    "    'S3': {'Up': [('S3', 1)], 'Right': [('S4', 1)], 'Left': [('S2', 1)]},       \n",
    "    'S4': {'Up': [('T3', 1)], 'Right': [('S5', 1)], 'Left': [('S3', 1)]},\n",
    "    'S5': {'Up': [('S5', 1)], 'Right': [('S6', 1)], 'Left': [('S4', 1)]},        \n",
    "    'S6': {'Up': [('T5', 1)], 'Right': [('S7', 1)], 'Left': [('S5', 1)]},\n",
    "    'S7': {'Up': [('S7', 1)], 'Right': [('S7', 1)], 'Left': [('S6', 1)]},\n",
    "    'T1': {'Up': [('T1', 1)], 'Right': [('T1', 1)], 'Left': [('T1', 1)]},\n",
    "    'T3': {'Up': [('T3', 1)], 'Right': [('T3', 1)], 'Left': [('T3', 1)]},\n",
    "    'T5': {'Up': [('T5', 1)], 'Right': [('T5', 1)], 'Left': [('T5', 1)]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(states, actions, reward_func, transition_func, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9, 1.0, 0.9, 0.81, 0.9, 1.0, 0.9, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "vi_values = TabularValueFunction()\n",
    "\n",
    "vi_max_iter = 1000\n",
    "\n",
    "time_start = time()\n",
    "vi_iter = ValueIteration(mdp, vi_values).value_iteration(max_iterations=vi_max_iter)\n",
    "time_end = time()\n",
    "\n",
    "vi_time = time_end - time_start\n",
    "\n",
    "print(vi_values.get_values(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_policy = ValuePolicy(mdp, vi_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for state in states:\n",
    "#     print(f\"{state}: {vi_policy.select_action(state, actions)}\")\n",
    "\n",
    "vi_policy = plot_custom_grid(vi_policy.select_action, state_positions, states, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_policy = TabularPolicy(default_action=\"Up\")\n",
    "\n",
    "pi_max_iter = 1000\n",
    "\n",
    "time_start = time()\n",
    "pi_iter = PolicyIteration(mdp, pi_policy).policy_iteration(max_iterations=pi_max_iter)\n",
    "time_end = time()\n",
    "\n",
    "pi_time = time_end - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi_policy = plot_custom_grid(pi_policy.select_action, state_positions, states, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison of PI and VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "| State   | Pi Policy Action   | Vi Policy Action   |\n",
      "+=========+====================+====================+\n",
      "| S1      | Right              | Right              |\n",
      "+---------+--------------------+--------------------+\n",
      "| S2      | Up                 | Up                 |\n",
      "+---------+--------------------+--------------------+\n",
      "| S3      | Left               | Left               |\n",
      "+---------+--------------------+--------------------+\n",
      "| S4      | Right              | Right              |\n",
      "+---------+--------------------+--------------------+\n",
      "| S5      | Right              | Right              |\n",
      "+---------+--------------------+--------------------+\n",
      "| S6      | Up                 | Up                 |\n",
      "+---------+--------------------+--------------------+\n",
      "| S7      | Left               | Left               |\n",
      "+---------+--------------------+--------------------+\n",
      "| T1      | Up                 | Left               |\n",
      "+---------+--------------------+--------------------+\n",
      "| T3      | Right              | Up                 |\n",
      "+---------+--------------------+--------------------+\n",
      "| T5      | Up                 | Right              |\n",
      "+---------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming `states` is a list of all states and \n",
    "# `actions` is a list of possible actions.\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    pi_action = pi_policy.get(state, 'N/A')\n",
    "    vi_action = vi_policy.get(state, 'N/A')\n",
    "    table_data.append([state, pi_action, vi_action])\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"State\", \"Pi Policy Action\", \"Vi Policy Action\"]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration: 212, Time: 0.09899711608886719\n",
      "Value Iteration: 3, Time: 0.0009992122650146484\n"
     ]
    }
   ],
   "source": [
    "print(f\"Policy Iteration: {pi_iter}, Time: {pi_time}\")\n",
    "print(f\"Value Iteration: {vi_iter}, Time: {vi_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "In this question, we examine the effect of episode length (Horizon) on the agent’s policy. Consider a robot that is tasked with managing stock shares. (Assume this problem can be represented as an MDP.)\n",
    "\n",
    "Let $ s $ represent the number of shares the robot currently has (an integer always between [0,10]). At each moment, the robot has two options: to sell (if possible, $ s $ decreases by one unit) or to buy (if possible, $ s $ increases by one unit).\n",
    "\n",
    "- If $ s > 0 $ and the agent sells, it receives a reward of +1 for the sale, and the stock level changes to $ s - 1 $. If $ s = 0 $, nothing happens.\n",
    "- If $ s < 9 $ and the agent buys, it receives no reward, and the stock level changes to $ s + 1 $.\n",
    "- The stock owner wants the inventory to be fully stocked at the end of the day; therefore, if the stock level reaches the maximum value of $ s = 10 $, the agent receives a reward of +100.\n",
    "- The state $ s = 10 $ is also a terminal state, and the problem ends if it is reached.\n",
    "\n",
    "The reward function, denoted by $ r(s, a, s') $, is summarized as follows:\n",
    "\n",
    "- $ r(s, \\text{sell}, s - 1) = 1 $ for $ s > 0 $\n",
    "- $ r(0, \\text{sell}, 0) = 0 $\n",
    "- $ r(s, \\text{buy}, s + 1) = 0 $ for $ s < 9 $\n",
    "- $ r(9, \\text{buy}, 10) = 100 $, indicating that moving from $ s = 9 $ to $ s = 10 $ gives a reward of +100, reaching the maximum stock level.\n",
    "\n",
    "It is assumed that the stock level always starts from $ s = 3 $ at the beginning of the day. We will examine how the agent’s optimal policy changes by setting a limited horizon $H$ for the problem. Recall that the horizon $ H $ refers to a limit on the number of time steps in which the agent can interact with the MDP before the episode ends, regardless of whether a terminal state has been reached. We will analyze the characteristics of the optimal policy (the policy that maximizes the episode’s reward) as the horizon $ H $ changes. (For the finite horizon, the discount factor is $ \\gamma = 1 $).\n",
    "\n",
    "![pics/P4.png](pics/P4.png)\n",
    "\n",
    "For example, assume $ H = 4 $. The agent can sell for three steps, moving from $ s = 3 $ to $ s = 2 $, then $ s = 1 $, and finally $ s = 0 $, receiving rewards of +1, +1, and +1 for each sell action. In the fourth step, the inventory is empty, so it can either sell or buy, but it will not receive any reward in either case. Then, the problem ends due to the time limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a \n",
    "Starting from the initial state $ s = 3 $, is it possible to choose a value of $ H $ such that the optimal policy includes both buying and selling steps during the execution? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, let’s explore $H = 5$ in detail:\n",
    "\n",
    "**Setting $H = 5$:**\n",
    "\n",
    "We begin at $s = 3$ and examine a scenario with 3 sells, followed by 1 buy and then 1 sell:\n",
    "\n",
    "- **First step (s = 3)**: Sell, move to $s = 2$, and receive a reward of +1.\n",
    "- **Second step (s = 2)**: Sell, move to $s = 1$, and receive a reward of +1.\n",
    "- **Third step (s = 1)**: Sell, move to $s = 0$, and receive a reward of +1.\n",
    "- **Fourth step (s = 0)**: Since there are no more shares to sell, the optimal action is to buy. Buy, move to $s = 1$, and receive no reward.\n",
    "- **Fifth step (s = 1)**: Sell, return to $s = 0$, and receive a reward of +1.\n",
    "\n",
    "In this scenario, with a horizon $H = 5$, we obtain 4 units of reward: 1 unit from each of the 4 sell actions. The agent first sells down to $s = 0$, buys back to $s = 1$, and finally sells again.\n",
    "\n",
    "This sequence contains both **buying** and **selling** steps, thus confirming that it is possible for the agent's optimal policy to include both actions if the horizon $H$ is set to 5.\n",
    "\n",
    "---\n",
    "\n",
    "#### **General Analysis of $H$ and Buying/Selling**\n",
    "\n",
    "- **For $H \\leq 4$**:\n",
    "  At this point, the agent **only sells** to receive immediate rewards without any buying actions, because there isn't enough time or benefit from trying to buy stocks. The reward for selling is immediate, whereas buying does not provide any immediate reward. \n",
    "\n",
    "  - For example, with **$H = 4$**:\n",
    "    - The optimal policy involves the agent selling for the first three steps, moving from $s = 3$ to $s = 0$.\n",
    "    - At the fourth step (at $s = 0$), the robot can either sell again (which would yield no reward) or buy, but the point is that **the optimal policy would not necessarily contain any buy** since the buy action doesn't lead to an immediate benefit and there's no sufficient time left to capitalize on it. So, you will often just see 3 sells and perhaps some arbitrary choice in the 4th.\n",
    "  \n",
    "- **For $H = 5$**:\n",
    "  With $H = 5$, the agent can start selling, just like $H = 4$, but might add **a single buy step** at the fourth step (after the third sale).\n",
    "\n",
    "  - Optimal policy for $H = 5$:\n",
    "    - The first three actions are sells: $s = 3 \\to 2 \\to 1 \\to 0$ with rewards of +1 for each sell.\n",
    "    - In the fourth step, the agent might buy (because there’s no benefit in selling, as $s = 0$).\n",
    "    - Finally, selling again in the fifth step yields another reward of +1.\n",
    "  \n",
    "  Thus, $H = 5$ allows for a mixture of **1 buy step and 4 sells**, leading to maximal reward accumulation.\n",
    "\n",
    "- **For $H = 6$**:\n",
    "  $H = 6$ is similar to $H = 5$. The agent can follow a similar strategy, where first they sell, then buy at one point, and finally sell again to get a reward.\n",
    "\n",
    "  - Optimal policy for $H = 6$:\n",
    "    - The agent might sell for three steps (moving from $s = 3 \\to s = 0$).\n",
    "    - Then the agent buys (e.g., at step 4), moves to $s = 1$, sells it back again to stay within the horizon.\n",
    "    - Additional sells/buys in a similar cycle could lead to a total return of 4-5, depending on the steps chosen.\n",
    "\n",
    "  Therefore, $H = 6$ contains a **mixture of several buys and sells** where the agent maximizes reward while handling the $H = 6$ steps.\n",
    "\n",
    "- **For $H = 7$**:\n",
    "    \n",
    "  At $H = 7$, the agent no longer sells to maximize rewards. Instead, the optimal policy will switch to buying actions aimed at maximizing the final large reward of 100 at $s = 10$. Since the starting state is $s = 3$, it can take up to 7 steps:\n",
    "    \n",
    "  - The agent will **only buy** to increase the stock from $s = 3 \\to s = 10$.\n",
    "  - By the seventh step, the robot will buy enough shares (moving from $s = 3 \\to s = 10$) and reach the terminal state at $s = 10$, where it receives the reward of 100.\n",
    "\n",
    "  Thus, for $H = 7$, **buying only** guarantees the large reward at the terminal state.\n",
    "\n",
    "- **For $H = 8$**:\n",
    "    \n",
    "  The situation is quite similar to $H = 7$. Even though the agent has 8 steps, the optimal trajectory is still to buy until it reaches the maximum stock level of $s = 10$.\n",
    "\n",
    "  - If the agent spends any actions selling (for small immediate rewards), it sacrifices the future reward of 100 from reaching $s = 10$, so selling is suboptimal.\n",
    "  - Therefore, the **only optimal policy** in this case is to **buy** in order to maximize future rewards.\n",
    "\n",
    "  The agent will still follow a pure-buy policy to reach $s = 10$ and get rewarded with 100.\n",
    "\n",
    "- **For $H \\geq 9$**:\n",
    "    \n",
    "  For horizons $H \\geq 9$, the agent has enough time to maximize the reward not just by stockpiling all the way up to $s = 10$, but also by mixing in intermittent sells to gain additional rewards along the way, before ultimately ensuring it ends at $s = 10$.\n",
    "\n",
    "  - The agent can use $s \\to s - 1$ transitions (selling) to obtain immediate rewards (e.g., +1) at several points and then buy back the shares in order to reach $s = 10$.\n",
    "  - The idea is to balance between immediate rewards from selling shares and the future reward of 100 for max stock level $s = 10$.\n",
    "\n",
    "  For example, for $H = 9$, the agent can:\n",
    "  - Sell for a couple of actions to gain small immediate rewards, then **switch to buying to ensure that the final state is $s = 10$**, and maximize the larger future reward.\n",
    "\n",
    "  For **$H \\geq 9$**, the optimal policy is a **mixture of sells and buys** to balance immediate rewards and ensure reaching $s = 10$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "Starting from the initial state $ s = 3 $, for what values of $ H $ does the optimal policy lead to a fully stocked inventory? In other words, provide a range for $ H $.\n",
    "\n",
    "*Note 1:* We consider the inventory fully stocked when the buy action is chosen in state $ s = 9 $, causing a transition to $ s = 10 $. This includes the last time step in the horizon as well.\n",
    "\n",
    "*Note 2:* By performing only buy actions, the agent can reach $ s = 10 $ from $ s = 3 $ in $ H = 7 $ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is $7 \\leq H$. Let's see why:\n",
    "\n",
    "#### $H \\leq 6$\n",
    "\n",
    "- The agent will prefer selling rather than buying, as reaching $s = 10$ is impossible within 6 steps. The agent will maximize the immediate rewards from selling, and the inventory will not become fully stocked.\n",
    "\n",
    "#### $H = 7$\n",
    "\n",
    "- Starting at $s = 3$, if $H = 7$, by **only buying**, the agent can reach $s = 10$ on the last step:\n",
    "  $$s = 3 \\to 4 \\to 5 \\to 6 \\to 7 \\to 8 \\to 9 \\to 10.$$\n",
    "  \n",
    "  Reaching $s = 10$ gives the agent a reward of +100 in the final step. This is the maximum possible reward for this horizon because selling earlier for a reward of +1 per sale sacrifices the large reward of +100 from reaching $s = 10$. The optimal policy in this case is to **always buy** and reach $s = 10$.\n",
    "\n",
    "Thus, **for $H = 7$**, the optimal policy leads to a fully stocked inventory, since the agent sacrifices no time for selling and focuses on accumulating the large terminal reward.\n",
    "\n",
    "#### $H = 8$\n",
    "\n",
    "- With $H = 8$, the agent can reach $s = 10$ by **only buying**:\n",
    "\n",
    "  $$s = 3 \\to 4 \\to 5 \\to 6 \\to 7 \\to 8 \\to 9 \\to 10.$$\n",
    "  \n",
    "  Like in the case of $H = 7$, the agent will receive the large reward of +100 upon reaching $s = 10$. There is one extra step after reaching $s = 10$, but since reaching $s = 10$ ends the episode, no further actions are possible once the maximum state is reached.\n",
    "\n",
    "  If the agent spends any time selling earlier, they will not collect the large reward of +100 from reaching $s = 10$. Therefore, the optimal policy for $H = 8$ is still to **buy exclusively** and reach $s = 10$, maximizing the final reward.\n",
    "\n",
    "Thus, **for $H = 8$**, the optimal policy leads to fully stocking the inventory. The agent once again only buys to reach $s = 10$ and get the large reward.\n",
    "\n",
    "#### $H = 9$ and beyond\n",
    "\n",
    "- For $H \\geq 9$, the agent has even more time. The important decision now is to determine whether earning small rewards by selling before reaching $s = 10$ can provide better total rewards than simply reaching $s = 10$ for the big reward of +100.\n",
    "\n",
    "- Since the reward from reaching $s = 10$ is **so large** (+100), the optimal strategy will still involve **reaching $s = 10$** within the time limit.\n",
    "\n",
    "  However, the agent might choose to sell once or twice **early** to gather a small immediate reward (e.g., selling at $s = 3 \\to s = 2$) and then buy back to ensure it still reaches $s = 10$. As long as reaching $s = 10$ is still feasible, the agent can mix buying and selling to maximize the total reward.\n",
    "\n",
    "  One possible policy for $H = 9$ is to sell for 1 step (gaining +1 reward), and then continue buying towards $s = 10$, ensuring that the agent still gets the major reward of +100.\n",
    "\n",
    "Thus, **for $H \\geq 9$**, the optimal policy may include a mix of **some early selling** for immediate rewards followed by **buying to reach $s = 10$**. The agent still values reaching the fully stocked inventory because of the large +100 reward. This means the inventory ends fully stocked, but the agent makes a few sales en route to $s = 10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c\n",
    "Now, consider the infinite-horizon setting with a discount factor $ \\gamma $. In other words, there is no time limit, and the problem only ends if a terminal state is reached. Suppose $ \\gamma = 0 $; what action does the optimal policy take when $ s = 3 $? What action does the optimal policy take when $ s = 9 $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\gamma = 0$, the agent only cares about rewards in the current step and ignores all possible future outcomes. So, **at $s = 3$**, the best action is to **sell** a share because this provides an immediate reward of +1. additionally, **at $s = 9$** the best action is to **buy** a share to reach $s = 10$, which gives the large immediate reward of +100.\n",
    "\n",
    "#### What does $\\gamma$ = 0 mean?\n",
    "\n",
    "The discount factor $\\gamma$ tells us how the agent values **future rewards**. If $\\gamma = 0$, the agent **does not care about future rewards at all**; it only cares about the **immediate rewards**. This means the agent will choose actions that maximize the current step's reward without considering what might happen next.\n",
    "\n",
    "In this scenario, with $\\gamma = 0$, **only the immediate reward matters** for the agent at every step. The agent isn't concerned about long-term outcomes, such as reaching the fully stocked inventory $s = 10$ and receiving the +100 reward at the end. Instead, it focuses solely on the action that gives the highest reward in the current step.\n",
    "\n",
    "#### What happens at $s = 3$?\n",
    "At $s = 3$, the agent has two options:\n",
    "1. **Sell**: Selling one share will give the agent an immediate reward of **+1** and move it to state $s = 2$.\n",
    "2. **Buy**: Buying one share will give the agent **0 immediate reward** and move it to state $s = 4$.\n",
    "\n",
    "Since $\\gamma = 0$, the agent only cares about **immediate reward**, not future rewards. Therefore, the agent will choose the action that gives the best **immediate** return. **Selling** at $s = 3$ provides an immediate reward of **+1**, while buying provides **0 immediate reward**.\n",
    "\n",
    "Thus, **the optimal action at $s = 3$** is to **sell** a share in order to get the immediate reward of +1.\n",
    "\n",
    "#### What happens at $s = 9$?\n",
    "Similarly, at $s = 9$, the agent has two options:\n",
    "1. **Sell**: Selling one share will give the agent an immediate reward of **+1** and move it to state $s = 8$.\n",
    "2. **Buy**: Buying one share will move the agent to state $s = 10$, which is the terminal state, and will immediately give a reward of **+100**.\n",
    "\n",
    "Since $\\gamma = 0$ means the agent only cares about **immediate reward**, the choice here is clear: **Buying** in $s = 9$ leads to an immediate reward of **+100**, which is much higher than the alternative of **+1** received from selling.\n",
    "\n",
    "Thus, **the optimal action at $s = 9$** is to **buy** the last share, transitioning to $s = 10$ and earning the large immediate reward of +100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d\n",
    "In the infinite-horizon setting with a discount factor $ \\gamma $, is it possible to choose a constant $ \\gamma \\in (0, 1] $ such that the optimal policy, starting from $ s = 3 $, never fully stocks the inventory? If so, find a range of $ \\gamma $ that meets this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider two policies:\n",
    "\n",
    "1. **Policy for filling the inventory (buying until $s = 10$)**:\n",
    "    - Starting from $s = 3$, the agent buys until it reaches $s = 10$ to get the large reward of +100.\n",
    "    - The cumulative discounted reward for reaching $s = 10$ is:\n",
    "      $$G_{\\text{stock}} = 100 \\cdot \\gamma^6$$\n",
    "    - This is because the reward of +100 is received after 7 steps of buying (zero rewards along the way), so the total gain is discounted by $\\gamma^6$.\n",
    "\n",
    "2. **Policy for not filling the inventory (selling first, then alternating between buying and selling)**:\n",
    "    - In this case, the agent first sells down to $s = 0$, collects small rewards of +1, and then alternates between buying and selling to collect additional rewards.\n",
    "    - The cumulative discounted reward for selling and not restocking the inventory is:\n",
    "      $$G_{\\text{no\\_stock}} = 1 + \\gamma + \\gamma^2 + 0 + \\gamma^4 + 0 + \\gamma^6 + \\cdots$$\n",
    "    - The important thing to notice here is that rewards are collected every other step, and this forms a geometric series. Therefore, using the summation formula for a geometric series, the total reward from selling and alternating is:\n",
    "      $$G_{\\text{no\\_stock}} = \\gamma + \\frac{1}{1 - \\gamma^2}$$\n",
    "\n",
    "#### Condition for avoiding fully stocking the inventory:\n",
    "We want to find the values of $\\gamma$ such that the reward from not stocking the inventory is larger than the reward from fully stocking the inventory:\n",
    "$$G_{\\text{no\\_stock}} > G_{\\text{stock}}$$\n",
    "This results in the inequality:\n",
    "$$\\gamma + \\frac{1}{1 - \\gamma^2} > 100 \\cdot \\gamma^6$$\n",
    "Multiply both sides by $(1 - \\gamma^2)$ to eliminate the denominator:\n",
    "$$(\\gamma - \\gamma^3) + 1 > 100 \\cdot \\gamma^6 \\cdot (1 - \\gamma^2)$$\n",
    "Next, expand the terms:\n",
    "$$1 - \\gamma^3 + \\gamma > 100 \\cdot \\gamma^6 - 100 \\cdot \\gamma^8$$\n",
    "Rearrange the terms to move everything to one side:\n",
    "$$100 \\cdot \\gamma^8 - 100 \\cdot \\gamma^6 - \\gamma^3 + \\gamma + 1 > 0$$\n",
    "This is the expression we need to solve.\n",
    "\n",
    "#### Solving the inequality:\n",
    "\n",
    "![pics/P4d.png](pics/P4d.png)\n",
    "\n",
    "The graph shows the function:\n",
    "$$f(x) = 100 \\cdot x^8 - 100 \\cdot x^6 - x^3 + x + 1$$\n",
    "By plotting the graph (as shown in the image), we find that the root of the inequality is approximately:\n",
    "$$\\gamma \\approx 0.51554$$\n",
    "Thus, the range of $\\gamma$ such that the optimal policy **never fully stocks the inventory** is:\n",
    "$$\\gamma \\in (0, 0.51554)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "\n",
    "Based on your answer to part (a) above, choose values of $H$ such that, after obtaining the optimal policy using the VI and PI algorithms, the optimal policy exhibits the following characteristics:  \n",
    "- The optimal policy only buys.  \n",
    "- The optimal policy only sells.  \n",
    "- The optimal policy performs both buying and selling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, reward_func, transition_func, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.reward_func = reward_func\n",
    "        self.transition_func = transition_func\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transition_func[state][action]\n",
    "    \n",
    "    def get_reward(self, state, action, new_state):\n",
    "        return self.reward_func[state][action][new_state]\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor\n",
    "    \n",
    "    # Generate a random episode based on the current ε-greedy policy\n",
    "    def generate_episode(self, start_state, policy, epsilon):\n",
    "        episode = []\n",
    "        state = start_state\n",
    "\n",
    "        while state != 'S10':  # Until we reach the terminal state S10\n",
    "            action = self.get_action_for_state_epsilon_greedy(state, policy, epsilon)\n",
    "            transitions = self.get_transitions(state, action)\n",
    "            new_state, prob = random.choices(transitions, [prob for _, prob in transitions])[0]\n",
    "            reward = self.get_reward(state, action, new_state)\n",
    "            episode.append((state, action, reward, new_state))\n",
    "            state = new_state\n",
    "        \n",
    "        return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['S0', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10']\n",
    "actions = ['buy', 'sell']\n",
    "discount_factor = 0.9   # Lambda = 0.9\n",
    "\n",
    "# Reward function R(s, a) -> `terminal (+1/-1)` and `0` elsewhere\n",
    "reward_func = {\n",
    "    'S0': {'buy': {'S1': 0}, 'sell': {'S0': 0}},\n",
    "    'S1': {'buy': {'S2': 0}, 'sell': {'S0': 1}},\n",
    "    'S2': {'buy': {'S3': 0}, 'sell': {'S1': 1}},\n",
    "    'S3': {'buy': {'S4': 0}, 'sell': {'S2': 1}},\n",
    "    'S4': {'buy': {'S5': 0}, 'sell': {'S3': 1}},\n",
    "    'S5': {'buy': {'S6': 0}, 'sell': {'S4': 1}},\n",
    "    'S6': {'buy': {'S7': 0}, 'sell': {'S5': 1}},\n",
    "    'S7': {'buy': {'S8': 0}, 'sell': {'S6': 1}},\n",
    "    'S8': {'buy': {'S9': 0}, 'sell': {'S7': 1}},\n",
    "    'S9': {'buy': {'S10': 100}, 'sell': {'S8': 1}},\n",
    "    'S10': {'buy': {'S10': 0}, 'sell': {'S10': 0}}\n",
    "}\n",
    "\n",
    "# Set the transition probability matrix P(s' | s, a)\n",
    "# Transitions represented as P[s_current][a] = list of possible (s_next, prob).\n",
    "transition_func = {\n",
    "    'S0': {'buy': [('S1', 1)], 'sell': [('S0', 1)]},\n",
    "    'S1': {'buy': [('S2', 1)], 'sell': [('S0', 1)]},\n",
    "    'S2': {'buy': [('S3', 1)], 'sell': [('S1', 1)]},\n",
    "    'S3': {'buy': [('S4', 1)], 'sell': [('S2', 1)]},\n",
    "    'S4': {'buy': [('S5', 1)], 'sell': [('S3', 1)]},\n",
    "    'S5': {'buy': [('S6', 1)], 'sell': [('S4', 1)]},\n",
    "    'S6': {'buy': [('S7', 1)], 'sell': [('S5', 1)]},\n",
    "    'S7': {'buy': [('S8', 1)], 'sell': [('S6', 1)]},\n",
    "    'S8': {'buy': [('S9', 1)], 'sell': [('S7', 1)]},\n",
    "    'S9': {'buy': [('S10', 1)], 'sell': [('S8', 1)]},\n",
    "    'S10': {'buy': [('S10', 1)], 'sell': [('S10', 1)]}\n",
    "}\n",
    "\n",
    "# Starting state is S3\n",
    "start_state = 'S3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(states, actions, reward_func, transition_func, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### M.C. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_value_iteration(mdp, start_state, num_episodes, horizon):\n",
    "    # Initialize value function V(s) for all states (initially set to 0)\n",
    "    V = {state: 0.0 for state in mdp.get_states()}\n",
    "    \n",
    "    # Initialize returns list for all states to keep track of all values observed\n",
    "    returns = {state: [] for state in mdp.get_states()}\n",
    "    \n",
    "    # Monte Carlo simulation for a fixed number of episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Generate an episode starting from the start_state\n",
    "        state = start_state\n",
    "        episode_history = []  # To store the (state, action, reward) for each transition in episode\n",
    "        \n",
    "        # Simulate an episode of up to 'horizon'\n",
    "        for t in range(horizon):\n",
    "            # Randomly select an action -- assuming exploring start (radom policy)\n",
    "            action = random.choice(mdp.get_actions(state))\n",
    "            \n",
    "            # Get transitions and rewards for that action in the current state\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            new_state, prob = transitions[0]  # Assuming deterministic with prob = 1 for our model\n",
    "            \n",
    "            reward = mdp.get_reward(state, action, new_state)\n",
    "            \n",
    "            # Append this step to the episode history\n",
    "            episode_history.append((state, action, reward))\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = new_state\n",
    "            \n",
    "            # If we encounter terminal state S10, break\n",
    "            if state == 'S10':\n",
    "                break\n",
    "\n",
    "        first_visited_states = set()  # Track visited states in this episode\n",
    "\n",
    "        for (state, action, reward) in episode_history:\n",
    "            # Skip if we have already visited this state in the episode\n",
    "            if state in first_visited_states:\n",
    "                continue\n",
    "            first_visited_states.add(state)\n",
    "        \n",
    "        # Now we have the entire episode, calculate the return and update values\n",
    "        G = 0  # Cumulative discounted reward\n",
    "        for step in reversed(episode_history):\n",
    "            state, action, reward = step\n",
    "            G = reward + mdp.get_discount_factor() * G  # Update G with rewards and discount\n",
    "\n",
    "            if state not in first_visited_states:  # First-visit condition\n",
    "                returns[state].append(G)  # Keep track of returns for this state\n",
    "\n",
    "                # Update value function by averaging all returns for the state\n",
    "                V[state] = np.mean(returns[state])\n",
    "    \n",
    "    return V\n",
    "\n",
    "def compute_optimal_policy(mdp, value_function):\n",
    "    policy = {}\n",
    "    \n",
    "    # For each state, determine the best action based on the value function\n",
    "    for state in mdp.get_states():\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        # Iterate over all possible actions in the current state\n",
    "        for action in mdp.get_actions(state):\n",
    "            action_value = 0  # Expected value of taking this action\n",
    "            \n",
    "            # Loop over possible transitions for action in this state\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            for new_state, prob in transitions:\n",
    "                reward = mdp.get_reward(state, action, new_state)\n",
    "                action_value += prob * (reward + mdp.get_discount_factor() * value_function[new_state])\n",
    "            \n",
    "            # Track the action that gives the highest expected return\n",
    "            if action_value > best_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        \n",
    "        # Save the best action for this state\n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function V(s) after Monte Carlo iterations:\n",
      "V(S0) = 0.00\n",
      "V(S1) = 0.00\n",
      "V(S2) = 0.00\n",
      "V(S3) = 0.00\n",
      "V(S4) = 0.00\n",
      "V(S5) = 0.00\n",
      "V(S6) = 0.00\n",
      "V(S7) = 0.00\n",
      "V(S8) = 0.00\n",
      "V(S9) = 0.00\n",
      "V(S10) = 0.00\n",
      "Optimal Policy for each state:\n",
      "State S0: Take action 'buy'\n",
      "State S1: Take action 'sell'\n",
      "State S2: Take action 'sell'\n",
      "State S3: Take action 'sell'\n",
      "State S4: Take action 'sell'\n",
      "State S5: Take action 'sell'\n",
      "State S6: Take action 'sell'\n",
      "State S7: Take action 'sell'\n",
      "State S8: Take action 'sell'\n",
      "State S9: Take action 'buy'\n",
      "State S10: Take action 'buy'\n"
     ]
    }
   ],
   "source": [
    "# Horizon (number of steps per episode simulation)\n",
    "horizon = 7\n",
    "\n",
    "# Number of episodes to simulate\n",
    "num_episodes = 5000\n",
    "\n",
    "# Starting state is S3\n",
    "start_state = 'S3'\n",
    "\n",
    "# Run Monte Carlo Value Iteration\n",
    "values = monte_carlo_value_iteration(mdp, start_state, num_episodes, horizon)\n",
    "\n",
    "# Print Value function\n",
    "print(\"Estimated Value Function V(s) after Monte Carlo iterations:\")\n",
    "for state, value in values.items():\n",
    "    print(f\"V({state}) = {value:.2f}\")\n",
    "\n",
    "# Compute optimal policy based on value function\n",
    "optimal_policy = compute_optimal_policy(mdp, values)\n",
    "\n",
    "# Print the optimal policy for each state\n",
    "print(\"Optimal Policy for each state:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State {state}: Take action '{action}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### M.C. Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_evaluation(mdp, policy, num_episodes, start_state, horizon, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Runs Monte Carlo policy evaluation for a fixed policy and returns the updated value function.\n",
    "    \"\"\"\n",
    "    V = {state: 0.0 for state in mdp.get_states()}  # Initialize value function\n",
    "    returns = {state: [] for state in mdp.get_states()}  # Track the returns seen\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = random.choice(mdp.get_states())  # Random starting state (could be set as needed)\n",
    "        # state = start_state  # Fixed starting state for all episodes\n",
    "        \n",
    "        episode_history = []  # To store the (state, action, reward) for each transition in episode\n",
    "\n",
    "        # Simulate an episode according to the current policy\n",
    "        for t in range(horizon):\n",
    "            # action = policy[state]  # Always follow the given policy\n",
    "            # action = random.choice(mdp.get_actions(state))\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                # Exploration: Choose a random action\n",
    "                action = random.choice(mdp.get_actions(state))\n",
    "            else:\n",
    "                # Exploitation: Follow the given policy\n",
    "                action = policy[state]\n",
    "            \n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            new_state, prob = transitions[0]  # Assuming deterministic transitions\n",
    "            \n",
    "            reward = mdp.get_reward(state, action, new_state)\n",
    "            episode_history.append((state, action, reward))  # Log the (state, action, reward)\n",
    "            \n",
    "            state = new_state\n",
    "            if state == 'S10':  # Terminal state\n",
    "                break\n",
    "        \n",
    "        # Calculate returns in a backward manner\n",
    "        G = 0  # Start with zero return\n",
    "        i = 0\n",
    "\n",
    "        # for step in reversed(episode_history):\n",
    "        for step in episode_history[::-1]:\n",
    "            state, action, reward = step\n",
    "            G = reward + mdp.get_discount_factor() * G  # Update return G\n",
    "\n",
    "            if state not in episode_history[::-1][i+1:]:  # First-visit condition\n",
    "                returns[state].append(G)  # Append the new return for this state\n",
    "                V[state] = np.mean(returns[state])  # Update the value function via returns averaging\n",
    "\n",
    "            i += 1\n",
    "    \n",
    "    return V\n",
    "\n",
    "def monte_carlo_policy_improvement(mdp, value_function):\n",
    "    \"\"\"\n",
    "    Performs policy improvement based on the given value function.\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    # Iterate over states to create the improved policy\n",
    "    for state in mdp.get_states():\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for action in mdp.get_actions(state):\n",
    "            action_value = 0\n",
    "            \n",
    "            # Calculate the expected value of taking this action in this state\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            for new_state, prob in transitions:\n",
    "                reward = mdp.get_reward(state, action, new_state)\n",
    "                action_value += prob * (reward + mdp.get_discount_factor() * value_function[new_state])\n",
    "            \n",
    "            if action_value > best_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        \n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def monte_carlo_policy_iteration(mdp, num_episodes, start_state, horizon):\n",
    "    \"\"\"\n",
    "    Monte Carlo Policy Iteration: iteratively performs policy evaluation and improvement.\n",
    "    \"\"\"\n",
    "    # Initialize the policy randomly -- we start with a random policy\n",
    "    policy = {state: random.choice(mdp.get_actions(state)) for state in mdp.get_states()}\n",
    "    \n",
    "    is_policy_stable = False\n",
    "    iteration = 0\n",
    "    \n",
    "    while not is_policy_stable:\n",
    "        iteration += 1\n",
    "        # print(f\"\\nPolicy Iteration {iteration}\")\n",
    "        \n",
    "        # Step 1: Policy Evaluation -> Estimate value function for the current policy\n",
    "        value_function = monte_carlo_policy_evaluation(mdp, policy, num_episodes, start_state, horizon)\n",
    "        # print(\"Value Function:\", value_function)\n",
    "        \n",
    "        # Step 2: Policy Improvement -> Improve the policy based on the current value function\n",
    "        new_policy = monte_carlo_policy_improvement(mdp, value_function)\n",
    "        # print(\"Policy:\", new_policy)\n",
    "\n",
    "        # Print final optimal policy and value function after convergence\n",
    "        # for state, action in policy.items():\n",
    "        #     print(f\"{state}: '{action}'\", end=', ')\n",
    "        # print()\n",
    "\n",
    "        # for state, value in value_function.items():\n",
    "        #     print(f\"{state} = {value:.2f}\", end=', ')\n",
    "        # print()\n",
    "\n",
    "        # Check if the policy has stabilized (i.e., no changes from the old policy)\n",
    "        if new_policy == policy:\n",
    "            is_policy_stable = True\n",
    "        else:\n",
    "            policy = new_policy  # Update the policy and continue\n",
    "    \n",
    "    return policy, value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m  \u001b[38;5;66;03m# Feel free to adjust this number for deeper training\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Perform Monte Carlo Policy Iteration\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m optimal_policy, optimal_value_function \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo_policy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print final optimal policy and value function after convergence\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOptimal Policy after Policy Iteration:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[117], line 96\u001b[0m, in \u001b[0;36mmonte_carlo_policy_iteration\u001b[1;34m(mdp, num_episodes, start_state, horizon)\u001b[0m\n\u001b[0;32m     92\u001b[0m iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# print(f\"\\nPolicy Iteration {iteration}\")\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Step 1: Policy Evaluation -> Estimate value function for the current policy\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m value_function \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo_policy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# print(\"Value Function:\", value_function)\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Step 2: Policy Improvement -> Improve the policy based on the current value function\u001b[39;00m\n\u001b[0;32m    100\u001b[0m new_policy \u001b[38;5;241m=\u001b[39m monte_carlo_policy_improvement(mdp, value_function)\n",
      "Cell \u001b[1;32mIn[117], line 47\u001b[0m, in \u001b[0;36mmonte_carlo_policy_evaluation\u001b[1;34m(mdp, policy, num_episodes, start_state, horizon, epsilon)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m episode_history[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:  \u001b[38;5;66;03m# First-visit condition\u001b[39;00m\n\u001b[0;32m     46\u001b[0m             returns[state]\u001b[38;5;241m.\u001b[39mappend(G)  \u001b[38;5;66;03m# Append the new return for this state\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m             V[state] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update the value function via returns averaging\u001b[39;00m\n\u001b[0;32m     49\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m V\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mjid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3465\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:188\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float16_result \u001b[38;5;129;01mand\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret)\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float16_result:\n\u001b[0;32m    190\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret \u001b[38;5;241m/\u001b[39m rcount)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Horizon (number of steps per episode simulation)\n",
    "horizon = 5\n",
    "\n",
    "# Number of episodes to evaluate the policy\n",
    "num_episodes = 500  # Feel free to adjust this number for deeper training\n",
    "\n",
    "# Perform Monte Carlo Policy Iteration\n",
    "optimal_policy, optimal_value_function = monte_carlo_policy_iteration(mdp, num_episodes, start_state, horizon)\n",
    "\n",
    "# Print final optimal policy and value function after convergence\n",
    "print(\"\\nOptimal Policy after Policy Iteration:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State {state}: Take action '{action}'\")\n",
    "\n",
    "print(\"\\nOptimal Value Function after Policy Iteration:\")\n",
    "for state, value in optimal_value_function.items():\n",
    "    print(f\"V({state}) = {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finite Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite Horizon Value Function and Policy:\n",
      "Time step 0:\n",
      "  S0: Value = 100.00, Best Action = buy\n",
      "  S1: Value = 100.00, Best Action = buy\n",
      "  S2: Value = 101.00, Best Action = buy\n",
      "  S3: Value = 101.00, Best Action = buy\n",
      "  S4: Value = 102.00, Best Action = buy\n",
      "  S5: Value = 102.00, Best Action = buy\n",
      "  S6: Value = 103.00, Best Action = buy\n",
      "  S7: Value = 103.00, Best Action = buy\n",
      "  S8: Value = 104.00, Best Action = buy\n",
      "  S9: Value = 104.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 1:\n",
      "  S0: Value = 4.00, Best Action = buy\n",
      "  S1: Value = 100.00, Best Action = buy\n",
      "  S2: Value = 100.00, Best Action = buy\n",
      "  S3: Value = 101.00, Best Action = buy\n",
      "  S4: Value = 101.00, Best Action = buy\n",
      "  S5: Value = 102.00, Best Action = buy\n",
      "  S6: Value = 102.00, Best Action = buy\n",
      "  S7: Value = 103.00, Best Action = buy\n",
      "  S8: Value = 103.00, Best Action = buy\n",
      "  S9: Value = 104.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 2:\n",
      "  S0: Value = 4.00, Best Action = buy\n",
      "  S1: Value = 4.00, Best Action = buy\n",
      "  S2: Value = 100.00, Best Action = buy\n",
      "  S3: Value = 100.00, Best Action = buy\n",
      "  S4: Value = 101.00, Best Action = buy\n",
      "  S5: Value = 101.00, Best Action = buy\n",
      "  S6: Value = 102.00, Best Action = buy\n",
      "  S7: Value = 102.00, Best Action = buy\n",
      "  S8: Value = 103.00, Best Action = buy\n",
      "  S9: Value = 103.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 3:\n",
      "  S0: Value = 3.00, Best Action = buy\n",
      "  S1: Value = 4.00, Best Action = buy\n",
      "  S2: Value = 4.00, Best Action = buy\n",
      "  S3: Value = 100.00, Best Action = buy\n",
      "  S4: Value = 100.00, Best Action = buy\n",
      "  S5: Value = 101.00, Best Action = buy\n",
      "  S6: Value = 101.00, Best Action = buy\n",
      "  S7: Value = 102.00, Best Action = buy\n",
      "  S8: Value = 102.00, Best Action = buy\n",
      "  S9: Value = 103.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 4:\n",
      "  S0: Value = 3.00, Best Action = buy\n",
      "  S1: Value = 3.00, Best Action = buy\n",
      "  S2: Value = 4.00, Best Action = buy\n",
      "  S3: Value = 4.00, Best Action = buy\n",
      "  S4: Value = 100.00, Best Action = buy\n",
      "  S5: Value = 100.00, Best Action = buy\n",
      "  S6: Value = 101.00, Best Action = buy\n",
      "  S7: Value = 101.00, Best Action = buy\n",
      "  S8: Value = 102.00, Best Action = buy\n",
      "  S9: Value = 102.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 5:\n",
      "  S0: Value = 2.00, Best Action = buy\n",
      "  S1: Value = 3.00, Best Action = buy\n",
      "  S2: Value = 3.00, Best Action = buy\n",
      "  S3: Value = 4.00, Best Action = buy\n",
      "  S4: Value = 4.00, Best Action = buy\n",
      "  S5: Value = 100.00, Best Action = buy\n",
      "  S6: Value = 100.00, Best Action = buy\n",
      "  S7: Value = 101.00, Best Action = buy\n",
      "  S8: Value = 101.00, Best Action = buy\n",
      "  S9: Value = 102.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 6:\n",
      "  S0: Value = 2.00, Best Action = buy\n",
      "  S1: Value = 2.00, Best Action = buy\n",
      "  S2: Value = 3.00, Best Action = buy\n",
      "  S3: Value = 3.00, Best Action = buy\n",
      "  S4: Value = 4.00, Best Action = sell\n",
      "  S5: Value = 4.00, Best Action = sell\n",
      "  S6: Value = 100.00, Best Action = buy\n",
      "  S7: Value = 100.00, Best Action = buy\n",
      "  S8: Value = 101.00, Best Action = buy\n",
      "  S9: Value = 101.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 7:\n",
      "  S0: Value = 1.00, Best Action = buy\n",
      "  S1: Value = 2.00, Best Action = buy\n",
      "  S2: Value = 2.00, Best Action = buy\n",
      "  S3: Value = 3.00, Best Action = sell\n",
      "  S4: Value = 3.00, Best Action = sell\n",
      "  S5: Value = 3.00, Best Action = sell\n",
      "  S6: Value = 3.00, Best Action = sell\n",
      "  S7: Value = 100.00, Best Action = buy\n",
      "  S8: Value = 100.00, Best Action = buy\n",
      "  S9: Value = 101.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 8:\n",
      "  S0: Value = 1.00, Best Action = buy\n",
      "  S1: Value = 1.00, Best Action = buy\n",
      "  S2: Value = 2.00, Best Action = sell\n",
      "  S3: Value = 2.00, Best Action = sell\n",
      "  S4: Value = 2.00, Best Action = sell\n",
      "  S5: Value = 2.00, Best Action = sell\n",
      "  S6: Value = 2.00, Best Action = sell\n",
      "  S7: Value = 2.00, Best Action = sell\n",
      "  S8: Value = 100.00, Best Action = buy\n",
      "  S9: Value = 100.00, Best Action = buy\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 9:\n",
      "  S0: Value = 0.00, Best Action = buy\n",
      "  S1: Value = 1.00, Best Action = sell\n",
      "  S2: Value = 1.00, Best Action = sell\n",
      "  S3: Value = 1.00, Best Action = sell\n",
      "  S4: Value = 1.00, Best Action = sell\n",
      "  S5: Value = 1.00, Best Action = sell\n",
      "  S6: Value = 1.00, Best Action = sell\n",
      "  S7: Value = 1.00, Best Action = sell\n",
      "  S8: Value = 1.00, Best Action = sell\n",
      "  S9: Value = 100.00, Best Action = buy\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 10:\n",
      "  S0: Value = 0.00, Best Action = -\n",
      "  S1: Value = 0.00, Best Action = -\n",
      "  S2: Value = 0.00, Best Action = -\n",
      "  S3: Value = 0.00, Best Action = -\n",
      "  S4: Value = 0.00, Best Action = -\n",
      "  S5: Value = 0.00, Best Action = -\n",
      "  S6: Value = 0.00, Best Action = -\n",
      "  S7: Value = 0.00, Best Action = -\n",
      "  S8: Value = 0.00, Best Action = -\n",
      "  S9: Value = 0.00, Best Action = -\n",
      "  S10: Value = 0.00, Best Action = -\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "def finite_horizon_value_iteration_with_policy(mdp, T):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.actions  # Same actions for all states\n",
    "    \n",
    "    # Initialize value functions for each time step t (V_t(s))\n",
    "    V = {t: {s: 0 for s in states} for t in range(T + 1)}  # V_T = 0 is terminal condition\n",
    "    policy = {t: {s: None for s in states} for t in range(T)}  # Initialize policy for each time t\n",
    "    \n",
    "    # Step backward from t = T-1 to t = 0\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        for state in states:\n",
    "            # Skip terminal state S10\n",
    "            if state == 'S10':\n",
    "                continue\n",
    "            \n",
    "            best_action = None\n",
    "            max_action_value = float('-inf')\n",
    "            \n",
    "            # Loop through all actions\n",
    "            for action in actions:\n",
    "                expected_value = 0\n",
    "                transitions = mdp.get_transitions(state, action)\n",
    "                \n",
    "                # Evaluate each transition based on next state's value function at time t+1\n",
    "                for next_state, prob in transitions:\n",
    "                    reward = mdp.get_reward(state, action, next_state)\n",
    "                    expected_value += prob * (reward + V[t + 1][next_state])\n",
    "                \n",
    "                # Find the best action for this state at time t\n",
    "                if expected_value > max_action_value:\n",
    "                    max_action_value = expected_value\n",
    "                    best_action = action\n",
    "            \n",
    "            # Update value function and policy\n",
    "            V[t][state] = max_action_value\n",
    "            policy[t][state] = best_action\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "# Finite horizon example with T = 5\n",
    "T = 10\n",
    "V_finite, policy_finite = finite_horizon_value_iteration_with_policy(mdp, T)\n",
    "\n",
    "print(\"Finite Horizon Value Function and Policy:\")\n",
    "for t in range(T + 1):\n",
    "    print(f\"Time step {t}:\")\n",
    "    for s in V_finite[t]:\n",
    "        action = policy_finite[t].get(s, '-') if t < T else '-'\n",
    "        print(f\"  {s}: Value = {V_finite[t][s]:.2f}, Best Action = {action}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite Horizon (Horizon: 5) - Start State: S3\n",
      "Optimal Value: 1.1101\n",
      "Optimal Policy: ['sell', 'sell', 'sell', 'sell', 'sell']\n",
      "\n",
      "Infinite Horizon - Start State: S3\n",
      "Optimal Value Function: {'S0': 0.10101, 'S1': 1.010101, 'S2': 1.10101, 'S3': 1.110101, 'S4': 1.11101, 'S5': 1.1111010000000001, 'S6': 1.11111, 'S7': 1.111111, 'S8': 10.0, 'S9': 100.0, 'S10': 0.0}\n",
      "Optimal Policy: {'S0': 'buy', 'S1': 'sell', 'S2': 'sell', 'S3': 'sell', 'S4': 'sell', 'S5': 'sell', 'S6': 'sell', 'S7': 'sell', 'S8': 'buy', 'S9': 'buy', 'S10': 'buy'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Finite Horizon Value Iteration\n",
    "def finite_horizon_value_iteration(mdp, start_state, horizon):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions(None) # Actions don't differ per state in this case\n",
    "    discount_factor = mdp.get_discount_factor()\n",
    "    \n",
    "    # Initialize value functions, one for each time step (horizon)\n",
    "    V = [{} for _ in range(horizon + 1)]\n",
    "    for h in range(horizon + 1):\n",
    "        for state in states:\n",
    "            V[h][state] = 0\n",
    "    \n",
    "    # Iterating backwards from the horizon to step 0\n",
    "    for h in range(horizon - 1, -1, -1):\n",
    "        for state in states:\n",
    "            V[h][state] = max(\n",
    "                sum(prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[h+1][next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action))\n",
    "                for action in actions\n",
    "            )\n",
    "    \n",
    "    # Return the computed values and optimal actions for the start state\n",
    "    pi = []\n",
    "    for h in range(horizon):\n",
    "        action_values = {}\n",
    "        for action in actions:\n",
    "            action_values[action] = sum(\n",
    "                prob * (mdp.get_reward(start_state, action, next_state) + discount_factor * V[h+1][next_state])\n",
    "                for next_state, prob in mdp.get_transitions(start_state, action)\n",
    "            )\n",
    "        optimal_action = max(action_values, key=action_values.get)\n",
    "        pi.append(optimal_action)\n",
    "    return V[0][start_state], pi\n",
    "\n",
    "discount_factor = 0.1\n",
    "start_state = 'S3'\n",
    "\n",
    "# Initialize the MDP\n",
    "mdp = MDP(states, actions, reward_func, transition_func, discount_factor)\n",
    "\n",
    "# Solve for finite horizon solution\n",
    "horizon = 5\n",
    "value, policy = finite_horizon_value_iteration(mdp, start_state, horizon)\n",
    "print(f\"Finite Horizon (Horizon: {horizon}) - Start State: {start_state}\")\n",
    "print(f\"Optimal Value: {value}\")\n",
    "print(f\"Optimal Policy: {policy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "\n",
    "In the infinite-horizon setting, obtain the optimal policy for $\\lambda = 0$, and compare it with your answer in part (g) above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Infinite Horizon Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite Horizon Value Iteration\n",
    "def infinite_horizon_value_iteration(mdp, epsilon=1e-5):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions(None)\n",
    "    discount_factor = mdp.get_discount_factor()\n",
    "    \n",
    "    # Initialize value function arbitrarily\n",
    "    V = {state: 0 for state in states}\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = V.copy()\n",
    "        \n",
    "        for state in states:\n",
    "            max_value = max(\n",
    "                sum(prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action))\n",
    "                for action in actions\n",
    "            )\n",
    "            new_V[state] = max_value\n",
    "            delta = max(delta, abs(V[state] - new_V[state]))\n",
    "        \n",
    "        V = new_V\n",
    "        \n",
    "        # Check for convergence\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    # Once we've converged, we can extract the optimal policy\n",
    "    policy = {}\n",
    "    for state in states:\n",
    "        action_values = {}\n",
    "        for action in actions:\n",
    "            action_values[action] = sum(\n",
    "                prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                for next_state, prob in mdp.get_transitions(state, action)\n",
    "            )\n",
    "        policy[state] = max(action_values, key=action_values.get)\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Infinite Horizon Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_horizon_policy_iteration(mdp, epsilon=1e-5):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions(None)\n",
    "    discount_factor = mdp.get_discount_factor()\n",
    "    \n",
    "    # Step 1: Initialize to a random policy\n",
    "    policy = {state: actions[0] for state in states}\n",
    "    \n",
    "    # Step 2: Initialize value function arbitrarily\n",
    "    V = {state: 0 for state in states}\n",
    "    \n",
    "    def policy_evaluation(policy, V):\n",
    "        \"\"\"\n",
    "        Evaluate the given policy by solving V(s) = sum over next states[T(s, pi(s), s') * (R(s, pi(s), s') + γ * V(s'))].\n",
    "        The function performs iterative evaluation until V converges.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_V = V.copy()\n",
    "            \n",
    "            for state in states:\n",
    "                action = policy[state]\n",
    "                new_V[state] = sum(\n",
    "                    prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action)\n",
    "                )\n",
    "                delta = max(delta, abs(V[state] - new_V[state]))\n",
    "            \n",
    "            V = new_V\n",
    "            \n",
    "            # Check for convergence\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "        \n",
    "        return V\n",
    "    \n",
    "    while True:\n",
    "        # Step 3: Policy Evaluation -> Compute V given current policy\n",
    "        V = policy_evaluation(policy, V)\n",
    "        \n",
    "        policy_stable = True\n",
    "        \n",
    "        # Step 4: Policy Improvement -> Greedily improve the policy\n",
    "        for state in states:\n",
    "            # Find the best action according to the current value function V\n",
    "            old_action = policy[state]\n",
    "            action_values = {}\n",
    "            \n",
    "            for action in actions:\n",
    "                action_values[action] = sum(\n",
    "                    prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action)\n",
    "                )\n",
    "            \n",
    "            # Choose the action that gives maximum value\n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            policy[state] = best_action\n",
    "            \n",
    "            # If the policy did change, we're not yet stable\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        # Step 5: Check if the policy is stable and terminate if it is\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MDP\n",
    "mdp = MDP(states, actions, reward_func, transition_func, discount_factor=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for infinite horizon solution\n",
    "value_vi, policy_vi = infinite_horizon_value_iteration(mdp)\n",
    "value_pi, policy_pi = infinite_horizon_policy_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+\n",
      "| State   | VI (0)   | PI (0)   |\n",
      "+=========+==========+==========+\n",
      "| S0      | buy      | buy      |\n",
      "+---------+----------+----------+\n",
      "| S1      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S2      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S3      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S4      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S5      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S6      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S7      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S8      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S9      | buy      | buy      |\n",
      "+---------+----------+----------+\n",
      "| S10     | buy      | buy      |\n",
      "+---------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    action_vi = policy_vi.get(state, 'N/A')\n",
    "    action_pi = policy_pi.get(state, 'N/A')\n",
    "    table_data.append([state, action_vi, action_pi])\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"State\", \"VI (0)\", \"PI (0)\"]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c\n",
    "\n",
    "In the infinite-horizon setting, obtain the optimal policy separately for $\\lambda = 0.1$ and $\\lambda = 0.9$, and compare them with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Infinite Horizon Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MDP\n",
    "mdp_1 = MDP(states, actions, reward_func, transition_func, discount_factor=0.1)\n",
    "mdp_9 = MDP(states, actions, reward_func, transition_func, discount_factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for infinite horizon solution\n",
    "value_vi_1, policy_vi_1 = infinite_horizon_value_iteration(mdp_1)\n",
    "value_pi_1, policy_pi_1 = infinite_horizon_policy_iteration(mdp_1)\n",
    "value_vi_9, policy_vi_9 = infinite_horizon_value_iteration(mdp_9)\n",
    "value_pi_9, policy_pi_9 = infinite_horizon_policy_iteration(mdp_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+-------------+------------+\n",
      "| State   | VI (0.1)   | PI (0.9)   |  VI (0.1)   | PI (0.9)   |\n",
      "+=========+============+============+=============+============+\n",
      "| S0      | buy        | buy        | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S1      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S2      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S3      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S4      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S5      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S6      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S7      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S8      | buy        | buy        | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S9      | buy        | buy        | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S10     | buy        | buy        | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    action_vi_1 = policy_vi_1.get(state, 'N/A')\n",
    "    action_pi_1 = policy_pi_1.get(state, 'N/A')\n",
    "    action_vi_9 = policy_vi_9.get(state, 'N/A')\n",
    "    action_pi_9 = policy_pi_9.get(state, 'N/A')\n",
    "    table_data.append([state, action_vi_1, action_pi_1, action_vi_9, action_pi_9])\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"State\", \"VI (0.1)\", \"PI (0.9)\", \" VI (0.1)\", \"PI (0.9)\"]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
