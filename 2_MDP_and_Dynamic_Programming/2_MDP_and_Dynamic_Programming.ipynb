{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA 2, Interactive Learning, Fall 2024\n",
    "- **Name**: Majid Faridfar\n",
    "- **Student ID**: 810199569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "What is the difference between reinforcement learning and supervised learning? Explain by providing two similar problems: one that requires reinforcement learning to solve, and another that can be solved with supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences:\n",
    "- How to learn? \n",
    "  - RL is based on an agent interacting with an environment. The agent takes actions, receives feedback in the form of rewards or penalties, and aims to maximize the cumulative reward over time. The feedback is sparse and often delayed, and there’s no explicit label or correct answer for each action.\n",
    "  - SL involves learning from labeled data where each input comes with a corresponding correct output (label). The model is trained to map inputs to outputs based on this labeled dataset.\n",
    "\n",
    "- What is the goal?\n",
    "  - The goal of RL is to learn a strategy (or policy) that will maximize the total reward over time. The agent learns through trial and error, exploring and exploiting the environment to improve its decisions.\n",
    "  - The goal of SL is to minimize the error between the predicted output and the true label, thereby learning a mapping function from inputs to outputs.\n",
    "\n",
    "Illustration (Maze):\n",
    "- A robot (agent) is placed in a maze (environment) and has to navigate from the start to the goal. The robot takes actions such as moving left, right, up, or down. Initially, it doesn’t know the best path to take and receives feedback in the form of rewards (e.g., +1 for reaching the goal, -1 for hitting a wall or making a wrong turn). The goal is to maximize the cumulative reward by learning which actions lead to the goal. The robot learns over time by exploring different paths, receiving feedback, and adjusting its strategy based on the rewards it receives.\n",
    "- Instead of having the robot learn by exploring the maze, you collect a dataset of maze configurations (states) and the correct sequence of moves (labels) that leads to the goal. For each configuration, you have a label indicating the correct next action (e.g., move left, move right, etc.). You train a model on this labeled dataset to predict the correct action for a given maze state. Once trained, the model can predict the correct move for any new maze configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "In an MDP problem, if the reward function undergoes a linear transformation, does the optimal policy change? (Provide a mathematical proof or a counterexample, and ignore the trivial case of multiplying by zero.) Does the answer to this question depend on whether the task is continuing or episodic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general linear transformation of a reward function $R(s, a)$ is:\n",
    "\n",
    "$$R'(s, a) = \\alpha R(s, a) + \\beta$$\n",
    "where $\\alpha > 0$ and $\\beta$ is a scalar.\n",
    "\n",
    "Here, we will show that when the reward function undergoes a linear transformation like this, the optimal policy does **not** change. This conclusion holds for both continuing and episodic tasks.\n",
    "\n",
    "### Understanding Optimal Policies:\n",
    "\n",
    "The goal in an MDP is to find an optimal policy $\\pi^*$ that maximizes the **expected cumulative reward**. Depending on whether the problem is episodic or continuing, this objective is either the total expected reward (episodic task) or the expected discounted cumulative reward (continuing task). We’ll focus on the discounted scenario, as the principles generalize well.\n",
    "\n",
    "In a discounted setting, the objective is to maximize the expected return,\n",
    "\n",
    "$$G_t = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) | s_t = s, a_t = a \\right]$$\n",
    "where $\\gamma$ is the discount factor such that $0 \\leq \\gamma \\leq 1$ (if $\\gamma$ is equal to $1$, then we are calculating the expected cumulative reward).\n",
    "\n",
    "We define the value function $V^\\pi(s)$ under a policy $\\pi$ as:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) \\middle| s_t = s \\right]$$\n",
    "\n",
    "Similarly, the action-value function or Q-value function $Q^\\pi(s, a)$ is:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) \\middle| s_t = s, a_t = a \\right]$$\n",
    "\n",
    "The optimal policy is the one that maximizes the value or Q-value function, satisfying:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n",
    "\n",
    "#### Effect on the Value Function:\n",
    "\n",
    "With the new transformed reward function $R'(s, a) = \\alpha R(s, a) + \\beta$, the new value function $V'^\\pi(s)$ under policy $\\pi$ becomes:\n",
    "\n",
    "$$V'^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R'(s_{t+k}, a_{t+k}) \\middle| s_t = s \\right]\n",
    "         = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k (\\alpha R(s_{t+k}, a_{t+k}) + \\beta) \\middle| s_t = s \\right]$$\n",
    "\n",
    "This expands into:\n",
    "\n",
    "$$V'^\\pi(s) = \\alpha \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) \\middle| s_t = s \\right]\n",
    "         + \\beta \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k \\middle| s_t = s \\right]$$\n",
    "\n",
    "Shortening the notation, we have:\n",
    "\n",
    "$$V'^\\pi(s) = \\alpha V^\\pi(s) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "where the sum $\\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}$ because it is a geometric series.\n",
    "\n",
    "#### Effect on the Q-Value Function:\n",
    "\n",
    "Similarly, the transformed Q-value function would be:\n",
    "\n",
    "$$Q'^\\pi(s, a) = \\alpha Q^\\pi(s, a) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "### Does the Optimal Policy Change?\n",
    "\n",
    "Answer is no. The optimal policy $\\pi^*(s)$ is determined by selecting the action $a$ that maximizes the Q-value function:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n",
    "\n",
    "When applying the linear transformation, the transformed Q-value function satisfies:\n",
    "\n",
    "$$Q'^\\pi(s, a) = \\alpha Q^\\pi(s, a) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "Since $\\alpha > 0$, this transformation preserves the order of the Q-values. That is, if action $a_1$ was better than action $a_2$ before the transformation (i.e., $Q^*(s, a_1) > Q^*(s, a_2)$), it will remain better after the transformation because the transformation is a positive linear scaling and a constant shift, both of which do not affect the relative ordering of numbers:\n",
    "\n",
    "$$Q'^\\pi(s, a_1) = \\alpha Q^\\pi(s, a_1) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "$$Q'^\\pi(s, a_2) = \\alpha Q^\\pi(s, a_2) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "Thus, $Q^\\pi(s, a_1) > Q^\\pi(s, a_2) \\Rightarrow Q'^\\pi(s, a_1) > Q'^\\pi(s, a_2)$.\n",
    "\n",
    "Since the relative rankings of the Q-values are preserved, the optimal action $\\arg\\max_a Q^*(s, a)$ does not change. Therefore, the optimal policy remains the same.\n",
    "\n",
    "The answer does not depend on whether the task is continuing or episodic. In an episodic task, you would also be maximizing the expected return, but limited to a certain number of steps before the episode ends. The reasoning around the transformation of the reward function and its impact on the Q-value functions still holds because the transformation is applied uniformly across the rewards, and the policy that maximizes value remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Assume a robot operates in a grid environment as follows. In each episode, the robot starts in one of the cells in the bottom row (with an equal probability of starting in each cell). The robot can move left, right, or up. If the robot chooses to move in a direction where there is a wall, it remains in place. If the robot enters one of the green cells, the episode ends.\n",
    "\n",
    "![pics/P3.png](pics/P3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1\n",
    "\n",
    "The robot knows the grid environment completely and is aware of its current location at any moment, using this information to make decisions. The goal is for the robot to reach the second row and enter one of the green cells. If the robot enters a green cell, it receives a reward of +1 (and the episode ends), and if it moves from one blue cell to another blue cell, it receives a reward of 0. For this task, $\\lambda = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "Can the defined task be represented by an MDP? If not, explain why; if yes, fully specify the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this task can be defined by an MDP. To fully specify the Markov Decision Process (MDP) for this task, we need to define **States (S)**, **Actions (A)**, **Transition probabilities (P(s'|s,a))**, **Rewards (R(s,a))**, and **Discount Factor (λ)**.\n",
    "\n",
    "##### States (S):\n",
    "The states are the cells in the grid:\n",
    "- S1, S2, S3, S4, S5, S6, S7 represent the blue cells.\n",
    "- T1, T3, and T5 are the green terminal cells (absorbing states).\n",
    "\n",
    "So the state space is:\n",
    "$$S = \\{S1, S2, S3, S4, S5, S6, S7, T1, T3, T5\\}$$\n",
    "\n",
    "##### Actions (A):\n",
    "The robot has three possible actions in the bottom row: **Up (U)**, **Left (L)**, **Right (R)**. However, when the robot is in the terminal green cells T1, T3, T5, the episode ends, and no further actions are available.\n",
    "\n",
    "Thus, the action set is:\n",
    "$$A = \\{\\text{Up}, \\text{Left}, \\text{Right}\\}$$\n",
    "\n",
    "##### Transition Probabilities (P(s' | s, a)):\n",
    "The robot transitions deterministically (we assume that there is no stochastic behavior, since it is not mentioned in the problem), so the probability of transitioning from one state to another depends on whether the robot encounters a wall.\n",
    "\n",
    "- Moving \"Left\" in state S1 will not change the state.\n",
    "- Moving \"Up\" in states S2, S4, S6 will move the robot into the green cells (and stop the episode), while in other states will not change the state.\n",
    "- Moving \"Right\" in state S7 will not change the state.\n",
    "\n",
    "Here are the key deterministic transitions:\n",
    "\n",
    "- **Up movement**:\n",
    "  - $P(T1 | S2, U) = 1$\n",
    "  - $P(T3 | S4, U) = 1$\n",
    "  - $P(T5 | S6, U) = 1$\n",
    "  - Other cases: The robot stays in the current state if \"Up\" is attempted from a cell without a green terminal cell above it.\n",
    "\n",
    "- **Left movement**:\n",
    "  - $P(S1 | S2, L) = 1$\n",
    "  - $P(S2 | S3, L) = 1$\n",
    "  - $P(S3 | S4, L) = 1$\n",
    "  - $P(S4 | S5, L) = 1$\n",
    "  - $P(S5 | S6, L) = 1$\n",
    "  - $P(S6 | S7, L) = 1$\n",
    "  - Other cases: The robot stays in the current state if \"Left\" is attempted from  S1.\n",
    "\n",
    "- **Right movement**:\n",
    "  - $P(S2 | S1, R) = 1$\n",
    "  - $P(S3 | S2, R) = 1$\n",
    "  - $P(S4 | S3, R) = 1$\n",
    "  - $P(S5 | S4, R) = 1$\n",
    "  - $P(S6 | S5, R) = 1$\n",
    "  - $P(S7 | S6, R) = 1$\n",
    "  - Other cases: The robot stays in the current state if \"Right\" is attempted from  S7.\n",
    "\n",
    "##### Rewards (R(s,a)):\n",
    "The robot receives:\n",
    "- A reward of **+1** upon entering a terminal green cell (T1, T3, T5).\n",
    "- A reward of **0** for any movement from one blue cell to another blue cell.\n",
    "\n",
    "Thus, the rewards are:\n",
    "- $R(s' | s, U) = 1$ if the robot moves from S2 to T1, S4 to T3, or S6 to T5.\n",
    "- $R(s' | s, a) = 0$ for any other movement.\n",
    "\n",
    "For example:\n",
    "- $R(T1 | S2, U) = 1$,\n",
    "- $R(T3 | S4, U) = 1$,\n",
    "- $R(T5 | S6, U) = 1$,\n",
    "- $R(S2 | S3, L) = 0$,\n",
    "- $R(S6 | S5, R) = 0$, etc.\n",
    "\n",
    "##### Discount Factor $λ$:\n",
    "The **discount factor** is $λ = 0.9$. This means that rewards received in future states are weighted by a factor of 0.9 for every step into the future. It reflects the agent's preference for immediate rewards over delayed ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "How many optimal deterministic policies exist for solving this task? Appropriately express $\\pi(s)$ for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the optimal policy, we go state by state and analyze the optimal actions:\n",
    "\n",
    "1. **S1**: The only possible optimal action here is to move **right**, since other actions are blocked by a wall. Thus, for $\\pi(S1)$:\n",
    "$$\\pi(S1) = \\text{R}$$\n",
    "- Hence, there’s only **one optimal action** in S1.\n",
    "\n",
    "2. **S2**: The optimal way to immediately end the episode and earn a reward is to move **Up** into the green terminal state T1. Thus:\n",
    "$$\\pi(S2) = \\text{U}$$\n",
    "- Hence, there’s only **one optimal action** in S2.\n",
    "\n",
    "3. **S3**: Moving **Up** is not blocked by the above wall. From S3, the robot can either move **left** to S2, letting it access the green cell T1, or move **right** to S4, potentially heading toward T3. Since rewards are the same (all terminal cells give +1 and actions cost 0), **both left and right are equally optimal** choices. Therefore:\n",
    "$$\\pi(S3) = \\text{L or R}$$\n",
    "- Hence, there are **two optimal actions** in S3.\n",
    "\n",
    "4. **S4**: The optimal action in state S4 is to move **Up** into T3 immediately to end the episode. Thus:\n",
    "$$\\pi(S4) = \\text{U}$$\n",
    "- Hence, there’s only **one optimal action** in S4.\n",
    "\n",
    "5. **S5**: Robot can move **right** toward S6 to reach the green terminal T5, or **left** to S4 and reach T3. Since both terminal cells T3 and T5 give the same reward, either direction is optimal. Therefore:\n",
    "$$\\pi(S5) = \\text{L or R}$$\n",
    "- Hence, there are **two optimal choices** in S5.\n",
    "\n",
    "6. **S6**: The optimal action is to go **Up** to T5 immediately. Thus:\n",
    "$$\\pi(S6) = \\text{U}$$\n",
    "- Hence, there’s only **one optimal action** in S6.\n",
    "\n",
    "7. **S7**: Since S7 has no option but to move **left** (right and up are blocked by a wall), the optimal policy is:\n",
    "$$\\pi(S7) = \\text{L}$$\n",
    "- Hence, there’s only **one optimal action** in S7.\n",
    "\n",
    "---\n",
    "\n",
    "- In **S1, S2, S4, S6, S7**, the robot has only **one optimal action**.\n",
    "- In **S3, S5**, the robot has **two optimal choices**: move left or right.\n",
    "\n",
    "Therefore, the total number of **optimal deterministic policies** is:\n",
    "\n",
    "$$\\text{Total optimal policies} = 2 \\times 2 = 4$$\n",
    "\n",
    "---\n",
    "\n",
    "![pics/P3S1b.png](pics/P3S1b.png)\n",
    "\n",
    "1. **Policy 1**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{L}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{L}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$\n",
    "   \n",
    "2. **Policy 2**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{L}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{R}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$\n",
    "\n",
    "3. **Policy 3**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{R}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{L}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$\n",
    "\n",
    "4. **Policy 4**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{R}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{R}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2\n",
    "\n",
    "The robot has no knowledge of the environment it's in. It has distance sensors on all four sides that tell it whether or not there is a wall next to it. For example, consider the images below.\n",
    "\n",
    "![pics/P3S2.png](pics/P3S2.png)\n",
    "\n",
    "Assume the objective is similar to the first scenario. In terms of key elements of an MDP, what has changed here? Can this problem still be represented by an MDP in a way that ultimately fulfills our objective? (Is the optimal policy what we want it to be?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the problem can still be represented by a **Markov Decision Process (MDP)**, as it fundamentally satisfies the requirements for an MDP. But there are key changes because the robot no longer knows exactly where it is within the environment.\n",
    "\n",
    "#### State Space (S)\n",
    "In general, states in this MDP are based on the robot’s sensory input: `{right, up, left, down}`. Each direction can either be blocked (0) or unblocked (1), thus defining the state space. Each state represents the robot’s local sensory information and position within the context of the grid. To simplify, we can define states $S_1, S_2, S_3, ...$ based on these sensory inputs.\n",
    "\n",
    "- **$S_1$**: Sensor reading `{right: 1, up: 0, left: 0, down: 0}`.\n",
    "- **$S_2$**: Sensor reading `{right: 0, up: 0, left: 1, down: 0}`.\n",
    "- **$S_3$**: Sensor reading `{right: 1, up: 0, left: 1, down: 0}`.\n",
    "- **$S_4$** (Ambiguous state): Sensor reading `{right: 1, up: 1, left: 1, down: 0}`.\n",
    "- **$S_5$** (terminal state): Sensor reading `{right: 0, up: 0, left: 0, down: 1}`. In can be either green or red.\n",
    "\n",
    "Thus, the **finite state space $S$** is:\n",
    "$$S = \\{ S_1, S_2, S_3, S_4, S_5 \\}$$\n",
    "\n",
    "#### Action Space (A)\n",
    "We define actions based on directional movements:\n",
    "- **$A_1$**: **Move Up**. The robot tries to move to a cell directly above in this action.\n",
    "- **$A_2$**: **Move Right**. The robot moves to the neighboring cell to the right.\n",
    "- **$A_3$**: **Move Left**. The robot moves to the neighboring cell to the left.\n",
    "  \n",
    "Thus, the **action space $A$** is:\n",
    "$$A = \\{ A_1, A_2, A_3 \\} = \\{ \\text{Up}, \\text{Right}, \\text{Left} \\}$$\n",
    "\n",
    "#### Transition Function (P(s', s, a))\n",
    "Using states $S_1 \\ldots S_5$ and actions $A_1, A_2, A_3$, consider each transition:\n",
    "\n",
    "| State Transition        | Action: Up | Action: Right | Action: Left |\n",
    "|-------------------------|------------------------|-----------------------------|----------------------------|\n",
    "| **$S_1$ → $S_1$** | 1 (blocked)            | 0                            | 1 (blocked)                  |\n",
    "| **$S_1$ → $S_4$** | 0                      | 1                            | 0                            |\n",
    "| **$S_2$ → $S_2$** | 1 (blocked)            | 1 (blocked)                  | 0                            |\n",
    "| **$S_2$ → $S_4$** | 0                      | 0                            | 1                            |\n",
    "| **$S_3$ → $S_3$** | 1 (blocked)            | 0                            | 0                            |\n",
    "| **$S_3$ → $S_4$** | 0                      | 1                            | 1                            |\n",
    "| **$S_4$ → $S_3$** | 0                      | $\\frac{2}{3}$ (probabilistic)| $\\frac{2}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_2$** | 0                      | $\\frac{1}{3}$ (probabilistic)| 0                            |\n",
    "| **$S_4$ → $S_1$** | 0                      | 0                            | $\\frac{1}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_5$** | 1 (terminal)           | 0                            | 0                            |\n",
    "\n",
    "Above:\n",
    "- **Blocked transitions** (e.g., \"Up\" when no upward movement is possible) have a **probability of 1** for staying in the same state.\n",
    "- Probabilistic transitions from $S_4$ to $S_1$, $S_2$, or $S_3$ when moving left or right.\n",
    "- Moving **up** from $S_4$ always leads to the terminal state $S_5$ with probability **1**.\n",
    "\n",
    "#### Reward Function (R(s, a))\n",
    "The reward structure also changes slightly. Instead of rewards depending on grid positions, the **reward is now a function of the sensor state** and the action taken:\n",
    "\n",
    "- If the robot takes the **Up** action in a sensor state where **Up is allowed** (i.e., there's no wall above), and this move leads to a terminal green cell, the robot gets a reward of **+1**.\n",
    "- For all other valid moves (where no immediate green state is reached), the robot receives **0 reward**.\n",
    "- If the robot tries an invalid action (e.g., moving into a wall), it stays in the same state and still receives a reward of **0**.\n",
    "  \n",
    "So, the reward function can be expressed as:\n",
    "\n",
    "$$R(s' | s, a) = \n",
    "     \\begin{cases}\n",
    "       +1 & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}, and a is \\text{Up}} \\\\\n",
    "       0 & \\text{Otherwise}\n",
    "     \\end{cases}$$\n",
    "\n",
    "#### Discount Factor (λ)\n",
    "The problem didn't change the discount factor, so $\\lambda$ remains at 0.9, meaning that future rewards are discounted slightly but still carry weight.\n",
    "\n",
    "---\n",
    "\n",
    "The optimal policy in the original problem was to **quickly reach the green terminal cells** by moving up when possible (if there was no wall above) and moving horizontally if necessary.\n",
    "\n",
    "In this new formulation, the **optimal policy would still aim to achieve the same goal**—the robot must use its sensor readings to figure out:\n",
    "- When it's best to move **Up** into a terminal cell,\n",
    "- How to move **Left** or **Right** to position itself below a terminal (green) cell if it's not already there.\n",
    "\n",
    "Thus, **in the optimal policy**, the robot will:\n",
    "- Move **Up** when its sensors indicate that moving up is allowed (i.e., in a sensor state like `{right: 1, up: 1, left: 1, down: 0}`).\n",
    "- Move **Left** or **Right** when needed to position itself for optimal transitions.\n",
    "\n",
    "Thus, the optimal policy will likely involve:\n",
    "$$\n",
    "\\pi(s) = \\begin{cases} \n",
    "\\text{Up} & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}} \\\\\n",
    "\\text{Right/Left} & \\text{If s = \\{right: 1, up: 0, left: 1, down: 0\\}} \\\\\n",
    "\\text{Right} & \\text{If s = \\{right: 1, up: 0, left: 0, down: 0\\}} \\\\\n",
    "\\text{Left} & \\text{If s = \\{right: 0, up: 0, left: 1, down: 0\\}} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So, the **optimal policy** still fulfills the ultimate goal by appropriately guiding the robot to the green terminal cells using sensor-guided movements. Therefore, under this formulation, the robot will still eventually learn the optimal way to achieve the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3\n",
    "\n",
    "Assume everything is the same as in Scenario 2, but our objective has changed, and we want the robot to enter only the two green cells on the left and right. If the robot enters the red cell in the figure below, it will receive a reward of -1, and if it enters one of the green cells, it will receive a reward of +1 (all other rewards are zero). For this task, $\\lambda = 0.9$.\n",
    "\n",
    "![pics/P3S3.png](pics/P3S3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "Can this problem still be represented by an MDP in a way that ultimately fulfills our objective? (Is the optimal policy what we want it to be?) If it can be represented, provide the desired optimal policy. If it cannot, explain what should be done, given the robot's input data, so that the robot can find an optimal policy that meets our objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can indeed be represented by a **Markov Decision Process (MDP)**. In MDPs, the environment's dynamics are expressed through states, actions, transitions, and rewards (the MDP is fully provided in the next section of question). The robot’s states can be represented as its position in the grid, its actions are left, right, and up, the rewards are received when entering specific \"trial\" cells (green +1 and the red -1), and the transitions outline how the robot moves based on its actions.\n",
    "\n",
    "However, **the optimal policy derived from this MDP may not be the one we desire** due to the robot's sensory limitation. From the robot’s perspective, when it starts in the bottom row, each of the bottom cells under the green or red cells looks the same until it receives feedback from the current state or the environment (i.e., reaching a terminal state). The robot does not know which upper cell will end up being a red or green cell because, from its starting position, all adjacent walls or sensors are identical. As a result, the optimal policy might make it behave similarly in cells beneath the red and green cells, which is not aligned with the desired behavior — the robot should avoid the cell under the red cell and prefer moving toward the cells under the green cells.\n",
    "\n",
    "##### Improved Distance Sensors as a Solution:\n",
    "To solve this issue, one method would be to modify the sensing capabilities of the robot. Currently, the robot’s sensors only tell it whether there’s a wall directly adjacent to it. If we **extend the sensor range** to allow the robot to detect walls that are a bit farther away (say, one extra cell ahead), the robot can then detect the presence of walls that define the boundary of the grid. This information can be crucial because the robot could infer whether it is in a corner (leftmost or rightmost cell) and indirectly deduce it is situated under one of the green target cells. When the robot detects no side walls, it knows it must be under the red cell, and thus the correct action would be to avoid moving up.\n",
    "\n",
    "With this extension to the robot’s sensors, the robot is able to recognize its position as either closer to the green terminal cells (left and right sides) or closer to the red terminal cell in the center. This extra sensory information helps the robot find an **optimal policy** that adheres to the task requirement of maximizing the number of times it reaches the green cells and minimizing the number of times it reaches the red cell.\n",
    "\n",
    "##### Alternative Approaches:\n",
    "\n",
    "There are alternative ways to resolve this problem without increasing the sensor range:\n",
    "\n",
    "1. **Learning from Experience via Reinforcement Learning**:\n",
    "    Instead of adjusting the robot's sensors, you can allow the robot to learn the environment through experience in the form of **Reinforcement Learning (RL)** algorithms such as Q-learning or SARSA. By using feedback (positive and negative rewards), the robot can gradually learn which states lead to favorable outcomes (green cells) and which states lead to undesirable outcomes (red cell) without needing upfront modifications to its sensors. Through exploration, it can distinguish the three bottom cells indirectly by learning the rewards that result from moving up from those cells.\n",
    "   \n",
    "   After a sufficient amount of exploration, the robot will learn that moving up from the cells below the green ones gives rewards, while moving up from the one below the red gives a penalty. As RL focuses on maximizing cumulative reward, the optimal policy will be to favor actions that lead to the green cells and avoid actions that lead to the red cell.\n",
    "\n",
    "2. **Pre-training the Robot**:\n",
    "   If time or reliability is a concern with the RL approach, you could **pre-train the robot** in a simulated environment to have it discover the favorable policy before deployment. By running multiple simulations in diverse grid configurations beforehand, the robot can learn to recognize patterns related to corner cells (green cells) and mid-grid cells (red cell). Once trained with a policy that favors green cells and avoids the red cell, it can be deployed with that knowledge.\n",
    "\n",
    "3. **Adding State Identifiers**:\n",
    "   Another solution is to introduce **state-based identifiers** (labels or distinct features) in the cells themselves. If each cell in the lower row had some distinguishing feature, such as a slight variation (visual cue, color, or numeric label), the robot’s input data could contain enough information to differentiate between the different regions under the green and red cells. This could be more practical than adjusting the robot’s sensory hardware for some real-world applications.\n",
    "\n",
    "##### Final Thoughts:\n",
    "\n",
    "1. **Distance Sensors Enhancement**: Extending the sensors to perceive walls that are farther away allowing the robot to infer its position with better accuracy.\n",
    "\n",
    "2. **Reinforcement Learning**: Allowing the robot to learn over time what the rewards of different states and actions are will naturally lead it to avoid the red cell over many episodes.\n",
    "\n",
    "3. **Pre-training**: Using simulations to help the robot develop an appropriate policy before it encounters the actual environment.\n",
    "\n",
    "4. **State Identifiers**: Each bottom row cell can have distinguishing features that make differentiation possible with existing sensors.\n",
    "\n",
    "In summary, while this problem is representable as an MDP, the optimal policy won't directly reflect the task's requirements because of the limitations of the robot’s input data (sensors). Enhancing the sensors, investing in RL, or simply marking the states differently are viable ways to tune the robot's policy toward the task objective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "If you solve Scenario 3 using dynamic programming methods, what MDP does the optimal solution correspond to? Represent the states of this MDP with $s_j$ (for $j = 0, 1, \\dots$), and indicate which grid cells correspond to these states. Represent the actions with $a_i$ (for $i = 0, 1, 2, \\dots$) and specify which robot actions they correspond to. Define the probability matrix $P$ accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pics/P3S3b](pics/P3S3b.png)\n",
    "\n",
    "##### States $s_j$\n",
    "In general, states in this MDP are based on the robot’s sensory input: `{right, up, left, down}`. Each direction can either be blocked (0) or unblocked (1), thus defining the state space. Each state represents the robot’s local sensory information and position within the context of the grid. To simplify, we can define states $S_1, S_2, S_3, ...$ based on these sensory inputs.\n",
    "\n",
    "- **$S_1$**: Sensor reading `{right: 1, up: 0, left: 0, down: 0}`.\n",
    "- **$S_2$**: Sensor reading `{right: 0, up: 0, left: 1, down: 0}`.\n",
    "- **$S_3$**: Sensor reading `{right: 1, up: 0, left: 1, down: 0}`.\n",
    "- **$S_4$** (Ambiguous state): Sensor reading `{right: 1, up: 1, left: 1, down: 0}`.\n",
    "- **$S_5$** (terminal state): Sensor reading `{right: 0, up: 0, left: 0, down: 1}`. In can be either green or red.\n",
    "\n",
    "Thus, the **finite state space $S$** is:\n",
    "$$S = \\{ S_1, S_2, S_3, S_4, S_5 \\}$$\n",
    "\n",
    "##### Actions $a_i$\n",
    "We define actions based on directional movements:\n",
    "- **$A_1$**: **Move Up**. The robot tries to move to a cell directly above in this action.\n",
    "- **$A_2$**: **Move Right**. The robot moves to the neighboring cell to the right.\n",
    "- **$A_3$**: **Move Left**. The robot moves to the neighboring cell to the left.\n",
    "  \n",
    "Thus, the **action space $A$** is:\n",
    "$$A = \\{ A_1, A_2, A_3 \\} = \\{ \\text{Up}, \\text{Right}, \\text{Left} \\}$$\n",
    "\n",
    "##### Transition Function\n",
    "This part describes how the robot transitions between states given specific actions. We assume transitions are **deterministic** and depend on whether the action is allowable based on the current sensory readings.\n",
    "\n",
    "**Transition for Action $A_1$ (Up)**\n",
    "- **$S_1$ through $S_3$ → $S_1, S_2, S_3$** (No change)\n",
    "  - If the robot tries to move **Up** in states $S_1, S_2, S_3$ where **moving up is blocked**, the robot stays in the same state.\n",
    "  \n",
    "- **$S_4$ → $S_5$**\n",
    "  - If the robot moves **Up** from $S_4$, it reaches one of the terminal cells (green or red), hence transitioning to the terminal state $S_5$.\n",
    "\n",
    "**Transition for Action $A_2$ (Right)**\n",
    "- **$S_1$ → $S_4$**: Move from the far left $S_1$ into the ambiguous central position $S_4$.\n",
    "  \n",
    "- **$S_3$ → $S_4$**: Move within the middle. From $S_3$, moving right brings the robot to the ambiguous central position $S_4$.\n",
    "\n",
    "- **$S_4$ → $S_3$ or $S_2$**: Moving **right** from $S_4$ potentially takes the robot to $S_2$ (For $\\frac{1}{3}$ times-S6) or $S_3$ (For $\\frac{2}{3}$ times-S2 and S4).\n",
    "\n",
    "**Transition for Action $A_3$ (Left)**\n",
    "- **$S_2$ → $S_4$**: Move from the far right $S_2$ to the central ambiguous state $S_4$.\n",
    "  \n",
    "- **$S_3$ → $S_4$**: Move from a middle position to the ambiguous $S_4$.\n",
    "\n",
    "- **$S_4$ → $S_3$ or $S_1$**: Moving **left** from $S_4$ potentially takes the robot to $S_1$ (For $\\frac{1}{3}$ times-S2) or $S_3$ (For $\\frac{2}{3}$ times-S4 and S6).\n",
    "\n",
    "**Probability Matrix $P$**\n",
    "\n",
    "Using states $S_1 \\ldots S_5$ and actions $A_1, A_2, A_3$, consider each transition:\n",
    "\n",
    "| State Transition        | Action: Up ($A_1$) | Action: Right ($A_2$) | Action: Left ($A_3$) |\n",
    "|-------------------------|------------------------|-----------------------------|----------------------------|\n",
    "| **$S_1$ → $S_1$** | 1 (blocked)            | 0                            | 1 (blocked)                  |\n",
    "| **$S_1$ → $S_4$** | 0                      | 1                            | 0                            |\n",
    "| **$S_2$ → $S_2$** | 1 (blocked)            | 1 (blocked)                  | 0                            |\n",
    "| **$S_2$ → $S_4$** | 0                      | 0                            | 1                            |\n",
    "| **$S_3$ → $S_3$** | 1 (blocked)            | 0                            | 0                            |\n",
    "| **$S_3$ → $S_4$** | 0                      | 1                            | 1                            |\n",
    "| **$S_4$ → $S_3$** | 0                      | $\\frac{2}{3}$ (probabilistic)| $\\frac{2}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_2$** | 0                      | $\\frac{1}{3}$ (probabilistic)| 0                            |\n",
    "| **$S_4$ → $S_1$** | 0                      | 0                            | $\\frac{1}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_5$** | 1 (terminal)           | 0                            | 0                            |\n",
    "  \n",
    "Above:\n",
    "- **Blocked transitions** (e.g., \"Up\" when no upward movement is possible) have a **probability of 1** for staying in the same state.\n",
    "- Probabilistic transitions from $S_4$ to $S_1$, $S_2$, or $S_3$ when moving left or right.\n",
    "- Moving **up** from $S_4$ always leads to the terminal state $S_5$ with probability **1**.\n",
    "\n",
    "**Reward Function $R$**\n",
    "\n",
    "- If the robot takes the **Up** action in a sensor state where **Up is allowed** (i.e., there's no wall above), in $\\frac{2}{3}$ times it receives **+1** (entering green cells), and in $\\frac{1}{3}$ times, it receives **-1** (red cell).\n",
    "- For all other valid moves (where no immediate green state is reached), the robot receives **0 reward**.\n",
    "- If the robot tries an invalid action (e.g., moving into a wall), it stays in the same state and still receives a reward of **0**.\n",
    "  \n",
    "So, the reward function can be expressed as:\n",
    "\n",
    "$$R(s' | s, a) = \n",
    "     \\begin{cases}\n",
    "       +1 & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}, and a is \\text{Up}, with probability of } \\frac{2}{3} \\\\\n",
    "       -1 & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}, and a is \\text{Up}, with probability of } \\frac{1}{3} \\\\\n",
    "       0 & \\text{Otherwise}\n",
    "     \\end{cases}$$\n",
    "\n",
    "This MDP can indeed be efficiently solved using **dynamic programming** methods or value iteration once set up this way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "\n",
    "In scenario 3, implement **MDP** in the Python environment and determine the optimal policy using **value iteration** and **policy iteration**. If the obtained optimal policy is applied to the robot, what decision will it make in each part of the grid environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from hands_on.src.value_iteration import ValueIteration\n",
    "from hands_on.src.tabular_value_function import TabularValueFunction\n",
    "from hands_on.src.value_policy import ValuePolicy\n",
    "\n",
    "from hands_on.src.policy_iteration import PolicyIteration\n",
    "from hands_on.src.tabular_policy import TabularPolicy\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the mapping of actions to directions\n",
    "action_to_vector = {\n",
    "    \"Right\": (1, 0),  # (dx, dy) for right\n",
    "    \"Left\":  (-1, 0), # (dx, dy) for left\n",
    "    \"Up\":    (0, 1),  # (dx, dy) for up\n",
    "}\n",
    "\n",
    "# Define grid positions for states\n",
    "state_positions = {\n",
    "    \"S1\": (0, 0), \"S2\": (6, 0), \"S3\": (2, 0), \"S4_1\": (1, 0), \"S4_2\": (3, 0), \"S4_3\": (5, 0),\n",
    "    \"S5_1\": (1, 1), \"S5_2\": (3, 1), \"S5_3\": (5, 1)\n",
    "}\n",
    "\n",
    "# Create a function to plot the custom grid with actions\n",
    "def plot_custom_grid(policy, state_positions, states, actions):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    # Set bounds of the plot (limits)\n",
    "    ax.set_xlim(-1, 7)\n",
    "    ax.set_ylim(-1, 2)\n",
    "\n",
    "    # Plot the top row terminal S5T states\n",
    "    for state, position in state_positions.items():\n",
    "        if \"S5\" in state:\n",
    "            # Color the middle terminal state red, others green\n",
    "            color = 'red' if state == \"S5_2\" else 'green'\n",
    "            ax.add_patch(plt.Rectangle(position, 1, 1, facecolor=color, edgecolor='black'))\n",
    "            ax.text(position[0] + 0.5, position[1] + 0.5, \"S5\", color='white', fontsize=14, ha='center', va='center')\n",
    "\n",
    "    # Plot the bottom row regular states\n",
    "    bottom_states = {\"S1\": (0, 0), \"S4_1\": (1, 0), \"S3\": (2, 0), \"S4_2\": (3, 0), \"S3_2\": (4, 0), \"S4_3\": (5, 0), \"S2\": (6, 0)}\n",
    "        \n",
    "    for state_key, position in bottom_states.items():\n",
    "        state_name = state_key[:-2] if \"_\" in state_key else state_key   # Handle state names (like \"S4_2\" -> \"S4\")\n",
    "        ax.add_patch(plt.Rectangle(position, 1, 1, facecolor='skyblue', edgecolor='black'))\n",
    "        ax.text(position[0] + 0.5, position[1] + 0.5, state_name, color='black', fontsize=14, ha='center', va='center')\n",
    "\n",
    "        # Plot the action arrows according to the policy\n",
    "        action = policy[state_name]\n",
    "        dx, dy = action_to_vector[action]\n",
    "        \n",
    "        # Plot the action arrow for the state\n",
    "        ax.arrow(position[0] + 0.5, position[1] + 0.5, 0.25 * dx, 0.25 * dy, head_width=0.1, head_length=0.1, fc='white', ec='white')\n",
    "\n",
    "    # Remove ticks and format plot\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.title(\"Policy Visualization\")\n",
    "    plt.grid(False)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, epsilon=1e-5):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions(None)\n",
    "    discount_factor = mdp.get_discount_factor()\n",
    "    \n",
    "    # Step 1: Initialize to a random policy\n",
    "    policy = {state: actions[0] for state in states}\n",
    "    \n",
    "    # Step 2: Initialize value function arbitrarily\n",
    "    V = {state: 0 for state in states}\n",
    "    \n",
    "    def policy_evaluation(policy, V):\n",
    "        \"\"\"\n",
    "        Evaluate the given policy by solving V(s) = sum over next states[T(s, pi(s), s') * (R(s, pi(s), s') + γ * V(s'))].\n",
    "        The function performs iterative evaluation until V converges.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_V = V.copy()\n",
    "            \n",
    "            for state in states:\n",
    "                action = policy[state]\n",
    "                new_V[state] = sum(\n",
    "                    prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action)\n",
    "                )\n",
    "                delta = max(delta, abs(V[state] - new_V[state]))\n",
    "            \n",
    "            V = new_V\n",
    "            \n",
    "            # Check for convergence\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "        \n",
    "        return V\n",
    "    \n",
    "    while True:\n",
    "        # Step 3: Policy Evaluation -> Compute V given current policy\n",
    "        V = policy_evaluation(policy, V)\n",
    "        \n",
    "        policy_stable = True\n",
    "        \n",
    "        # Step 4: Policy Improvement -> Greedily improve the policy\n",
    "        for state in states:\n",
    "            # Find the best action according to the current value function V\n",
    "            old_action = policy[state]\n",
    "            action_values = {}\n",
    "            \n",
    "            for action in actions:\n",
    "                action_values[action] = sum(\n",
    "                    prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action)\n",
    "                )\n",
    "            \n",
    "            # Choose the action that gives maximum value\n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            policy[state] = best_action\n",
    "            \n",
    "            # If the policy did change, we're not yet stable\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        # Step 5: Check if the policy is stable and terminate if it is\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, epsilon=1e-5):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions(None)\n",
    "    discount_factor = mdp.get_discount_factor()\n",
    "    \n",
    "    # Initialize value function arbitrarily\n",
    "    V = {state: 0 for state in states}\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = V.copy()\n",
    "        \n",
    "        for state in states:\n",
    "            max_value = max(\n",
    "                sum(prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action))\n",
    "                for action in actions\n",
    "            )\n",
    "            new_V[state] = max_value\n",
    "            delta = max(delta, abs(V[state] - new_V[state]))\n",
    "        \n",
    "        V = new_V\n",
    "        \n",
    "        # Check for convergence\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    # Once we've converged, we can extract the optimal policy\n",
    "    policy = {}\n",
    "    for state in states:\n",
    "        action_values = {}\n",
    "        for action in actions:\n",
    "            action_values[action] = sum(\n",
    "                prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                for next_state, prob in mdp.get_transitions(state, action)\n",
    "            )\n",
    "        policy[state] = max(action_values, key=action_values.get)\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, transition_func, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_func = transition_func\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transition_func[state][action]\n",
    "    \n",
    "    def get_reward(self, state, action, new_state):\n",
    "        if state == \"S4\" and action == \"Up\":\n",
    "            return random.choices([1, -1], weights=[2, 1], k=1)[0]\n",
    "        return 0\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDP components\n",
    "states = ['S1', 'S2', 'S3', 'S4', 'S5']  # S5 is the terminal state\n",
    "actions = ['Up', 'Right', 'Left']\n",
    "discount_factor = 0.9   # Lambda = 0.9\n",
    "\n",
    "# Set the transition probability matrix P(s' | s, a)\n",
    "# Transitions represented as P[s_current][a] = list of possible (s_next, prob).\n",
    "transition_func = {\n",
    "    'S1': {'Up': [('S1', 1)], 'Right': [('S4', 1)], 'Left': [('S1', 1)]},       # S1 -> right leads to S4\n",
    "    'S2': {'Up': [('S2', 1)], 'Right': [('S2', 1)], 'Left': [('S4', 1)]},       # S2 -> left leads to S4\n",
    "    'S3': {'Up': [('S3', 1)], 'Right': [('S4', 1)], 'Left': [('S4', 1)]},       # S3 -> left/right leads to S4\n",
    "    'S4': {'Up': [('S5', 1)], 'Right': [('S2', 0.33), ('S3', 0.67)],            # S4 -> Up to terminal\n",
    "                              'Left': [('S1', 0.33), ('S3', 0.67)]},\n",
    "    'S5': {'Up': [('S5', 1)], 'Right': [('S5', 1)], 'Left': [('S5', 1)]}        # S5 is terminal (absorbing)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(states, actions, transition_func, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "vi_policies = {}\n",
    "vi_times = 0\n",
    "\n",
    "for i in range(iters):\n",
    "    time_start = time()\n",
    "    _, policy = value_iteration(mdp)\n",
    "    time_end = time()\n",
    "\n",
    "    policy_str = policy.__str__()\n",
    "\n",
    "    if policy_str not in vi_policies:\n",
    "        vi_policies[policy_str] = {'count': 1, 'policy': policy}\n",
    "    else:\n",
    "        vi_policies[policy_str]['count'] += 1\n",
    "\n",
    "    vi_time = time_end - time_start\n",
    "    vi_times += vi_time\n",
    "\n",
    "vi_times /= iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 447, 'policy': {'S1': 'Right', 'S2': 'Left', 'S3': 'Right', 'S4': 'Up', 'S5': 'Up'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 103, 'policy': {'S1': 'Up', 'S2': 'Up', 'S3': 'Up', 'S4': 'Right', 'S5': 'Up'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 219, 'policy': {'S1': 'Up', 'S2': 'Up', 'S3': 'Up', 'S4': 'Up', 'S5': 'Up'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 231, 'policy': {'S1': 'Right', 'S2': 'Left', 'S3': 'Right', 'S4': 'Right', 'S5': 'Up'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vi_policy = ValuePolicy(mdp, vi_values)\n",
    "\n",
    "for policy in vi_policies:\n",
    "    print(vi_policies[policy])\n",
    "    plot_custom_grid(vi_policies[policy]['policy'], state_positions, states, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "pi_policies = {}\n",
    "pi_times = 0\n",
    "\n",
    "for i in range(iters):\n",
    "    time_start = time()\n",
    "    _, policy = policy_iteration(mdp)\n",
    "    time_end = time()\n",
    "\n",
    "    policy_str = policy.__str__()\n",
    "\n",
    "    if policy_str not in pi_policies:\n",
    "        pi_policies[policy_str] = {'count': 1, 'policy': policy}\n",
    "    else:\n",
    "        pi_policies[policy_str]['count'] += 1\n",
    "\n",
    "    pi_time = time_end - time_start\n",
    "    pi_times += pi_time\n",
    "\n",
    "pi_times /= iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 89, 'policy': {'S1': 'Up', 'S2': 'Up', 'S3': 'Up', 'S4': 'Right', 'S5': 'Up'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 298, 'policy': {'S1': 'Up', 'S2': 'Up', 'S3': 'Up', 'S4': 'Up', 'S5': 'Up'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 613, 'policy': {'S1': 'Right', 'S2': 'Left', 'S3': 'Right', 'S4': 'Up', 'S5': 'Up'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for policy in pi_policies:\n",
    "    print(pi_policies[policy])\n",
    "    plot_custom_grid(pi_policies[policy]['policy'], state_positions, states, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison of VI and PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+------------+------------+-----------+------------+------------+\n",
      "| State   | VI (447)   | VI (103)   | VI (219)   | VI (231)   | PI (89)   | PI (298)   | PI (613)   |\n",
      "+=========+============+============+============+============+===========+============+============+\n",
      "| S1      | Right      | Up         | Up         | Right      | Up        | Up         | Right      |\n",
      "+---------+------------+------------+------------+------------+-----------+------------+------------+\n",
      "| S2      | Left       | Up         | Up         | Left       | Up        | Up         | Left       |\n",
      "+---------+------------+------------+------------+------------+-----------+------------+------------+\n",
      "| S3      | Right      | Up         | Up         | Right      | Up        | Up         | Right      |\n",
      "+---------+------------+------------+------------+------------+-----------+------------+------------+\n",
      "| S4      | Up         | Right      | Up         | Right      | Right     | Up         | Up         |\n",
      "+---------+------------+------------+------------+------------+-----------+------------+------------+\n",
      "| S5      | Up         | Up         | Up         | Up         | Up        | Up         | Up         |\n",
      "+---------+------------+------------+------------+------------+-----------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming `states` is a list of all states and \n",
    "# `actions` is a list of possible actions.\n",
    "\n",
    "headers = [\"State\"]\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "\n",
    "    t = [state]\n",
    "    for policy in vi_policies:\n",
    "        action = vi_policies[policy]['policy'].get(state, 'N/A')\n",
    "        vi_action = vi_policies[policy]['policy'].get(state, 'N/A')\n",
    "\n",
    "        headers.append(f\"VI ({vi_policies[policy]['count']})\")\n",
    "        t.append(action)\n",
    "\n",
    "    for policy in pi_policies:\n",
    "        action = pi_policies[policy]['policy'].get(state, 'N/A')\n",
    "        vi_action = pi_policies[policy]['policy'].get(state, 'N/A')\n",
    "\n",
    "        headers.append(f\"PI ({pi_policies[policy]['count']})\")\n",
    "        t.append(action)\n",
    "\n",
    "    table_data.append(t)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy time: 0.009132862091064453\n",
      "Value time: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Policy time: {pi_time}\")\n",
    "print(f\"Value time: {vi_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran VI and PI 1000 times, and tracked the number of occurrences of different policies for each algorithm. In VI, 4 different policies were found, whereas in PI, 3 unique policies were found. The time taken for each iteration has been provided, with PI having the duration around **0.00913 seconds** and VI's duration being shown as **0.0 seconds**, which is most likely a measurement limitation implying that its execution is very fast.\n",
    "\n",
    "1. **VI** found **4 policies** across **1000 iterations**, highlighting some variability (showing it's less deterministic than PI). It's extremely fast but may lead to slightly more unstable policies as iteration counts increase.\n",
    "   \n",
    "2. **PI** found **3 consistent policies** across **1000 iterations**, showing its more stable and deterministic nature. However, it's marginally slower, taking an average of **0.00913 seconds** per run.\n",
    "\n",
    "3. **Both algorithms effectively guide the actions toward the optimal policy that either maximizes reward (+1 for green cells) or minimizes punishment (-1 for red cells)**. In the given MDP, both methods tend to converge toward fairly similar optimal policies with minor differences in the earlier stages of convergence.\n",
    "\n",
    "---\n",
    "\n",
    "**Analysis of Results**\n",
    "\n",
    "1. **VI Policies and Iterations**:\n",
    "\n",
    "   - VI reached *4 different policies* across the 1000 iterations.\n",
    "   - Policies are stable and consistent for each number of iterations:\n",
    "     - In **447** iterations, the policy suggests: \n",
    "       - S1: Right, S2: Left, S3: Right, S4: Up, S5: Up\n",
    "     - In **103** iterations:\n",
    "       - S1: Up, S2: Up, S3: Up, S4: Right, S5: Up\n",
    "     - In **219** iterations:\n",
    "       - S1: Up, S2: Up, S3: Up, S4: Up, S5: Up\n",
    "     - In **231** iterations:\n",
    "       - S1: Right, S2: Left, S3: Right, S4: Right, S5: Up\n",
    "\n",
    "2. **PI Policies and Iterations**:\n",
    "\n",
    "   - PI reached *3 different policies* across the 1000 iterations.\n",
    "   - PI seems to be more deterministic as compared to VI (fewer variations in policies):\n",
    "     - In **89** iterations, the policy suggests: \n",
    "       - S1: Up, S2: Up, S3: Up, S4: Right, S5: Up\n",
    "     - In **298** iterations:\n",
    "       - S1: Up, S2: Left, S3: Up, S4: Right, S5: Up\n",
    "     - In **613** iterations:\n",
    "       - S1: Right, S2: Left, S3: Right, S4: Up, S5: Up\n",
    "\n",
    "3. **Converged Policies**:\n",
    "   \n",
    "   Both methods yielded some similar policies, but overall **PI seemed to converge to fewer policies** than VI, especially with higher iteration counts.\n",
    "\n",
    "   - **Convergence Insight**:\n",
    "     - In both VI and PI, one noticeable policy shows directions for most of the states pointing 'Up,' except in cases where transitioning to a specific state (S4) is more beneficial (green or red rewards are involved).\n",
    "     - The strategy to go 'Up' in all cases indicates that the agents are trying to either move toward the reward (green states) or avoid a penalty (red state) as quickly as possible.\n",
    "   \n",
    "4. **Comparing Policies for S1, S2, S3, S4, and S5**:\n",
    "   \n",
    "   - **S1 (Bottom-left):**\n",
    "     - The decision fluctuates between 'Up' and 'Right,' indicating that it's sometimes better to reach S4 or go toward S2.\n",
    "   \n",
    "   - **S2 (Bottom-right):**\n",
    "     - Both 'Left' and 'Up' actions appear across policies, showing indecisiveness between moving toward S1 or S4 for a neutral or a beneficial state.\n",
    "   \n",
    "   - **S3 and S4:**\n",
    "     - The general consensus across most runs is to move 'Up,' suggesting these states are either trying to reach the green cell or avoid the red. Occasionally, 'Right' is favored for S4.\n",
    "   \n",
    "   - **S5 (Top-middle):**\n",
    "     - Actions typically remain consistent toward 'Up,' driving towards the final green reward state. The consistency indicates clear optimal action.\n",
    "\n",
    "---\n",
    "\n",
    "**Time Analysis**\n",
    "\n",
    "- On average, Policy Iteration (PI) took **0.00913 seconds**, while **VI took 0.0 seconds** (indicating very quick computation, possibly nanoseconds or not measurable within the tool's precision).\n",
    "  \n",
    "- **PI is slightly slower than VI**:\n",
    "  - PI involves evaluating the value of a given policy and updating it until it converges, which may take longer as the policy is continuously compared.\n",
    "  - VI speeds up the process by directly iterating over state values until convergence, without as much fine-tuning as PI.\n",
    "  \n",
    "- **VI completes in fewer iterations overall but with more variance in results**:\n",
    "  - More policies were found in VI, which could suggest that it has more fluctuations in intermediate policy decisions before convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "\n",
    "Consider scenario 3 in a situation where the robot is aware of its location on the map. Once again, determine the optimal policy using the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the mapping of actions to directions\n",
    "action_to_vector = {\n",
    "    \"Right\": (1, 0),  # (dx, dy) for right\n",
    "    \"Left\":  (-1, 0), # (dx, dy) for left\n",
    "    \"Up\":    (0, 1),  # (dx, dy) for up\n",
    "}\n",
    "\n",
    "# Define grid positions for states\n",
    "state_positions = {\n",
    "    \"S1\": (0, 0), \"S2\": (1, 0), \"S3\": (2, 0), \"S4\": (3, 0), \"S5\": (4, 0), \"S6\": (5, 0), \"S7\": (6, 0),\n",
    "    \"T1\": (1, 1), \"T3\": (3, 1), \"T5\": (5, 1)\n",
    "}\n",
    "\n",
    "# Create a function to plot the custom grid with actions\n",
    "def plot_custom_grid(policy, state_positions, states, actions):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    # Set bounds of the plot (limits)\n",
    "    ax.set_xlim(-1, 7)\n",
    "    ax.set_ylim(-1, 2)\n",
    "\n",
    "    # Plot the top row terminal S5T states\n",
    "    for state, position in state_positions.items():\n",
    "        if \"T\" in state:\n",
    "            # Color the middle terminal state red, others green\n",
    "            color = 'red' if state == \"T3\" else 'green'\n",
    "            ax.add_patch(plt.Rectangle(position, 1, 1, facecolor=color, edgecolor='black'))\n",
    "            ax.text(position[0] + 0.5, position[1] + 0.5, \"T\", color='white', fontsize=14, ha='center', va='center')\n",
    "\n",
    "    # Plot the bottom row regular states\n",
    "    bottom_states = {\"S1\": (0, 0), \"S2\": (1, 0), \"S3\": (2, 0), \"S4\": (3, 0), \"S5\": (4, 0), \"S6\": (5, 0), \"S7\": (6, 0)}\n",
    "    \n",
    "    for state_key, position in bottom_states.items():\n",
    "        state_name = state_key   # Handle state names (like \"S4_2\" -> \"S4\")\n",
    "        ax.add_patch(plt.Rectangle(position, 1, 1, facecolor='skyblue', edgecolor='black'))\n",
    "        ax.text(position[0] + 0.5, position[1] + 0.5, state_name, color='black', fontsize=14, ha='center', va='center')\n",
    "\n",
    "        # Plot the action arrows according to the policy\n",
    "        action = policy[state_name]\n",
    "        dx, dy = action_to_vector[action]\n",
    "        \n",
    "        # Plot the action arrow for the state\n",
    "        ax.arrow(position[0] + 0.5, position[1] + 0.5, 0.25 * dx, 0.25 * dy, head_width=0.1, head_length=0.1, fc='white', ec='white')\n",
    "\n",
    "    # Remove ticks and format plot\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.title(\"Policy Visualization\")\n",
    "    plt.grid(False)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, reward_func, transition_func, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.reward_func = reward_func\n",
    "        self.transition_func = transition_func\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transition_func[state][action]\n",
    "    \n",
    "    def get_reward(self, state, action, new_state):\n",
    "        return self.reward_func[state][action][new_state]\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDP components\n",
    "states = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'T1', 'T3', 'T5'] \n",
    "actions = ['Up', 'Right', 'Left']\n",
    "discount_factor = 0.9   # Lambda = 0.9\n",
    "\n",
    "# Reward function R(s, a) -> `terminal (+1/-1)` and `0` elsewhere (unused)\n",
    "reward_func = {\n",
    "    'S1': {'Up': {'S1': 0}, 'Right': {'S2': 0}, 'Left': {'S1': 0}},       \n",
    "    'S2': {'Up': {'T1': +1}, 'Right': {'S3': 0}, 'Left': {'S1': 0}},       \n",
    "    'S3': {'Up': {'S3': 0}, 'Right': {'S4': 0}, 'Left': {'S2': 0}},       \n",
    "    'S4': {'Up': {'T3': -1}, 'Right': {'S5': 0}, 'Left': {'S3': 0}},\n",
    "    'S5': {'Up': {'S5': 0}, 'Right': {'S6': 0}, 'Left': {'S4': 0}},        \n",
    "    'S6': {'Up': {'T5': +1}, 'Right': {'S7': 0}, 'Left': {'S5': 0}},\n",
    "    'S7': {'Up': {'S7': 0}, 'Right': {'S7': 0}, 'Left': {'S6': 0}},\n",
    "    'T1': {'Up': {'T1': 0}, 'Right': {'T1': 0}, 'Left': {'T1': 0}},\n",
    "    'T3': {'Up': {'T3': 0}, 'Right': {'T3': 0}, 'Left': {'T3': 0}},\n",
    "    'T5': {'Up': {'T5': 0}, 'Right': {'T5': 0}, 'Left': {'T5': 0}}\n",
    "}\n",
    "\n",
    "# Set the transition probability matrix P(s' | s, a)\n",
    "# Transitions represented as P[s_current][a] = list of possible (s_next, prob).\n",
    "transition_func = {\n",
    "    'S1': {'Up': [('S1', 1)], 'Right': [('S2', 1)], 'Left': [('S1', 1)]},       \n",
    "    'S2': {'Up': [('T1', 1)], 'Right': [('S3', 1)], 'Left': [('S1', 1)]},       \n",
    "    'S3': {'Up': [('S3', 1)], 'Right': [('S4', 1)], 'Left': [('S2', 1)]},       \n",
    "    'S4': {'Up': [('T3', 1)], 'Right': [('S5', 1)], 'Left': [('S3', 1)]},\n",
    "    'S5': {'Up': [('S5', 1)], 'Right': [('S6', 1)], 'Left': [('S4', 1)]},        \n",
    "    'S6': {'Up': [('T5', 1)], 'Right': [('S7', 1)], 'Left': [('S5', 1)]},\n",
    "    'S7': {'Up': [('S7', 1)], 'Right': [('S7', 1)], 'Left': [('S6', 1)]},\n",
    "    'T1': {'Up': [('T1', 1)], 'Right': [('T1', 1)], 'Left': [('T1', 1)]},\n",
    "    'T3': {'Up': [('T3', 1)], 'Right': [('T3', 1)], 'Left': [('T3', 1)]},\n",
    "    'T5': {'Up': [('T5', 1)], 'Right': [('T5', 1)], 'Left': [('T5', 1)]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(states, actions, reward_func, transition_func, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "vi_policies = {}\n",
    "vi_times = 0\n",
    "\n",
    "for i in range(iters):\n",
    "    time_start = time()\n",
    "    _, policy = value_iteration(mdp)\n",
    "    time_end = time()\n",
    "\n",
    "    policy_str = policy.__str__()\n",
    "\n",
    "    if policy_str not in vi_policies:\n",
    "        vi_policies[policy_str] = {'count': 1, 'policy': policy}\n",
    "    else:\n",
    "        vi_policies[policy_str]['count'] += 1\n",
    "\n",
    "    vi_time = time_end - time_start\n",
    "    vi_times += vi_time\n",
    "\n",
    "vi_times /= iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 1000, 'policy': {'S1': 'Right', 'S2': 'Up', 'S3': 'Left', 'S4': 'Right', 'S5': 'Right', 'S6': 'Up', 'S7': 'Left', 'T1': 'Up', 'T3': 'Up', 'T5': 'Up'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for policy in vi_policies:\n",
    "    print(vi_policies[policy])\n",
    "    plot_custom_grid(vi_policies[policy]['policy'], state_positions, states, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "pi_policies = {}\n",
    "pi_times = 0\n",
    "\n",
    "for i in range(iters):\n",
    "    time_start = time()\n",
    "    _, policy = policy_iteration(mdp)\n",
    "    time_end = time()\n",
    "\n",
    "    policy_str = policy.__str__()\n",
    "\n",
    "    if policy_str not in pi_policies:\n",
    "        pi_policies[policy_str] = {'count': 1, 'policy': policy}\n",
    "    else:\n",
    "        pi_policies[policy_str]['count'] += 1\n",
    "\n",
    "    pi_time = time_end - time_start\n",
    "    pi_times += pi_time\n",
    "\n",
    "pi_times /= iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 1000, 'policy': {'S1': 'Right', 'S2': 'Up', 'S3': 'Left', 'S4': 'Right', 'S5': 'Right', 'S6': 'Up', 'S7': 'Left', 'T1': 'Up', 'T3': 'Up', 'T5': 'Up'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for policy in pi_policies:\n",
    "    print(pi_policies[policy])\n",
    "    plot_custom_grid(pi_policies[policy]['policy'], state_positions, states, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison of PI and VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+\n",
      "| State   | VI (1000)   | PI (1000)   |\n",
      "+=========+=============+=============+\n",
      "| S1      | Right       | Right       |\n",
      "+---------+-------------+-------------+\n",
      "| S2      | Up          | Up          |\n",
      "+---------+-------------+-------------+\n",
      "| S3      | Left        | Left        |\n",
      "+---------+-------------+-------------+\n",
      "| S4      | Right       | Right       |\n",
      "+---------+-------------+-------------+\n",
      "| S5      | Right       | Right       |\n",
      "+---------+-------------+-------------+\n",
      "| S6      | Up          | Up          |\n",
      "+---------+-------------+-------------+\n",
      "| S7      | Left        | Left        |\n",
      "+---------+-------------+-------------+\n",
      "| T1      | Up          | Up          |\n",
      "+---------+-------------+-------------+\n",
      "| T3      | Up          | Up          |\n",
      "+---------+-------------+-------------+\n",
      "| T5      | Up          | Up          |\n",
      "+---------+-------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming `states` is a list of all states and \n",
    "# `actions` is a list of possible actions.\n",
    "\n",
    "headers = [\"State\"]\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "\n",
    "    t = [state]\n",
    "    for policy in vi_policies:\n",
    "        action = vi_policies[policy]['policy'].get(state, 'N/A')\n",
    "        vi_action = vi_policies[policy]['policy'].get(state, 'N/A')\n",
    "\n",
    "        headers.append(f\"VI ({vi_policies[policy]['count']})\")\n",
    "        t.append(action)\n",
    "\n",
    "    for policy in pi_policies:\n",
    "        action = pi_policies[policy]['policy'].get(state, 'N/A')\n",
    "        vi_action = pi_policies[policy]['policy'].get(state, 'N/A')\n",
    "\n",
    "        headers.append(f\"PI ({pi_policies[policy]['count']})\")\n",
    "        t.append(action)\n",
    "\n",
    "    table_data.append(t)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy time: 0.0\n",
      "Value time: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Policy time: {pi_time}\")\n",
    "print(f\"Value time: {vi_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, I ran both **Value Iteration (VI)** and **Policy Iteration (PI)** for **1000 iterations** each, and reported consistent policies for both methods. The time data for both methods (value and policy) was recorded as **0.0 seconds**, which indicates these operations concluded very quickly.\n",
    "\n",
    "1. **Both VI and PI yield the same optimal policy**, meaning agents should navigate towards green cells (T1 and T5), avoid the red cell (T3), and optimize rewards.\n",
    "   \n",
    "2. **Time measurements** indicate that both VI and PI completed the tasks rapidly (near-zero duration) due to the simplified nature of the state transitions.\n",
    "\n",
    "3. The policies determined by VI and PI show logical and optimal movements, driven by transitioning away from penalties and toward reward-based outcomes. This reflects efficient navigation, perfectly aligning with the given rewards and penalties in the MDP.\n",
    "\n",
    "---\n",
    "\n",
    "**Policy Analysis for Value Iteration (VI) and Policy Iteration (PI)**\n",
    "\n",
    "From the provided table and analysis, both **VI (1000 iterations)** and **PI (1000 iterations)** resulted in **identical policies** for all states (S1 to S7, and terminal states T1, T3, T5).\n",
    "\n",
    "**Determined Policies for Each State (for both VI and PI):**\n",
    "\n",
    "1. **S1**:\n",
    "   - Action: `Right`\n",
    "   - Moving right seems to be the best action for S1 to reach a beneficial state.\n",
    "   \n",
    "2. **S2**:\n",
    "   - Action: `Up`\n",
    "   - Moving up from S2 likely leads directly toward the green state T1, which offers a reward of +1.\n",
    "   \n",
    "3. **S3**:\n",
    "   - Action: `Left`\n",
    "   - This action avoids moving right towards T3 (the red state with a -1 penalty). Hence, moving left is a safer strategy.\n",
    "   \n",
    "4. **S4**:\n",
    "   - Action: `Right`\n",
    "   - Moving right seems to be best for S4, likely leading toward higher-valued states (like T5) or avoiding proximity to penalties.\n",
    "   \n",
    "5. **S5**:\n",
    "   - Action: `Right`\n",
    "   - Moving right from S5 probably leads to T5 (the green state on the right side), which gives a +1 reward.\n",
    "   \n",
    "6. **S6**:\n",
    "   - Action: `Up`\n",
    "   - Moving up from S6 likely leads directly to T5, a green state with a +1 reward.\n",
    "\n",
    "7. **S7**:\n",
    "   - Action: `Left`\n",
    "   - Moving left is optimal in this case to avoid harm from T3 (the red cell) and lead towards more neutral or beneficial parts of the map.\n",
    "\n",
    "**Evaluation of Terminal States**\n",
    "\n",
    "- **T1 (Green) and T5 (Green)**:\n",
    "  - Action: `Up`\n",
    "  - Since these terminal states represent positive outcomes (+1 reward), the agent can't perform any further actions.\n",
    "\n",
    "- **T3 (Red)**:\n",
    "  - Action: `Up`\n",
    "  - No further transitions occur after reaching a terminal state. In this case, the agent incurs a penalty of -1 after moving into T3.\n",
    "\n",
    "**Policy Consistency Between VI and PI**\n",
    "\n",
    "Since the policies are **identical under both algorithms**, it shows strong convergence to the optimal policy for this specific MDP. Here's what we can infer:\n",
    "\n",
    "1. **Policy Convergence**:\n",
    "   - Both methods converge to the same optimal policy after 1000 iterations. While **VI** typically updates state values iteratively and **PI** iterates over entire policies, both approaches guide actions toward similar goals: avoiding penalties and obtaining rewards.\n",
    "\n",
    "2. **Optimality**:\n",
    "   - The policies reflect intelligent movement strategies — the agent avoids the red state (T3, penalty) and moves toward either neutral or positive paths (green cells: T1, T5).\n",
    "   - States like **S3** and **S7** demonstrate how the agent prefers to move away from potential penalties by strategically moving left.\n",
    "\n",
    "---\n",
    "\n",
    "**Time Analysis**\n",
    "\n",
    "- **Policy Timing**:\n",
    "  - Both **Policy Iteration (PI)** and **Value Iteration (VI)** showed a timing of **0.0 seconds** in the provided results. This suggests the operations were extremely fast and likely executed in a minimal number of time measurements. Given the simplicity of the MDP, this is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "In this question, we examine the effect of episode length (Horizon) on the agent’s policy. Consider a robot that is tasked with managing stock shares. (Assume this problem can be represented as an MDP.)\n",
    "\n",
    "Let $ s $ represent the number of shares the robot currently has (an integer always between [0,10]). At each moment, the robot has two options: to sell (if possible, $ s $ decreases by one unit) or to buy (if possible, $ s $ increases by one unit).\n",
    "\n",
    "- If $ s > 0 $ and the agent sells, it receives a reward of +1 for the sale, and the stock level changes to $ s - 1 $. If $ s = 0 $, nothing happens.\n",
    "- If $ s < 9 $ and the agent buys, it receives no reward, and the stock level changes to $ s + 1 $.\n",
    "- The stock owner wants the inventory to be fully stocked at the end of the day; therefore, if the stock level reaches the maximum value of $ s = 10 $, the agent receives a reward of +100.\n",
    "- The state $ s = 10 $ is also a terminal state, and the problem ends if it is reached.\n",
    "\n",
    "The reward function, denoted by $ r(s, a, s') $, is summarized as follows:\n",
    "\n",
    "- $ r(s, \\text{sell}, s - 1) = 1 $ for $ s > 0 $\n",
    "- $ r(0, \\text{sell}, 0) = 0 $\n",
    "- $ r(s, \\text{buy}, s + 1) = 0 $ for $ s < 9 $\n",
    "- $ r(9, \\text{buy}, 10) = 100 $, indicating that moving from $ s = 9 $ to $ s = 10 $ gives a reward of +100, reaching the maximum stock level.\n",
    "\n",
    "It is assumed that the stock level always starts from $ s = 3 $ at the beginning of the day. We will examine how the agent’s optimal policy changes by setting a limited horizon $H$ for the problem. Recall that the horizon $ H $ refers to a limit on the number of time steps in which the agent can interact with the MDP before the episode ends, regardless of whether a terminal state has been reached. We will analyze the characteristics of the optimal policy (the policy that maximizes the episode’s reward) as the horizon $ H $ changes. (For the finite horizon, the discount factor is $ \\gamma = 1 $).\n",
    "\n",
    "![pics/P4.png](pics/P4.png)\n",
    "\n",
    "For example, assume $ H = 4 $. The agent can sell for three steps, moving from $ s = 3 $ to $ s = 2 $, then $ s = 1 $, and finally $ s = 0 $, receiving rewards of +1, +1, and +1 for each sell action. In the fourth step, the inventory is empty, so it can either sell or buy, but it will not receive any reward in either case. Then, the problem ends due to the time limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a \n",
    "Starting from the initial state $ s = 3 $, is it possible to choose a value of $ H $ such that the optimal policy includes both buying and selling steps during the execution? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, let’s explore $H = 5$ in detail:\n",
    "\n",
    "**Setting $H = 5$:**\n",
    "\n",
    "We begin at $s = 3$ and examine a scenario with 3 sells, followed by 1 buy and then 1 sell:\n",
    "\n",
    "- **First step (s = 3)**: Sell, move to $s = 2$, and receive a reward of +1.\n",
    "- **Second step (s = 2)**: Sell, move to $s = 1$, and receive a reward of +1.\n",
    "- **Third step (s = 1)**: Sell, move to $s = 0$, and receive a reward of +1.\n",
    "- **Fourth step (s = 0)**: Since there are no more shares to sell, the optimal action is to buy. Buy, move to $s = 1$, and receive no reward.\n",
    "- **Fifth step (s = 1)**: Sell, return to $s = 0$, and receive a reward of +1.\n",
    "\n",
    "In this scenario, with a horizon $H = 5$, we obtain 4 units of reward: 1 unit from each of the 4 sell actions. The agent first sells down to $s = 0$, buys back to $s = 1$, and finally sells again.\n",
    "\n",
    "This sequence contains both **buying** and **selling** steps, thus confirming that it is possible for the agent's optimal policy to include both actions if the horizon $H$ is set to 5.\n",
    "\n",
    "---\n",
    "\n",
    "#### **General Analysis of $H$ and Buying/Selling**\n",
    "\n",
    "- **For $H \\leq 4$**:\n",
    "  At this point, the agent **only sells** to receive immediate rewards without any buying actions, because there isn't enough time or benefit from trying to buy stocks. The reward for selling is immediate, whereas buying does not provide any immediate reward. \n",
    "\n",
    "  - For example, with **$H = 4$**:\n",
    "    - The optimal policy involves the agent selling for the first three steps, moving from $s = 3$ to $s = 0$.\n",
    "    - At the fourth step (at $s = 0$), the robot can either sell again (which would yield no reward) or buy, but the point is that **the optimal policy would not necessarily contain any buy** since the buy action doesn't lead to an immediate benefit and there's no sufficient time left to capitalize on it. So, you will often just see 3 sells and perhaps some arbitrary choice in the 4th.\n",
    "  \n",
    "- **For $H = 5$**:\n",
    "  With $H = 5$, the agent can start selling, just like $H = 4$, but might add **a single buy step** at the fourth step (after the third sale).\n",
    "\n",
    "  - Optimal policy for $H = 5$:\n",
    "    - The first three actions are sells: $s = 3 \\to 2 \\to 1 \\to 0$ with rewards of +1 for each sell.\n",
    "    - In the fourth step, the agent might buy (because there’s no benefit in selling, as $s = 0$).\n",
    "    - Finally, selling again in the fifth step yields another reward of +1.\n",
    "  \n",
    "  Thus, $H = 5$ allows for a mixture of **1 buy step and 4 sells**, leading to maximal reward accumulation.\n",
    "\n",
    "- **For $H = 6$**:\n",
    "  $H = 6$ is similar to $H = 5$. The agent can follow a similar strategy, where first they sell, then buy at one point, and finally sell again to get a reward.\n",
    "\n",
    "  - Optimal policy for $H = 6$:\n",
    "    - The agent might sell for three steps (moving from $s = 3 \\to s = 0$).\n",
    "    - Then the agent buys (e.g., at step 4), moves to $s = 1$, sells it back again to stay within the horizon.\n",
    "    - Additional sells/buys in a similar cycle could lead to a total return of 4-5, depending on the steps chosen.\n",
    "\n",
    "  Therefore, $H = 6$ contains a **mixture of several buys and sells** where the agent maximizes reward while handling the $H = 6$ steps.\n",
    "\n",
    "- **For $H = 7$**:\n",
    "    \n",
    "  At $H = 7$, the agent no longer sells to maximize rewards. Instead, the optimal policy will switch to buying actions aimed at maximizing the final large reward of 100 at $s = 10$. Since the starting state is $s = 3$, it can take up to 7 steps:\n",
    "    \n",
    "  - The agent will **only buy** to increase the stock from $s = 3 \\to s = 10$.\n",
    "  - By the seventh step, the robot will buy enough shares (moving from $s = 3 \\to s = 10$) and reach the terminal state at $s = 10$, where it receives the reward of 100.\n",
    "\n",
    "  Thus, for $H = 7$, **buying only** guarantees the large reward at the terminal state.\n",
    "\n",
    "- **For $H = 8$**:\n",
    "    \n",
    "  The situation is quite similar to $H = 7$. Even though the agent has 8 steps, the optimal trajectory is still to buy until it reaches the maximum stock level of $s = 10$.\n",
    "\n",
    "  - If the agent spends any actions selling (for small immediate rewards), it sacrifices the future reward of 100 from reaching $s = 10$, so selling is suboptimal.\n",
    "  - Therefore, the **only optimal policy** in this case is to **buy** in order to maximize future rewards.\n",
    "\n",
    "  The agent will still follow a pure-buy policy to reach $s = 10$ and get rewarded with 100.\n",
    "\n",
    "- **For $H \\geq 9$**:\n",
    "    \n",
    "  For horizons $H \\geq 9$, the agent has enough time to maximize the reward not just by stockpiling all the way up to $s = 10$, but also by mixing in intermittent sells to gain additional rewards along the way, before ultimately ensuring it ends at $s = 10$.\n",
    "\n",
    "  - The agent can use $s \\to s - 1$ transitions (selling) to obtain immediate rewards (e.g., +1) at several points and then buy back the shares in order to reach $s = 10$.\n",
    "  - The idea is to balance between immediate rewards from selling shares and the future reward of 100 for max stock level $s = 10$.\n",
    "\n",
    "  For example, for $H = 9$, the agent can:\n",
    "  - Sell for a couple of actions to gain small immediate rewards, then **switch to buying to ensure that the final state is $s = 10$**, and maximize the larger future reward.\n",
    "\n",
    "  For **$H \\geq 9$**, the optimal policy is a **mixture of sells and buys** to balance immediate rewards and ensure reaching $s = 10$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "Starting from the initial state $ s = 3 $, for what values of $ H $ does the optimal policy lead to a fully stocked inventory? In other words, provide a range for $ H $.\n",
    "\n",
    "*Note 1:* We consider the inventory fully stocked when the buy action is chosen in state $ s = 9 $, causing a transition to $ s = 10 $. This includes the last time step in the horizon as well.\n",
    "\n",
    "*Note 2:* By performing only buy actions, the agent can reach $ s = 10 $ from $ s = 3 $ in $ H = 7 $ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is $7 \\leq H$. Let's see why:\n",
    "\n",
    "#### $H \\leq 6$\n",
    "\n",
    "- The agent will prefer selling rather than buying, as reaching $s = 10$ is impossible within 6 steps. The agent will maximize the immediate rewards from selling, and the inventory will not become fully stocked.\n",
    "\n",
    "#### $H = 7$\n",
    "\n",
    "- Starting at $s = 3$, if $H = 7$, by **only buying**, the agent can reach $s = 10$ on the last step:\n",
    "  $$s = 3 \\to 4 \\to 5 \\to 6 \\to 7 \\to 8 \\to 9 \\to 10.$$\n",
    "  \n",
    "  Reaching $s = 10$ gives the agent a reward of +100 in the final step. This is the maximum possible reward for this horizon because selling earlier for a reward of +1 per sale sacrifices the large reward of +100 from reaching $s = 10$. The optimal policy in this case is to **always buy** and reach $s = 10$.\n",
    "\n",
    "Thus, **for $H = 7$**, the optimal policy leads to a fully stocked inventory, since the agent sacrifices no time for selling and focuses on accumulating the large terminal reward.\n",
    "\n",
    "#### $H = 8$\n",
    "\n",
    "- With $H = 8$, the agent can reach $s = 10$ by **only buying**:\n",
    "\n",
    "  $$s = 3 \\to 4 \\to 5 \\to 6 \\to 7 \\to 8 \\to 9 \\to 10.$$\n",
    "  \n",
    "  Like in the case of $H = 7$, the agent will receive the large reward of +100 upon reaching $s = 10$. There is one extra step after reaching $s = 10$, but since reaching $s = 10$ ends the episode, no further actions are possible once the maximum state is reached.\n",
    "\n",
    "  If the agent spends any time selling earlier, they will not collect the large reward of +100 from reaching $s = 10$. Therefore, the optimal policy for $H = 8$ is still to **buy exclusively** and reach $s = 10$, maximizing the final reward.\n",
    "\n",
    "Thus, **for $H = 8$**, the optimal policy leads to fully stocking the inventory. The agent once again only buys to reach $s = 10$ and get the large reward.\n",
    "\n",
    "#### $H = 9$ and beyond\n",
    "\n",
    "- For $H \\geq 9$, the agent has even more time. The important decision now is to determine whether earning small rewards by selling before reaching $s = 10$ can provide better total rewards than simply reaching $s = 10$ for the big reward of +100.\n",
    "\n",
    "- Since the reward from reaching $s = 10$ is **so large** (+100), the optimal strategy will still involve **reaching $s = 10$** within the time limit.\n",
    "\n",
    "  However, the agent might choose to sell once or twice **early** to gather a small immediate reward (e.g., selling at $s = 3 \\to s = 2$) and then buy back to ensure it still reaches $s = 10$. As long as reaching $s = 10$ is still feasible, the agent can mix buying and selling to maximize the total reward.\n",
    "\n",
    "  One possible policy for $H = 9$ is to sell for 1 step (gaining +1 reward), and then continue buying towards $s = 10$, ensuring that the agent still gets the major reward of +100.\n",
    "\n",
    "Thus, **for $H \\geq 9$**, the optimal policy may include a mix of **some early selling** for immediate rewards followed by **buying to reach $s = 10$**. The agent still values reaching the fully stocked inventory because of the large +100 reward. This means the inventory ends fully stocked, but the agent makes a few sales en route to $s = 10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c\n",
    "Now, consider the infinite-horizon setting with a discount factor $ \\gamma $. In other words, there is no time limit, and the problem only ends if a terminal state is reached. Suppose $ \\gamma = 0 $; what action does the optimal policy take when $ s = 3 $? What action does the optimal policy take when $ s = 9 $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\gamma = 0$, the agent only cares about rewards in the current step and ignores all possible future outcomes. So, **at $s = 3$**, the best action is to **sell** a share because this provides an immediate reward of +1. additionally, **at $s = 9$** the best action is to **buy** a share to reach $s = 10$, which gives the large immediate reward of +100.\n",
    "\n",
    "#### What does $\\gamma$ = 0 mean?\n",
    "\n",
    "The discount factor $\\gamma$ tells us how the agent values **future rewards**. If $\\gamma = 0$, the agent **does not care about future rewards at all**; it only cares about the **immediate rewards**. This means the agent will choose actions that maximize the current step's reward without considering what might happen next.\n",
    "\n",
    "In this scenario, with $\\gamma = 0$, **only the immediate reward matters** for the agent at every step. The agent isn't concerned about long-term outcomes, such as reaching the fully stocked inventory $s = 10$ and receiving the +100 reward at the end. Instead, it focuses solely on the action that gives the highest reward in the current step.\n",
    "\n",
    "#### What happens at $s = 3$?\n",
    "At $s = 3$, the agent has two options:\n",
    "1. **Sell**: Selling one share will give the agent an immediate reward of **+1** and move it to state $s = 2$.\n",
    "2. **Buy**: Buying one share will give the agent **0 immediate reward** and move it to state $s = 4$.\n",
    "\n",
    "Since $\\gamma = 0$, the agent only cares about **immediate reward**, not future rewards. Therefore, the agent will choose the action that gives the best **immediate** return. **Selling** at $s = 3$ provides an immediate reward of **+1**, while buying provides **0 immediate reward**.\n",
    "\n",
    "Thus, **the optimal action at $s = 3$** is to **sell** a share in order to get the immediate reward of +1.\n",
    "\n",
    "#### What happens at $s = 9$?\n",
    "Similarly, at $s = 9$, the agent has two options:\n",
    "1. **Sell**: Selling one share will give the agent an immediate reward of **+1** and move it to state $s = 8$.\n",
    "2. **Buy**: Buying one share will move the agent to state $s = 10$, which is the terminal state, and will immediately give a reward of **+100**.\n",
    "\n",
    "Since $\\gamma = 0$ means the agent only cares about **immediate reward**, the choice here is clear: **Buying** in $s = 9$ leads to an immediate reward of **+100**, which is much higher than the alternative of **+1** received from selling.\n",
    "\n",
    "Thus, **the optimal action at $s = 9$** is to **buy** the last share, transitioning to $s = 10$ and earning the large immediate reward of +100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d\n",
    "In the infinite-horizon setting with a discount factor $ \\gamma $, is it possible to choose a constant $ \\gamma \\in (0, 1] $ such that the optimal policy, starting from $ s = 3 $, never fully stocks the inventory? If so, find a range of $ \\gamma $ that meets this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider two policies:\n",
    "\n",
    "1. **Policy for filling the inventory (buying until $s = 10$)**:\n",
    "    - Starting from $s = 3$, the agent buys until it reaches $s = 10$ to get the large reward of +100.\n",
    "    - The cumulative discounted reward for reaching $s = 10$ is:\n",
    "      $$G_{\\text{stock}} = 100 \\cdot \\gamma^6$$\n",
    "    - This is because the reward of +100 is received after 7 steps of buying (zero rewards along the way), so the total gain is discounted by $\\gamma^6$.\n",
    "\n",
    "2. **Policy for not filling the inventory (selling first, then alternating between buying and selling)**:\n",
    "    - In this case, the agent first sells down to $s = 0$, collects small rewards of +1, and then alternates between buying and selling to collect additional rewards.\n",
    "    - The cumulative discounted reward for selling and not restocking the inventory is:\n",
    "      $$G_{\\text{no\\_stock}} = 1 + \\gamma + \\gamma^2 + 0 + \\gamma^4 + 0 + \\gamma^6 + \\cdots$$\n",
    "    - The important thing to notice here is that rewards are collected every other step, and this forms a geometric series. Therefore, using the summation formula for a geometric series, the total reward from selling and alternating is:\n",
    "      $$G_{\\text{no\\_stock}} = \\gamma + \\frac{1}{1 - \\gamma^2}$$\n",
    "\n",
    "#### Condition for avoiding fully stocking the inventory:\n",
    "We want to find the values of $\\gamma$ such that the reward from not stocking the inventory is larger than the reward from fully stocking the inventory:\n",
    "$$G_{\\text{no\\_stock}} > G_{\\text{stock}}$$\n",
    "This results in the inequality:\n",
    "$$\\gamma + \\frac{1}{1 - \\gamma^2} > 100 \\cdot \\gamma^6$$\n",
    "Multiply both sides by $(1 - \\gamma^2)$ to eliminate the denominator:\n",
    "$$(\\gamma - \\gamma^3) + 1 > 100 \\cdot \\gamma^6 \\cdot (1 - \\gamma^2)$$\n",
    "Next, expand the terms:\n",
    "$$1 - \\gamma^3 + \\gamma > 100 \\cdot \\gamma^6 - 100 \\cdot \\gamma^8$$\n",
    "Rearrange the terms to move everything to one side:\n",
    "$$100 \\cdot \\gamma^8 - 100 \\cdot \\gamma^6 - \\gamma^3 + \\gamma + 1 > 0$$\n",
    "This is the expression we need to solve.\n",
    "\n",
    "#### Solving the inequality:\n",
    "\n",
    "![pics/P4d.png](pics/P4d.png)\n",
    "\n",
    "The graph shows the function:\n",
    "$$f(x) = 100 \\cdot x^8 - 100 \\cdot x^6 - x^3 + x + 1$$\n",
    "By plotting the graph (as shown in the image), we find that the root of the inequality is approximately:\n",
    "$$\\gamma \\approx 0.51554$$\n",
    "Thus, the range of $\\gamma$ such that the optimal policy **never fully stocks the inventory** is:\n",
    "$$\\gamma \\in (0, 0.51554)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "\n",
    "Based on your answer to part (a) above, choose values of $H$ such that, after obtaining the optimal policy using the VI and PI algorithms, the optimal policy exhibits the following characteristics:  \n",
    "- The optimal policy only buys.  \n",
    "- The optimal policy only sells.  \n",
    "- The optimal policy performs both buying and selling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, reward_func, transition_func, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.reward_func = reward_func\n",
    "        self.transition_func = transition_func\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transition_func[state][action]\n",
    "    \n",
    "    def get_reward(self, state, action, new_state):\n",
    "        return self.reward_func[state][action][new_state]\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor\n",
    "    \n",
    "    # Generate a random episode based on the current ε-greedy policy\n",
    "    def generate_episode(self, start_state, policy, epsilon):\n",
    "        episode = []\n",
    "        state = start_state\n",
    "\n",
    "        while state != 'S10':  # Until we reach the terminal state S10\n",
    "            action = self.get_action_for_state_epsilon_greedy(state, policy, epsilon)\n",
    "            transitions = self.get_transitions(state, action)\n",
    "            new_state, prob = random.choices(transitions, [prob for _, prob in transitions])[0]\n",
    "            reward = self.get_reward(state, action, new_state)\n",
    "            episode.append((state, action, reward, new_state))\n",
    "            state = new_state\n",
    "        \n",
    "        return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['S0', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10']\n",
    "actions = ['buy', 'sell']\n",
    "discount_factor = 0.9   # Lambda = 0.9\n",
    "\n",
    "# Reward function R(s, a) -> `terminal (+1/-1)` and `0` elsewhere\n",
    "reward_func = {\n",
    "    'S0': {'buy': {'S1': 0}, 'sell': {'S0': 0}},\n",
    "    'S1': {'buy': {'S2': 0}, 'sell': {'S0': 1}},\n",
    "    'S2': {'buy': {'S3': 0}, 'sell': {'S1': 1}},\n",
    "    'S3': {'buy': {'S4': 0}, 'sell': {'S2': 1}},\n",
    "    'S4': {'buy': {'S5': 0}, 'sell': {'S3': 1}},\n",
    "    'S5': {'buy': {'S6': 0}, 'sell': {'S4': 1}},\n",
    "    'S6': {'buy': {'S7': 0}, 'sell': {'S5': 1}},\n",
    "    'S7': {'buy': {'S8': 0}, 'sell': {'S6': 1}},\n",
    "    'S8': {'buy': {'S9': 0}, 'sell': {'S7': 1}},\n",
    "    'S9': {'buy': {'S10': 100}, 'sell': {'S8': 1}},\n",
    "    'S10': {'buy': {'S10': 0}, 'sell': {'S10': 0}}\n",
    "}\n",
    "\n",
    "# Set the transition probability matrix P(s' | s, a)\n",
    "# Transitions represented as P[s_current][a] = list of possible (s_next, prob).\n",
    "transition_func = {\n",
    "    'S0': {'buy': [('S1', 1)], 'sell': [('S0', 1)]},\n",
    "    'S1': {'buy': [('S2', 1)], 'sell': [('S0', 1)]},\n",
    "    'S2': {'buy': [('S3', 1)], 'sell': [('S1', 1)]},\n",
    "    'S3': {'buy': [('S4', 1)], 'sell': [('S2', 1)]},\n",
    "    'S4': {'buy': [('S5', 1)], 'sell': [('S3', 1)]},\n",
    "    'S5': {'buy': [('S6', 1)], 'sell': [('S4', 1)]},\n",
    "    'S6': {'buy': [('S7', 1)], 'sell': [('S5', 1)]},\n",
    "    'S7': {'buy': [('S8', 1)], 'sell': [('S6', 1)]},\n",
    "    'S8': {'buy': [('S9', 1)], 'sell': [('S7', 1)]},\n",
    "    'S9': {'buy': [('S10', 1)], 'sell': [('S8', 1)]},\n",
    "    'S10': {'buy': [('S10', 1)], 'sell': [('S10', 1)]}\n",
    "}\n",
    "\n",
    "# Starting state is S3\n",
    "start_state = 'S3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(states, actions, reward_func, transition_func, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finite VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_horizon_value_iteration(mdp, T, epsilon=1e-5):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.actions  # Same actions for all states\n",
    "    \n",
    "    # Initialize value functions for each time step t (V_t(s))\n",
    "    V = {t: {s: 0 for s in states} for t in range(T + 1)}  # V_T = 0 is terminal condition\n",
    "    policy = {t: {s: None for s in states} for t in range(T + 1)}  # Initialize policy for each time t\n",
    "\n",
    "    # Step backwards from t = T-1 to t = 0\n",
    "    for t in range(T - 1, -1, -1):\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_V = V[t].copy()\n",
    "            \n",
    "            for state in states:\n",
    "                max_value = max(\n",
    "                    sum(prob * (mdp.get_reward(state, action, next_state) + V[t+1][next_state])\n",
    "                        for next_state, prob in mdp.get_transitions(state, action))\n",
    "                    for action in actions\n",
    "                )\n",
    "                new_V[state] = max_value\n",
    "                delta = max(delta, abs(V[t][state] - new_V[state]))\n",
    "            \n",
    "            V[t] = new_V\n",
    "            \n",
    "            # Check for convergence\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "\n",
    "        for state in states:\n",
    "            action_values = {}\n",
    "            for action in actions:\n",
    "                action_values[action] = sum(\n",
    "                    prob * (mdp.get_reward(state, action, next_state) + V[t+1][next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action)\n",
    "                )\n",
    "            policy[t][state] = max(action_values, key=action_values.get)\n",
    "    \n",
    "    return V, policy  # Return value functions at different time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input and Initialization:**\n",
    "\n",
    "1. **Input Parameters:**\n",
    "   * `mdp`: the Markov Decision Process for which we are calculating the value functions and policy. It assumes that the MDP provides methods like:\n",
    "     - `mdp.get_states()` to get the list of all possible states.\n",
    "     - `mdp.get_transitions(state, action)` to get the transition probabilities (i.e., the probability of moving to each possible next-state after taking a specific action from a current state).\n",
    "     - `mdp.get_reward(state, action, next_state)` to get the immediate reward for a transition from `state` to `next_state` after taking `action`.\n",
    "   * `T`: Length of the time horizon (i.e., the number of decision steps). The process spans from $t = 0$ to $t = T-1$.\n",
    "   * `epsilon`: A parameter controlling the convergence threshold for the algorithm (default value of `epsilon = 1e-5`).\n",
    "\n",
    "2. **Initialization:**\n",
    "   * `states`: A list of all possible states in the MDP retrieved using `mdp.get_states()`.\n",
    "   * `actions`: A list of all actions (the code assumes that the MDP has the same actions for all states).\n",
    "   * `V`: A dictionary that will store the **value function** $V_t[s]$ at each time step $t$ for each state $s$. It is initialized with all values at 0 initially (Note: $V_T[s] = 0$ is defined as the terminal condition because beyond time horizon $T$ there is no reward to be collected).\n",
    "   \n",
    "     - **Value function $V_t(s)$:** Represents the maximum expected cumulative reward from state $s$ at time-step $t$, assuming optimal decisions are made afterward.\n",
    "   \n",
    "   * `policy`: A dictionary that will store the **optimal policy** for each state $s$ at each time $t$ (initially set to `None` for all state-ath time-step pairs).\n",
    "     \n",
    "     - **Policy $\\pi_t(s)$:** Represents the optimal action to take in state $s$ at time-step $t$ to maximize expected rewards for the future.\n",
    "\n",
    "**Backwards Iteration over Time Steps:**\n",
    "\n",
    "The algorithm proceeds **backwards** starting from time-step $t = T-1$ down to $t = 0$, towards the beginning of the planning horizon. For each time step $t$:\n",
    "\n",
    "1. **Value Iteration at Time Step $t$:**\n",
    "\n",
    "   For each time step $t$, the algorithm updates the current value function estimates $V_t(s)$ for each state $s$, considering the rewards and the continued value from the future (referred to as $V_{t+1}(s)$) resulting from optimal policies. The value update for each state happens iteratively until convergence.\n",
    "\n",
    "   **Steps for each state $s$ in the time-step**:\n",
    "   - For each state `s`, try all possible actions $a$ from this state.\n",
    "   - For each action $a$, calculate the cumulative expected value:\n",
    "     - Look at all possible next states $s'$ and their transition probabilities given by $\\text{prob}(s' \\mid s, a)$.\n",
    "     - Compute the sum of the expected future rewards from taking action $a$ in state $s$,\n",
    "       - This is computed as:\n",
    "         $$\\text{expected value} = \\sum_{s'} \\text{prob}(s' | s, a) \\cdot \\left[ \\text{reward}(s, a, s') + V_{t+1}(s') \\right]$$\n",
    "       - The reward depends on the current state $s$, action $a$, and resulting next state $s'$, while $V_{t+1}(s')$ reflects the future value from that next state $s'$ at the next time-step $t+1$.\n",
    "   - Choose the action that maximizes this expected value for the state.\n",
    "\n",
    "   - After exploring all actions, set the new value of $V_t(s)$ to the best (maximized) value and update the policy $\\pi_t(s)$ to the action that gives this best value. This iteration continues for the current time-step $t$ until the value function $V_t$ converges for all states (i.e., changes are lower than `epsilon`).\n",
    "\n",
    "2. **Policy Extraction:**\n",
    "   - Once the value function $V_t$ has converged, the resulting policy $\\pi_t(s)$ is extracted by considering the optimal action for each state $s$. For each state, iterate through possible actions and look for the one with the highest expected value (this is stored in the `policy` dictionary).\n",
    "\n",
    "**Breaking the Process:**\n",
    "\n",
    "The inner while-loop runs continuously during each time-step until the value function stabilizes for that time step. The process ends if the largest difference between successive value updates (tracked by `delta`) is smaller than the convergence threshold $\\epsilon$. This is a heuristic to ensure that the value function has sufficiently converged, meaning further updates will not significantly improve the result.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "At the end of the algorithm:\n",
    "- **`V`** will contain the final value functions for all states at each time step (from $t = 0$ to $t = T$).\n",
    "- **`policy`** will contain the optimal action to take for each state at each time step (this yields the optimal policy sequence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finite PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_horizon_policy_iteration(mdp, T, epsilon=1e-5):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.actions  # Same actions for all states\n",
    "    \n",
    "    # Initialize value functions for each time step t (V_t(s))\n",
    "    V = {t: {s: 0 for s in states} for t in range(T + 1)}  # V_T = 0 is terminal condition\n",
    "    policy = {t: {s: actions[0] for s in states} for t in range(T + 1)}  # Initialize policy for each time t\n",
    "\n",
    "    def policy_evaluation(policy, V1, V2):\n",
    "        \"\"\"\n",
    "        Evaluate the given policy by solving V(s) = sum over next states[T(s, pi(s), s') * (R(s, pi(s), s') + γ * V(s'))].\n",
    "        The function performs iterative evaluation until V converges.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_V = V1.copy()\n",
    "            \n",
    "            for state in states:\n",
    "                action = policy[state]\n",
    "                new_V[state] = sum(\n",
    "                    prob * (mdp.get_reward(state, action, next_state) + V2[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action)\n",
    "                )\n",
    "                delta = max(delta, abs(V1[state] - new_V[state]))\n",
    "            \n",
    "            V1 = new_V\n",
    "            \n",
    "            # Check for convergence\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "        \n",
    "        return V1\n",
    "    \n",
    "    # Step backwards from t = T-1 to t = 0\n",
    "    for t in range(T - 1, -1, -1):\n",
    "\n",
    "        while True:\n",
    "            # Step 3: Policy Evaluation -> Compute V given current policy\n",
    "            V[t] = policy_evaluation(policy[t], V[t], V[t+1])\n",
    "            \n",
    "            policy_stable = True\n",
    "            \n",
    "            # Step 4: Policy Improvement -> Greedily improve the policy\n",
    "            for state in states:\n",
    "                # Find the best action according to the current value function V\n",
    "                old_action = policy[t][state]\n",
    "                action_values = {}\n",
    "                \n",
    "                for action in actions:\n",
    "                    action_values[action] = sum(\n",
    "                        prob * (mdp.get_reward(state, action, next_state) + V[t+1][next_state])\n",
    "                        for next_state, prob in mdp.get_transitions(state, action)\n",
    "                    )\n",
    "                \n",
    "                # Choose the action that gives maximum value\n",
    "                best_action = max(action_values, key=action_values.get)\n",
    "                policy[t][state] = best_action\n",
    "                \n",
    "                # If the policy did change, we're not yet stable\n",
    "                if old_action != best_action:\n",
    "                    policy_stable = False\n",
    "            \n",
    "            # Step 5: Check if the policy is stable and terminate if it is\n",
    "            if policy_stable:\n",
    "                break\n",
    "    \n",
    "    return V, policy  # Return value functions at different time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization:**\n",
    "\n",
    "1. **Input Parameters:**\n",
    "   * `mdp`: The Markov decision process, which provides methods like:\n",
    "     - `mdp.get_states()` to get the list of all possible states.\n",
    "     - `mdp.get_transitions(state, action)` to get the transition probabilities for a given state-action pair.\n",
    "     - `mdp.get_reward(state, action, next_state)` to get the reward for transitioning from a state to a next state after taking an action.\n",
    "   * `T`: A finite horizon, meaning there is a limited number $T$ of decision time steps. The goal is to maximize the cumulative reward over this fixed period.\n",
    "   * `epsilon`: A threshold for convergence, i.e., how small the change in the value function must be for the algorithm to decide that it has converged (default set to $10^{-5}$).\n",
    "\n",
    "2. **Initialization of Variables:**\n",
    "   - `states`: The list of states in the MDP is retrieved using `mdp.get_states()`.\n",
    "   - `actions`: Actions available in the MDP, assumed to be identical for all states as `mdp.actions`.\n",
    "   - `V`: A dictionary representing the **value function** $V_t(s)$ for each state $s$ and time step $t$. It's initialized to zero. $V_T(s)$ is set to zero for all states, implying no future rewards beyond the time horizon $T$.\n",
    "   - `policy`: The **policy** $\\pi_t(s)$ specifies the action to take from each state $s$ at every time step $t$. The policy is initialized to take a default action (the first action in the list, `actions[0]`) for every state at each step.\n",
    "\n",
    "**Core Steps in Finite-Horizon Policy Iteration:**\n",
    "\n",
    "1. **Policy Evaluation** (`policy_evaluation` function):\n",
    "   * This function is given a current policy, and it calculates the value function for that policy by solving the Bellman equation:\n",
    "     $$V_t(s) = \\sum_{s'} P(s'|s, \\pi_t(s)) \\left[ R(s, \\pi_t(s), s') + V_{t+1}(s') \\right]$$\n",
    "     - For each state $s$, it queries the next possible states $s'$ and their associated probabilities (`mdp.get_transitions(state, action)`) and the corresponding rewards across these transitions.\n",
    "     - Using these, it calculates the expected discounted reward and assigns it to the value function `V1[state]`. This is done iteratively until the value function converges (i.e., updates are smaller than `epsilon`).\n",
    "   * The evaluated value function for the current policy at time step $t$ is returned once it converges.\n",
    "\n",
    "2. **Policy Improvement**:\n",
    "   * After evaluating the current policy, the next step is **policy improvement**, where the algorithm updates the policy to a \"better\" one if possible:\n",
    "     - For each state $s$, the algorithm explores all possible actions.\n",
    "     - It calculates the **expected value** for taking each action $a$ (similar to Bellman backups):\n",
    "       $$Q_t(s, a) = \\sum_{s'} P(s'|s, a) \\left[ R(s, a, s') + V_{t+1}(s') \\right]$$\n",
    "     - The action that yields the highest Q-value is selected as the new action for `policy[t][state]`.\n",
    "     - If the new action is different from the previous action (`old_action != best_action`), the policy is updated, and the algorithm marks the policy as **unstable** (i.e., the policy has changed).\n",
    "   \n",
    "   * If the policy doesn't change for any state when compared to the old policy, then the policy is considered **stable**, and policy iteration terminates for this time-step.\n",
    "\n",
    "3. **Backward Iteration**:\n",
    "   * The outer loop iterates **backward** from $t = T-1$ to $t = 0$. For each time step $t$, it alternates between **policy evaluation** and **policy improvement** until the policy stabilizes at that time step.\n",
    "\n",
    "**Algorithm Sequence:**\n",
    "\n",
    "For each time step $t$ (starting from $T-1$ and going backward to $0$):\n",
    "1. **Policy Evaluation** is performed to compute the value function $V_t(s)$ for each state $s$ under the current policy.\n",
    "2. **Policy Improvement** then adjusts the policy based on the current value function to improve the overall reward.\n",
    "3. This process repeats (Evaluate → Improve) until the policy converges for the current time step. When the policy is stable at that $t$, the algorithm moves to a previous time step ($t-1$) and begins the process again.\n",
    "\n",
    "**Exit Condition:**\n",
    "\n",
    "The algorithm terminates once the policy becomes stable for all time steps, meaning no further changes occur in the policy in any iteration.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "* **`V`**: A dictionary representing the value function $V_t(s)$ for each state $s$ at each time step $t$.\n",
    "* **`policy`**: The final policy $\\pi_t(s)$, indicating the optimal action to take at each time step $t$ and in each state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run $H = 7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| State   | Time 1        | Time 2        | Time 3        | Time 4        | Time 5        | Time 6       | Time 7       |\n",
      "+=========+===============+===============+===============+===============+===============+==============+==============+\n",
      "| S0      | 3.00 (buy)    | 3.00 (buy)    | 2.00 (buy)    | 2.00 (buy)    | 1.00 (buy)    | 1.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S1      | 4.00 (buy)    | 3.00 (buy)    | 3.00 (buy)    | 2.00 (buy)    | 2.00 (buy)    | 1.00 (buy)   | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S2      | 4.00 (buy)    | 4.00 (buy)    | 3.00 (buy)    | 3.00 (buy)    | 2.00 (buy)    | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S3      | 100.00 (buy)  | 4.00 (buy)    | 4.00 (buy)    | 3.00 (buy)    | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S4      | 100.00 (buy)  | 100.00 (buy)  | 4.00 (buy)    | 4.00 (sell)   | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S5      | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy)  | 4.00 (sell)   | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S6      | 101.00 (buy)  | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy)  | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S7      | 102.00 (buy)  | 101.00 (buy)  | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy)  | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S8      | 102.00 (buy)  | 102.00 (buy)  | 101.00 (buy)  | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy) | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S9      | 103.00 (sell) | 102.00 (sell) | 102.00 (sell) | 101.00 (sell) | 101.00 (sell) | 100.00 (buy) | 100.00 (buy) |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S10     | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "T = 7\n",
    "V_finite, policy_finite = finite_horizon_value_iteration(mdp, T)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = [\"State\"]\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    td = [state]\n",
    "    for t in range(T):\n",
    "        headers.append(f\"Time {t+1}\")\n",
    "        td.append(f\"{V_finite[t][state]:.2f} ({policy_finite[t][state]})\")\n",
    "    table_data.append(td)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if we execute the optimal policy on the agent, we will got: \n",
    "\n",
    "S3 --(buy)--> S4 --(buy)--> S5 --(buy)--> S6 --(buy)--> S7 --(buy)--> S8 --(buy)--> S9 --(buy)--> S10\n",
    "\n",
    "which only contains \"buy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| State   | Time 1        | Time 2        | Time 3        | Time 4        | Time 5        | Time 6       | Time 7       |\n",
      "+=========+===============+===============+===============+===============+===============+==============+==============+\n",
      "| S0      | 3.00 (buy)    | 3.00 (buy)    | 2.00 (buy)    | 2.00 (buy)    | 1.00 (buy)    | 1.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S1      | 4.00 (buy)    | 3.00 (buy)    | 3.00 (buy)    | 2.00 (buy)    | 2.00 (buy)    | 1.00 (buy)   | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S2      | 4.00 (buy)    | 4.00 (buy)    | 3.00 (buy)    | 3.00 (buy)    | 2.00 (buy)    | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S3      | 100.00 (buy)  | 4.00 (buy)    | 4.00 (buy)    | 3.00 (buy)    | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S4      | 100.00 (buy)  | 100.00 (buy)  | 4.00 (buy)    | 4.00 (sell)   | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S5      | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy)  | 4.00 (sell)   | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S6      | 101.00 (buy)  | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy)  | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S7      | 102.00 (buy)  | 101.00 (buy)  | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy)  | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S8      | 102.00 (buy)  | 102.00 (buy)  | 101.00 (buy)  | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy) | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S9      | 103.00 (sell) | 102.00 (sell) | 102.00 (sell) | 101.00 (sell) | 101.00 (sell) | 100.00 (buy) | 100.00 (buy) |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S10     | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+---------------+---------------+---------------+---------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "T = 7\n",
    "V_finite, policy_finite = finite_horizon_policy_iteration(mdp, T)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = [\"State\"]\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    td = [state]\n",
    "    for t in range(T):\n",
    "        headers.append(f\"Time {t+1}\")\n",
    "        td.append(f\"{V_finite[t][state]:.2f} ({policy_finite[t][state]})\")\n",
    "    table_data.append(td)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, just like the policy of VI if we execute the optimal policy on the agent, we will got: \n",
    "\n",
    "S3 --(buy)--> S4 --(buy)--> S5 --(buy)--> S6 --(buy)--> S7 --(buy)--> S8 --(buy)--> S9 --(buy)--> S10\n",
    "\n",
    "which only contains \"buy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run $H = 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------+--------------+\n",
      "| State   | Time 1        | Time 2       | Time 3       |\n",
      "+=========+===============+==============+==============+\n",
      "| S0      | 1.00 (buy)    | 1.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S1      | 2.00 (buy)    | 1.00 (buy)   | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S2      | 2.00 (buy)    | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S3      | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S4      | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S5      | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S6      | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S7      | 100.00 (buy)  | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S8      | 100.00 (buy)  | 100.00 (buy) | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S9      | 101.00 (sell) | 100.00 (buy) | 100.00 (buy) |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S10     | 0.00 (buy)    | 0.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "T = 3\n",
    "V_finite, policy_finite = finite_horizon_value_iteration(mdp, T)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = [\"State\"]\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    td = [state]\n",
    "    for t in range(T):\n",
    "        headers.append(f\"Time {t+1}\")\n",
    "        td.append(f\"{V_finite[t][state]:.2f} ({policy_finite[t][state]})\")\n",
    "    table_data.append(td)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if we execute the optimal policy on the agent, we will got: \n",
    "\n",
    "S3 --(sell)--> S2 --(sell)--> S1 --(sell)--> S0\n",
    "\n",
    "which only contains \"sell\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------+--------------+\n",
      "| State   | Time 1        | Time 2       | Time 3       |\n",
      "+=========+===============+==============+==============+\n",
      "| S0      | 1.00 (buy)    | 1.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S1      | 2.00 (buy)    | 1.00 (buy)   | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S2      | 2.00 (buy)    | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S3      | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S4      | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S5      | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S6      | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S7      | 100.00 (buy)  | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S8      | 100.00 (buy)  | 100.00 (buy) | 1.00 (sell)  |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S9      | 101.00 (sell) | 100.00 (buy) | 100.00 (buy) |\n",
      "+---------+---------------+--------------+--------------+\n",
      "| S10     | 0.00 (buy)    | 0.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "T = 3\n",
    "V_finite, policy_finite = finite_horizon_policy_iteration(mdp, T)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = [\"State\"]\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    td = [state]\n",
    "    for t in range(T):\n",
    "        headers.append(f\"Time {t+1}\")\n",
    "        td.append(f\"{V_finite[t][state]:.2f} ({policy_finite[t][state]})\")\n",
    "    table_data.append(td)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, just like the policy of PI if we execute the optimal policy on the agent, we will got: \n",
    "\n",
    "S3 --(sell)--> S2 --(sell)--> S1 --(sell)--> S0\n",
    "\n",
    "which only contains \"sell\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run $H = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| State   | Time 1        | Time 2        | Time 3        | Time 4       | Time 5       |\n",
      "+=========+===============+===============+===============+==============+==============+\n",
      "| S0      | 2.00 (buy)    | 2.00 (buy)    | 1.00 (buy)    | 1.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S1      | 3.00 (buy)    | 2.00 (buy)    | 2.00 (buy)    | 1.00 (buy)   | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S2      | 3.00 (buy)    | 3.00 (buy)    | 2.00 (buy)    | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S3      | 4.00 (buy)    | 3.00 (buy)    | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S4      | 4.00 (buy)    | 4.00 (sell)   | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S5      | 100.00 (buy)  | 4.00 (sell)   | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S6      | 100.00 (buy)  | 100.00 (buy)  | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S7      | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy)  | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S8      | 101.00 (buy)  | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy) | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S9      | 102.00 (sell) | 101.00 (sell) | 101.00 (sell) | 100.00 (buy) | 100.00 (buy) |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S10     | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "T = 5\n",
    "V_finite, policy_finite = finite_horizon_value_iteration(mdp, T)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = [\"State\"]\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    td = [state]\n",
    "    for t in range(T):\n",
    "        headers.append(f\"Time {t+1}\")\n",
    "        td.append(f\"{V_finite[t][state]:.2f} ({policy_finite[t][state]})\")\n",
    "    table_data.append(td)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if we execute the optimal policy on the agent, we will got: \n",
    "\n",
    "S3 --(buy)--> S4 --(sell)--> S3 --(sell)--> S2 --(sell)--> S1 --(sell)--> S0\n",
    "\n",
    "which is a mix of \"buy\" and \"sell\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| State   | Time 1        | Time 2        | Time 3        | Time 4       | Time 5       |\n",
      "+=========+===============+===============+===============+==============+==============+\n",
      "| S0      | 2.00 (buy)    | 2.00 (buy)    | 1.00 (buy)    | 1.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S1      | 3.00 (buy)    | 2.00 (buy)    | 2.00 (buy)    | 1.00 (buy)   | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S2      | 3.00 (buy)    | 3.00 (buy)    | 2.00 (buy)    | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S3      | 4.00 (buy)    | 3.00 (buy)    | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S4      | 4.00 (buy)    | 4.00 (sell)   | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S5      | 100.00 (buy)  | 4.00 (sell)   | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S6      | 100.00 (buy)  | 100.00 (buy)  | 3.00 (sell)   | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S7      | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy)  | 2.00 (sell)  | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S8      | 101.00 (buy)  | 101.00 (buy)  | 100.00 (buy)  | 100.00 (buy) | 1.00 (sell)  |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S9      | 102.00 (sell) | 101.00 (sell) | 101.00 (sell) | 100.00 (buy) | 100.00 (buy) |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n",
      "| S10     | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)    | 0.00 (buy)   | 0.00 (buy)   |\n",
      "+---------+---------------+---------------+---------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "T = 5\n",
    "V_finite, policy_finite = finite_horizon_policy_iteration(mdp, T)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = [\"State\"]\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    td = [state]\n",
    "    for t in range(T):\n",
    "        headers.append(f\"Time {t+1}\")\n",
    "        td.append(f\"{V_finite[t][state]:.2f} ({policy_finite[t][state]})\")\n",
    "    table_data.append(td)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, just like the policy of VI if we execute the optimal policy on the agent, we will got: \n",
    "\n",
    "S3 --(buy)--> S4 --(sell)--> S3 --(sell)--> S2 --(sell)--> S1 --(sell)--> S0\n",
    "\n",
    "which is a mix of \"buy\" and \"sell\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "\n",
    "In the infinite-horizon setting, obtain the optimal policy for $\\lambda = 0$, and compare it with your answer in part (c) above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Infinite Horizon Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite Horizon Value Iteration\n",
    "def infinite_horizon_value_iteration(mdp, epsilon=1e-5):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions(None)\n",
    "    discount_factor = mdp.get_discount_factor()\n",
    "    \n",
    "    # Initialize value function arbitrarily\n",
    "    V = {state: 0 for state in states}\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = V.copy()\n",
    "        \n",
    "        for state in states:\n",
    "            max_value = max(\n",
    "                sum(prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action))\n",
    "                for action in actions\n",
    "            )\n",
    "            new_V[state] = max_value\n",
    "            delta = max(delta, abs(V[state] - new_V[state]))\n",
    "        \n",
    "        V = new_V\n",
    "        \n",
    "        # Check for convergence\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    # Once we've converged, we can extract the optimal policy\n",
    "    policy = {}\n",
    "    for state in states:\n",
    "        action_values = {}\n",
    "        for action in actions:\n",
    "            action_values[action] = sum(\n",
    "                prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                for next_state, prob in mdp.get_transitions(state, action)\n",
    "            )\n",
    "        policy[state] = max(action_values, key=action_values.get)\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Infinite Horizon Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_horizon_policy_iteration(mdp, epsilon=1e-5):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions(None)\n",
    "    discount_factor = mdp.get_discount_factor()\n",
    "    \n",
    "    # Step 1: Initialize to a random policy\n",
    "    policy = {state: actions[0] for state in states}\n",
    "    \n",
    "    # Step 2: Initialize value function arbitrarily\n",
    "    V = {state: 0 for state in states}\n",
    "    \n",
    "    def policy_evaluation(policy, V):\n",
    "        \"\"\"\n",
    "        Evaluate the given policy by solving V(s) = sum over next states[T(s, pi(s), s') * (R(s, pi(s), s') + γ * V(s'))].\n",
    "        The function performs iterative evaluation until V converges.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_V = V.copy()\n",
    "            \n",
    "            for state in states:\n",
    "                action = policy[state]\n",
    "                new_V[state] = sum(\n",
    "                    prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action)\n",
    "                )\n",
    "                delta = max(delta, abs(V[state] - new_V[state]))\n",
    "            \n",
    "            V = new_V\n",
    "            \n",
    "            # Check for convergence\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "        \n",
    "        return V\n",
    "    \n",
    "    while True:\n",
    "        # Step 3: Policy Evaluation -> Compute V given current policy\n",
    "        V = policy_evaluation(policy, V)\n",
    "        \n",
    "        policy_stable = True\n",
    "        \n",
    "        # Step 4: Policy Improvement -> Greedily improve the policy\n",
    "        for state in states:\n",
    "            # Find the best action according to the current value function V\n",
    "            old_action = policy[state]\n",
    "            action_values = {}\n",
    "            \n",
    "            for action in actions:\n",
    "                action_values[action] = sum(\n",
    "                    prob * (mdp.get_reward(state, action, next_state) + discount_factor * V[next_state])\n",
    "                    for next_state, prob in mdp.get_transitions(state, action)\n",
    "                )\n",
    "            \n",
    "            # Choose the action that gives maximum value\n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            policy[state] = best_action\n",
    "            \n",
    "            # If the policy did change, we're not yet stable\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        # Step 5: Check if the policy is stable and terminate if it is\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MDP\n",
    "mdp = MDP(states, actions, reward_func, transition_func, discount_factor=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for infinite horizon solution\n",
    "value_vi, policy_vi = infinite_horizon_value_iteration(mdp)\n",
    "value_pi, policy_pi = infinite_horizon_policy_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+\n",
      "| State   | VI (0)   | PI (0)   |\n",
      "+=========+==========+==========+\n",
      "| S0      | buy      | buy      |\n",
      "+---------+----------+----------+\n",
      "| S1      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S2      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S3      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S4      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S5      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S6      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S7      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S8      | sell     | sell     |\n",
      "+---------+----------+----------+\n",
      "| S9      | buy      | buy      |\n",
      "+---------+----------+----------+\n",
      "| S10     | buy      | buy      |\n",
      "+---------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    action_vi = policy_vi.get(state, 'N/A')\n",
    "    action_pi = policy_pi.get(state, 'N/A')\n",
    "    table_data.append([state, action_vi, action_pi])\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"State\", \"VI (0)\", \"PI (0)\"]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the optimal action for agent in $S_3$ is **to sell**, and in $S_9$ is **to buy**, as we discussed in part (c).\n",
    "\n",
    "---\n",
    "\n",
    "The result of both **VI** and **PI** is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c\n",
    "\n",
    "In the infinite-horizon setting, obtain the optimal policy separately for $\\lambda = 0.1$ and $\\lambda = 0.9$, and compare them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MDP\n",
    "mdp_1 = MDP(states, actions, reward_func, transition_func, discount_factor=0.1)\n",
    "mdp_9 = MDP(states, actions, reward_func, transition_func, discount_factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for infinite horizon solution\n",
    "value_vi_1, policy_vi_1 = infinite_horizon_value_iteration(mdp_1)\n",
    "value_pi_1, policy_pi_1 = infinite_horizon_policy_iteration(mdp_1)\n",
    "value_vi_9, policy_vi_9 = infinite_horizon_value_iteration(mdp_9)\n",
    "value_pi_9, policy_pi_9 = infinite_horizon_policy_iteration(mdp_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+-------------+------------+\n",
      "| State   | VI (0.1)   | PI (0.9)   |  VI (0.1)   | PI (0.9)   |\n",
      "+=========+============+============+=============+============+\n",
      "| S0      | buy        | buy        | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S1      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S2      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S3      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S4      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S5      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S6      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S7      | sell       | sell       | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S8      | buy        | buy        | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S9      | buy        | buy        | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n",
      "| S10     | buy        | buy        | buy         | buy        |\n",
      "+---------+------------+------------+-------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "for state in states:\n",
    "    action_vi_1 = policy_vi_1.get(state, 'N/A')\n",
    "    action_pi_1 = policy_pi_1.get(state, 'N/A')\n",
    "    action_vi_9 = policy_vi_9.get(state, 'N/A')\n",
    "    action_pi_9 = policy_pi_9.get(state, 'N/A')\n",
    "    table_data.append([state, action_vi_1, action_pi_1, action_vi_9, action_pi_9])\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"State\", \"VI (0.1)\", \"PI (0.9)\", \" VI (0.1)\", \"PI (0.9)\"]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for optimal policies abotained by both **VI** and **PI**, the policies are the same.\n",
    "\n",
    "For $\\gamma = 0.1$ (first second columns), the policy is to sell first three shares, and then repeatedly buy and sell next shares. This is because the immediate rewards are more important. \n",
    "\n",
    "For $\\gamma = 0.9$ (next second columns), the policy is to buy shares and reach $S_10$ to receive +100 reard. This is because late rewards are valued here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all parts of this question, the optimal policy is calculated with both **VI** and **PI**, and the results are compared. However, time analysis was not conisdered in this question, as it was donw in the previous question for two different MDPs. Overall, PI converages a bit later as opposed to VI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
