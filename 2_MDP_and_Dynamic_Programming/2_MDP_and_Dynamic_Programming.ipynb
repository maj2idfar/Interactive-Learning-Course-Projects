{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA 2, Interactive Learning, Fall 2024\n",
    "- **Name**: Majid Faridfar\n",
    "- **Student ID**: 810199569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "What is the difference between reinforcement learning and supervised learning? Explain by providing two similar problems: one that requires reinforcement learning to solve, and another that can be solved with supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences:\n",
    "- How to learn? \n",
    "  - RL is based on an agent interacting with an environment. The agent takes actions, receives feedback in the form of rewards or penalties, and aims to maximize the cumulative reward over time. The feedback is sparse and often delayed, and there’s no explicit label or correct answer for each action.\n",
    "  - SL involves learning from labeled data where each input comes with a corresponding correct output (label). The model is trained to map inputs to outputs based on this labeled dataset.\n",
    "\n",
    "- What is the goal?\n",
    "  - The goal of RL is to learn a strategy (or policy) that will maximize the total reward over time. The agent learns through trial and error, exploring and exploiting the environment to improve its decisions.\n",
    "  - The goal of SL is to minimize the error between the predicted output and the true label, thereby learning a mapping function from inputs to outputs.\n",
    "\n",
    "Illustration (Maze):\n",
    "- A robot (agent) is placed in a maze (environment) and has to navigate from the start to the goal. The robot takes actions such as moving left, right, up, or down. Initially, it doesn’t know the best path to take and receives feedback in the form of rewards (e.g., +1 for reaching the goal, -1 for hitting a wall or making a wrong turn). The goal is to maximize the cumulative reward by learning which actions lead to the goal. The robot learns over time by exploring different paths, receiving feedback, and adjusting its strategy based on the rewards it receives.\n",
    "- Instead of having the robot learn by exploring the maze, you collect a dataset of maze configurations (states) and the correct sequence of moves (labels) that leads to the goal. For each configuration, you have a label indicating the correct next action (e.g., move left, move right, etc.). You train a model on this labeled dataset to predict the correct action for a given maze state. Once trained, the model can predict the correct move for any new maze configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "In an MDP problem, if the reward function undergoes a linear transformation, does the optimal policy change? (Provide a mathematical proof or a counterexample, and ignore the trivial case of multiplying by zero.) Does the answer to this question depend on whether the task is continuing or episodic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general linear transformation of a reward function $R(s, a)$ is:\n",
    "\n",
    "$$R'(s, a) = \\alpha R(s, a) + \\beta$$\n",
    "where $\\alpha > 0$ and $\\beta$ is a scalar.\n",
    "\n",
    "Here, we will show that when the reward function undergoes a linear transformation like this, the optimal policy does **not** change. This conclusion holds for both continuing and episodic tasks.\n",
    "\n",
    "### Understanding Optimal Policies:\n",
    "\n",
    "The goal in an MDP is to find an optimal policy $\\pi^*$ that maximizes the **expected cumulative reward**. Depending on whether the problem is episodic or continuing, this objective is either the total expected reward (episodic task) or the expected discounted cumulative reward (continuing task). We’ll focus on the discounted scenario, as the principles generalize well.\n",
    "\n",
    "In a discounted setting, the objective is to maximize the expected return,\n",
    "\n",
    "$$G_t = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) | s_t = s, a_t = a \\right]$$\n",
    "where $\\gamma$ is the discount factor such that $0 \\leq \\gamma \\leq 1$ (if $\\gamma$ is equal to $1$, then we are calculating the expected cumulative reward).\n",
    "\n",
    "We define the value function $V^\\pi(s)$ under a policy $\\pi$ as:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) \\middle| s_t = s \\right]$$\n",
    "\n",
    "Similarly, the action-value function or Q-value function $Q^\\pi(s, a)$ is:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) \\middle| s_t = s, a_t = a \\right]$$\n",
    "\n",
    "The optimal policy is the one that maximizes the value or Q-value function, satisfying:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n",
    "\n",
    "#### Effect on the Value Function:\n",
    "\n",
    "With the new transformed reward function $R'(s, a) = \\alpha R(s, a) + \\beta$, the new value function $V'^\\pi(s)$ under policy $\\pi$ becomes:\n",
    "\n",
    "$$V'^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R'(s_{t+k}, a_{t+k}) \\middle| s_t = s \\right]\n",
    "         = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k (\\alpha R(s_{t+k}, a_{t+k}) + \\beta) \\middle| s_t = s \\right]$$\n",
    "\n",
    "This expands into:\n",
    "\n",
    "$$V'^\\pi(s) = \\alpha \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, a_{t+k}) \\middle| s_t = s \\right]\n",
    "         + \\beta \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k \\middle| s_t = s \\right]$$\n",
    "\n",
    "Shortening the notation, we have:\n",
    "\n",
    "$$V'^\\pi(s) = \\alpha V^\\pi(s) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "where the sum $\\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}$ because it is a geometric series.\n",
    "\n",
    "#### Effect on the Q-Value Function:\n",
    "\n",
    "Similarly, the transformed Q-value function would be:\n",
    "\n",
    "$$Q'^\\pi(s, a) = \\alpha Q^\\pi(s, a) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "### Does the Optimal Policy Change?\n",
    "\n",
    "Answer is no. The optimal policy $\\pi^*(s)$ is determined by selecting the action $a$ that maximizes the Q-value function:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n",
    "\n",
    "When applying the linear transformation, the transformed Q-value function satisfies:\n",
    "\n",
    "$$Q'^\\pi(s, a) = \\alpha Q^\\pi(s, a) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "Since $\\alpha > 0$, this transformation preserves the order of the Q-values. That is, if action $a_1$ was better than action $a_2$ before the transformation (i.e., $Q^*(s, a_1) > Q^*(s, a_2)$), it will remain better after the transformation because the transformation is a positive linear scaling and a constant shift, both of which do not affect the relative ordering of numbers:\n",
    "\n",
    "$$Q'^\\pi(s, a_1) = \\alpha Q^\\pi(s, a_1) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "$$Q'^\\pi(s, a_2) = \\alpha Q^\\pi(s, a_2) + \\beta \\frac{1}{1 - \\gamma}$$\n",
    "\n",
    "Thus, $Q^\\pi(s, a_1) > Q^\\pi(s, a_2) \\Rightarrow Q'^\\pi(s, a_1) > Q'^\\pi(s, a_2)$.\n",
    "\n",
    "Since the relative rankings of the Q-values are preserved, the optimal action $\\arg\\max_a Q^*(s, a)$ does not change. Therefore, the optimal policy remains the same.\n",
    "\n",
    "The answer does not depend on whether the task is continuing or episodic. In an episodic task, you would also be maximizing the expected return, but limited to a certain number of steps before the episode ends. The reasoning around the transformation of the reward function and its impact on the Q-value functions still holds because the transformation is applied uniformly across the rewards, and the policy that maximizes value remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Assume a robot operates in a grid environment as follows. In each episode, the robot starts in one of the cells in the bottom row (with an equal probability of starting in each cell). The robot can move left, right, or up. If the robot chooses to move in a direction where there is a wall, it remains in place. If the robot enters one of the green cells, the episode ends.\n",
    "\n",
    "![pics/P3.png](pics/P3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1\n",
    "\n",
    "The robot knows the grid environment completely and is aware of its current location at any moment, using this information to make decisions. The goal is for the robot to reach the second row and enter one of the green cells. If the robot enters a green cell, it receives a reward of +1 (and the episode ends), and if it moves from one blue cell to another blue cell, it receives a reward of 0. For this task, $\\lambda = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "Can the defined task be represented by an MDP? If not, explain why; if yes, fully specify the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this task can be defined by an MDP. To fully specify the Markov Decision Process (MDP) for this task, we need to define **States (S)**, **Actions (A)**, **Transition probabilities (P(s'|s,a))**, **Rewards (R(s,a))**, and **Discount Factor (λ)**.\n",
    "\n",
    "##### States (S):\n",
    "The states are the cells in the grid:\n",
    "- S1, S2, S3, S4, S5, S6, S7 represent the blue cells.\n",
    "- T1, T3, and T5 are the green terminal cells (absorbing states).\n",
    "\n",
    "So the state space is:\n",
    "$$S = \\{S1, S2, S3, S4, S5, S6, S7, T1, T3, T5\\}$$\n",
    "\n",
    "##### Actions (A):\n",
    "The robot has three possible actions in the bottom row: **Up (U)**, **Left (L)**, **Right (R)**. However, when the robot is in the terminal green cells T1, T3, T5, the episode ends, and no further actions are available.\n",
    "\n",
    "Thus, the action set is:\n",
    "$$A = \\{\\text{Up}, \\text{Left}, \\text{Right}\\}$$\n",
    "\n",
    "##### Transition Probabilities (P(s' | s, a)):\n",
    "The robot transitions deterministically (we assume that there is no stochastic behavior, since it is not mentioned in the problem), so the probability of transitioning from one state to another depends on whether the robot encounters a wall.\n",
    "\n",
    "- Moving \"Left\" in state S1 will not change the state.\n",
    "- Moving \"Up\" in states S2, S4, S6 will move the robot into the green cells (and stop the episode), while in other states will not change the state.\n",
    "- Moving \"Right\" in state S7 will not change the state.\n",
    "\n",
    "Here are the key deterministic transitions:\n",
    "\n",
    "- **Up movement**:\n",
    "  - $P(T1 | S2, U) = 1$\n",
    "  - $P(T3 | S4, U) = 1$\n",
    "  - $P(T5 | S6, U) = 1$\n",
    "  - Other cases: The robot stays in the current state if \"Up\" is attempted from a cell without a green terminal cell above it.\n",
    "\n",
    "- **Left movement**:\n",
    "  - $P(S1 | S2, L) = 1$\n",
    "  - $P(S2 | S3, L) = 1$\n",
    "  - $P(S3 | S4, L) = 1$\n",
    "  - $P(S4 | S5, L) = 1$\n",
    "  - $P(S5 | S6, L) = 1$\n",
    "  - $P(S6 | S7, L) = 1$\n",
    "  - Other cases: The robot stays in the current state if \"Left\" is attempted from  S1.\n",
    "\n",
    "- **Right movement**:\n",
    "  - $P(S2 | S1, R) = 1$\n",
    "  - $P(S3 | S2, R) = 1$\n",
    "  - $P(S4 | S3, R) = 1$\n",
    "  - $P(S5 | S4, R) = 1$\n",
    "  - $P(S6 | S5, R) = 1$\n",
    "  - $P(S7 | S6, R) = 1$\n",
    "  - Other cases: The robot stays in the current state if \"Right\" is attempted from  S7.\n",
    "\n",
    "##### Rewards (R(s,a)):\n",
    "The robot receives:\n",
    "- A reward of **+1** upon entering a terminal green cell (T1, T3, T5).\n",
    "- A reward of **0** for any movement from one blue cell to another blue cell.\n",
    "\n",
    "Thus, the rewards are:\n",
    "- $R(s' | s, U) = 1$ if the robot moves from S2 to T1, S4 to T3, or S6 to T5.\n",
    "- $R(s' | s, a) = 0$ for any other movement.\n",
    "\n",
    "For example:\n",
    "- $R(T1 | S2, U) = 1$,\n",
    "- $R(T3 | S4, U) = 1$,\n",
    "- $R(T5 | S6, U) = 1$,\n",
    "- $R(S2 | S3, L) = 0$,\n",
    "- $R(S6 | S5, R) = 0$, etc.\n",
    "\n",
    "##### Discount Factor $λ$:\n",
    "The **discount factor** is $λ = 0.9$. This means that rewards received in future states are weighted by a factor of 0.9 for every step into the future. It reflects the agent's preference for immediate rewards over delayed ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "How many optimal deterministic policies exist for solving this task? Appropriately express $\\pi(s)$ for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the optimal policy, we go state by state and analyze the optimal actions:\n",
    "\n",
    "1. **S1**: The only possible optimal action here is to move **right**, since other actions are blocked by a wall. Thus, for $\\pi(S1)$:\n",
    "$$\\pi(S1) = \\text{R}$$\n",
    "- Hence, there’s only **one optimal action** in S1.\n",
    "\n",
    "2. **S2**: The optimal way to immediately end the episode and earn a reward is to move **Up** into the green terminal state T1. Thus:\n",
    "$$\\pi(S2) = \\text{U}$$\n",
    "- Hence, there’s only **one optimal action** in S2.\n",
    "\n",
    "3. **S3**: Moving **Up** is not blocked by the above wall. From S3, the robot can either move **left** to S2, letting it access the green cell T1, or move **right** to S4, potentially heading toward T3. Since rewards are the same (all terminal cells give +1 and actions cost 0), **both left and right are equally optimal** choices. Therefore:\n",
    "$$\\pi(S3) = \\text{L or R}$$\n",
    "- Hence, there are **two optimal actions** in S3.\n",
    "\n",
    "4. **S4**: The optimal action in state S4 is to move **Up** into T3 immediately to end the episode. Thus:\n",
    "$$\\pi(S4) = \\text{U}$$\n",
    "- Hence, there’s only **one optimal action** in S4.\n",
    "\n",
    "5. **S5**: Robot can move **right** toward S6 to reach the green terminal T5, or **left** to S4 and reach T3. Since both terminal cells T3 and T5 give the same reward, either direction is optimal. Therefore:\n",
    "$$\\pi(S5) = \\text{L or R}$$\n",
    "- Hence, there are **two optimal choices** in S5.\n",
    "\n",
    "6. **S6**: The optimal action is to go **Up** to T5 immediately. Thus:\n",
    "$$\\pi(S6) = \\text{U}$$\n",
    "- Hence, there’s only **one optimal action** in S6.\n",
    "\n",
    "7. **S7**: Since S7 has no option but to move **left** (right and up are blocked by a wall), the optimal policy is:\n",
    "$$\\pi(S7) = \\text{L}$$\n",
    "- Hence, there’s only **one optimal action** in S7.\n",
    "\n",
    "---\n",
    "\n",
    "- In **S1, S2, S4, S6, S7**, the robot has only **one optimal action**.\n",
    "- In **S3, S5**, the robot has **two optimal choices**: move left or right.\n",
    "\n",
    "Therefore, the total number of **optimal deterministic policies** is:\n",
    "\n",
    "$$\\text{Total optimal policies} = 2 \\times 2 = 4$$\n",
    "\n",
    "---\n",
    "\n",
    "![pics/P3S1b.png](pics/P3S1b.png)\n",
    "\n",
    "1. **Policy 1**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{L}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{L}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$\n",
    "   \n",
    "2. **Policy 2**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{L}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{R}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$\n",
    "\n",
    "3. **Policy 3**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{R}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{L}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$\n",
    "\n",
    "4. **Policy 4**:\n",
    "$$\\pi(S1) = \\text{R}, \\quad \\pi(S2) = \\text{U}, \\quad \\pi(S3) = \\text{R}, \\quad \\pi(S4) = \\text{U}, \\quad \\pi(S5) = \\text{R}, \\quad \\pi(S6) = \\text{U}, \\quad \\pi(S7) = \\text{L}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2\n",
    "\n",
    "The robot has no knowledge of the environment it's in. It has distance sensors on all four sides that tell it whether or not there is a wall next to it. For example, consider the images below.\n",
    "\n",
    "![pics/P3S2.png](pics/P3S2.png)\n",
    "\n",
    "Assume the objective is similar to the first scenario. In terms of key elements of an MDP, what has changed here? Can this problem still be represented by an MDP in a way that ultimately fulfills our objective? (Is the optimal policy what we want it to be?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the problem can still be represented by a **Markov Decision Process (MDP)**, as it fundamentally satisfies the requirements for an MDP. But there are key changes because the robot no longer knows exactly where it is within the environment.\n",
    "\n",
    "#### State Space (S)\n",
    "In general, states in this MDP are based on the robot’s sensory input: `{right, up, left, down}`. Each direction can either be blocked (0) or unblocked (1), thus defining the state space. Each state represents the robot’s local sensory information and position within the context of the grid. To simplify, we can define states $S_1, S_2, S_3, ...$ based on these sensory inputs.\n",
    "\n",
    "- **$S_1$**: Sensor reading `{right: 1, up: 0, left: 0, down: 0}`.\n",
    "- **$S_2$**: Sensor reading `{right: 0, up: 0, left: 1, down: 0}`.\n",
    "- **$S_3$**: Sensor reading `{right: 1, up: 0, left: 1, down: 0}`.\n",
    "- **$S_4$** (Ambiguous state): Sensor reading `{right: 1, up: 1, left: 1, down: 0}`.\n",
    "- **$S_5$** (terminal state): Sensor reading `{right: 0, up: 0, left: 0, down: 1}`. In can be either green or red.\n",
    "\n",
    "Thus, the **finite state space $S$** is:\n",
    "$$S = \\{ S_1, S_2, S_3, S_4, S_5 \\}$$\n",
    "\n",
    "#### Action Space (A)\n",
    "We define actions based on directional movements:\n",
    "- **$A_1$**: **Move Up**. The robot tries to move to a cell directly above in this action.\n",
    "- **$A_2$**: **Move Right**. The robot moves to the neighboring cell to the right.\n",
    "- **$A_3$**: **Move Left**. The robot moves to the neighboring cell to the left.\n",
    "  \n",
    "Thus, the **action space $A$** is:\n",
    "$$A = \\{ A_1, A_2, A_3 \\} = \\{ \\text{Up}, \\text{Right}, \\text{Left} \\}$$\n",
    "\n",
    "#### Transition Function (P(s', s, a))\n",
    "Using states $S_1 \\ldots S_5$ and actions $A_1, A_2, A_3$, consider each transition:\n",
    "\n",
    "| State Transition        | Action: Up | Action: Right | Action: Left |\n",
    "|-------------------------|------------------------|-----------------------------|----------------------------|\n",
    "| **$S_1$ → $S_1$** | 1 (blocked)            | 0                            | 1 (blocked)                  |\n",
    "| **$S_1$ → $S_4$** | 0                      | 1                            | 0                            |\n",
    "| **$S_2$ → $S_2$** | 1 (blocked)            | 1 (blocked)                  | 0                            |\n",
    "| **$S_2$ → $S_4$** | 0                      | 0                            | 1                            |\n",
    "| **$S_3$ → $S_3$** | 1 (blocked)            | 0                            | 0                            |\n",
    "| **$S_3$ → $S_4$** | 0                      | 1                            | 1                            |\n",
    "| **$S_4$ → $S_3$** | 0                      | $\\frac{2}{3}$ (probabilistic)| $\\frac{2}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_2$** | 0                      | $\\frac{1}{3}$ (probabilistic)| 0                            |\n",
    "| **$S_4$ → $S_1$** | 0                      | 0                            | $\\frac{1}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_5$** | 1 (terminal)           | 0                            | 0                            |\n",
    "\n",
    "Above:\n",
    "- **Blocked transitions** (e.g., \"Up\" when no upward movement is possible) have a **probability of 1** for staying in the same state.\n",
    "- Probabilistic transitions from $S_4$ to $S_1$, $S_2$, or $S_3$ when moving left or right.\n",
    "- Moving **up** from $S_4$ always leads to the terminal state $S_5$ with probability **1**.\n",
    "\n",
    "#### Reward Function (R(s, a))\n",
    "The reward structure also changes slightly. Instead of rewards depending on grid positions, the **reward is now a function of the sensor state** and the action taken:\n",
    "\n",
    "- If the robot takes the **Up** action in a sensor state where **Up is allowed** (i.e., there's no wall above), and this move leads to a terminal green cell, the robot gets a reward of **+1**.\n",
    "- For all other valid moves (where no immediate green state is reached), the robot receives **0 reward**.\n",
    "- If the robot tries an invalid action (e.g., moving into a wall), it stays in the same state and still receives a reward of **0**.\n",
    "  \n",
    "So, the reward function can be expressed as:\n",
    "\n",
    "$$R(s' | s, a) = \n",
    "     \\begin{cases}\n",
    "       +1 & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}, and a is \\text{Up}} \\\\\n",
    "       0 & \\text{Otherwise}\n",
    "     \\end{cases}$$\n",
    "\n",
    "#### Discount Factor (λ)\n",
    "The problem didn't change the discount factor, so $\\lambda$ remains at 0.9, meaning that future rewards are discounted slightly but still carry weight.\n",
    "\n",
    "---\n",
    "\n",
    "The optimal policy in the original problem was to **quickly reach the green terminal cells** by moving up when possible (if there was no wall above) and moving horizontally if necessary.\n",
    "\n",
    "In this new formulation, the **optimal policy would still aim to achieve the same goal**—the robot must use its sensor readings to figure out:\n",
    "- When it's best to move **Up** into a terminal cell,\n",
    "- How to move **Left** or **Right** to position itself below a terminal (green) cell if it's not already there.\n",
    "\n",
    "Thus, **in the optimal policy**, the robot will:\n",
    "- Move **Up** when its sensors indicate that moving up is allowed (i.e., in a sensor state like `{right: 1, up: 1, left: 1, down: 0}`).\n",
    "- Move **Left** or **Right** when needed to position itself for optimal transitions.\n",
    "\n",
    "Thus, the optimal policy will likely involve:\n",
    "$$\n",
    "\\pi(s) = \\begin{cases} \n",
    "\\text{Up} & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}} \\\\\n",
    "\\text{Right/Left} & \\text{If s = \\{right: 1, up: 0, left: 1, down: 0\\}} \\\\\n",
    "\\text{Right} & \\text{If s = \\{right: 1, up: 0, left: 0, down: 0\\}} \\\\\n",
    "\\text{Left} & \\text{If s = \\{right: 0, up: 0, left: 1, down: 0\\}} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So, the **optimal policy** still fulfills the ultimate goal by appropriately guiding the robot to the green terminal cells using sensor-guided movements. Therefore, under this formulation, the robot will still eventually learn the optimal way to achieve the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3\n",
    "\n",
    "Assume everything is the same as in Scenario 2, but our objective has changed, and we want the robot to enter only the two green cells on the left and right. If the robot enters the red cell in the figure below, it will receive a reward of -1, and if it enters one of the green cells, it will receive a reward of +1 (all other rewards are zero). For this task, $\\lambda = 0.9$.\n",
    "\n",
    "![pics/P3S3.png](pics/P3S3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "Can this problem still be represented by an MDP in a way that ultimately fulfills our objective? (Is the optimal policy what we want it to be?) If it can be represented, provide the desired optimal policy. If it cannot, explain what should be done, given the robot's input data, so that the robot can find an optimal policy that meets our objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can indeed be represented by a **Markov Decision Process (MDP)**. In MDPs, the environment's dynamics are expressed through states, actions, transitions, and rewards (the MDP is fully provided in the next section of question). The robot’s states can be represented as its position in the grid, its actions are left, right, and up, the rewards are received when entering specific \"trial\" cells (green +1 and the red -1), and the transitions outline how the robot moves based on its actions.\n",
    "\n",
    "However, **the optimal policy derived from this MDP may not be the one we desire** due to the robot's sensory limitation. From the robot’s perspective, when it starts in the bottom row, each of the bottom cells under the green or red cells looks the same until it receives feedback from the current state or the environment (i.e., reaching a terminal state). The robot does not know which upper cell will end up being a red or green cell because, from its starting position, all adjacent walls or sensors are identical. As a result, the optimal policy might make it behave similarly in cells beneath the red and green cells, which is not aligned with the desired behavior — the robot should avoid the cell under the red cell and prefer moving toward the cells under the green cells.\n",
    "\n",
    "##### Improved Distance Sensors as a Solution:\n",
    "To solve this issue, one method would be to modify the sensing capabilities of the robot. Currently, the robot’s sensors only tell it whether there’s a wall directly adjacent to it. If we **extend the sensor range** to allow the robot to detect walls that are a bit farther away (say, one extra cell ahead), the robot can then detect the presence of walls that define the boundary of the grid. This information can be crucial because the robot could infer whether it is in a corner (leftmost or rightmost cell) and indirectly deduce it is situated under one of the green target cells. When the robot detects no side walls, it knows it must be under the red cell, and thus the correct action would be to avoid moving up.\n",
    "\n",
    "With this extension to the robot’s sensors, the robot is able to recognize its position as either closer to the green terminal cells (left and right sides) or closer to the red terminal cell in the center. This extra sensory information helps the robot find an **optimal policy** that adheres to the task requirement of maximizing the number of times it reaches the green cells and minimizing the number of times it reaches the red cell.\n",
    "\n",
    "##### Alternative Approaches:\n",
    "\n",
    "There are alternative ways to resolve this problem without increasing the sensor range:\n",
    "\n",
    "1. **Learning from Experience via Reinforcement Learning**:\n",
    "    Instead of adjusting the robot's sensors, you can allow the robot to learn the environment through experience in the form of **Reinforcement Learning (RL)** algorithms such as Q-learning or SARSA. By using feedback (positive and negative rewards), the robot can gradually learn which states lead to favorable outcomes (green cells) and which states lead to undesirable outcomes (red cell) without needing upfront modifications to its sensors. Through exploration, it can distinguish the three bottom cells indirectly by learning the rewards that result from moving up from those cells.\n",
    "   \n",
    "   After a sufficient amount of exploration, the robot will learn that moving up from the cells below the green ones gives rewards, while moving up from the one below the red gives a penalty. As RL focuses on maximizing cumulative reward, the optimal policy will be to favor actions that lead to the green cells and avoid actions that lead to the red cell.\n",
    "\n",
    "2. **Pre-training the Robot**:\n",
    "   If time or reliability is a concern with the RL approach, you could **pre-train the robot** in a simulated environment to have it discover the favorable policy before deployment. By running multiple simulations in diverse grid configurations beforehand, the robot can learn to recognize patterns related to corner cells (green cells) and mid-grid cells (red cell). Once trained with a policy that favors green cells and avoids the red cell, it can be deployed with that knowledge.\n",
    "\n",
    "3. **Adding State Identifiers**:\n",
    "   Another solution is to introduce **state-based identifiers** (labels or distinct features) in the cells themselves. If each cell in the lower row had some distinguishing feature, such as a slight variation (visual cue, color, or numeric label), the robot’s input data could contain enough information to differentiate between the different regions under the green and red cells. This could be more practical than adjusting the robot’s sensory hardware for some real-world applications.\n",
    "\n",
    "##### Final Thoughts:\n",
    "\n",
    "1. **Distance Sensors Enhancement**: Extending the sensors to perceive walls that are farther away allowing the robot to infer its position with better accuracy.\n",
    "\n",
    "2. **Reinforcement Learning**: Allowing the robot to learn over time what the rewards of different states and actions are will naturally lead it to avoid the red cell over many episodes.\n",
    "\n",
    "3. **Pre-training**: Using simulations to help the robot develop an appropriate policy before it encounters the actual environment.\n",
    "\n",
    "4. **State Identifiers**: Each bottom row cell can have distinguishing features that make differentiation possible with existing sensors.\n",
    "\n",
    "In summary, while this problem is representable as an MDP, the optimal policy won't directly reflect the task's requirements because of the limitations of the robot’s input data (sensors). Enhancing the sensors, investing in RL, or simply marking the states differently are viable ways to tune the robot's policy toward the task objective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "If you solve Scenario 3 using dynamic programming methods, what MDP does the optimal solution correspond to? Represent the states of this MDP with $s_j$ (for $j = 0, 1, \\dots$), and indicate which grid cells correspond to these states. Represent the actions with $a_i$ (for $i = 0, 1, 2, \\dots$) and specify which robot actions they correspond to. Define the probability matrix $P$ accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pics/P3S3b](pics/P3S3b.png)\n",
    "\n",
    "##### States $s_j$\n",
    "In general, states in this MDP are based on the robot’s sensory input: `{right, up, left, down}`. Each direction can either be blocked (0) or unblocked (1), thus defining the state space. Each state represents the robot’s local sensory information and position within the context of the grid. To simplify, we can define states $S_1, S_2, S_3, ...$ based on these sensory inputs.\n",
    "\n",
    "- **$S_1$**: Sensor reading `{right: 1, up: 0, left: 0, down: 0}`.\n",
    "- **$S_2$**: Sensor reading `{right: 0, up: 0, left: 1, down: 0}`.\n",
    "- **$S_3$**: Sensor reading `{right: 1, up: 0, left: 1, down: 0}`.\n",
    "- **$S_4$** (Ambiguous state): Sensor reading `{right: 1, up: 1, left: 1, down: 0}`.\n",
    "- **$S_5$** (terminal state): Sensor reading `{right: 0, up: 0, left: 0, down: 1}`. In can be either green or red.\n",
    "\n",
    "Thus, the **finite state space $S$** is:\n",
    "$$S = \\{ S_1, S_2, S_3, S_4, S_5 \\}$$\n",
    "\n",
    "##### Actions $a_i$\n",
    "We define actions based on directional movements:\n",
    "- **$A_1$**: **Move Up**. The robot tries to move to a cell directly above in this action.\n",
    "- **$A_2$**: **Move Right**. The robot moves to the neighboring cell to the right.\n",
    "- **$A_3$**: **Move Left**. The robot moves to the neighboring cell to the left.\n",
    "  \n",
    "Thus, the **action space $A$** is:\n",
    "$$A = \\{ A_1, A_2, A_3 \\} = \\{ \\text{Up}, \\text{Right}, \\text{Left} \\}$$\n",
    "\n",
    "##### Transition Function\n",
    "This part describes how the robot transitions between states given specific actions. We assume transitions are **deterministic** and depend on whether the action is allowable based on the current sensory readings.\n",
    "\n",
    "**Transition for Action $A_1$ (Up)**\n",
    "- **$S_1$ through $S_3$ → $S_1, S_2, S_3$** (No change)\n",
    "  - If the robot tries to move **Up** in states $S_1, S_2, S_3$ where **moving up is blocked**, the robot stays in the same state.\n",
    "  \n",
    "- **$S_4$ → $S_5$**\n",
    "  - If the robot moves **Up** from $S_4$, it reaches one of the terminal cells (green or red), hence transitioning to the terminal state $S_5$.\n",
    "\n",
    "**Transition for Action $A_2$ (Right)**\n",
    "- **$S_1$ → $S_4$**: Move from the far left $S_1$ into the ambiguous central position $S_4$.\n",
    "  \n",
    "- **$S_3$ → $S_4$**: Move within the middle. From $S_3$, moving right brings the robot to the ambiguous central position $S_4$.\n",
    "\n",
    "- **$S_4$ → $S_3$ or $S_2$**: Moving **right** from $S_4$ potentially takes the robot to $S_2$ (For $\\frac{1}{3}$ times-S6) or $S_3$ (For $\\frac{2}{3}$ times-S2 and S4).\n",
    "\n",
    "**Transition for Action $A_3$ (Left)**\n",
    "- **$S_2$ → $S_4$**: Move from the far right $S_2$ to the central ambiguous state $S_4$.\n",
    "  \n",
    "- **$S_3$ → $S_4$**: Move from a middle position to the ambiguous $S_4$.\n",
    "\n",
    "- **$S_4$ → $S_3$ or $S_1$**: Moving **left** from $S_4$ potentially takes the robot to $S_1$ (For $\\frac{1}{3}$ times-S2) or $S_3$ (For $\\frac{2}{3}$ times-S4 and S6).\n",
    "\n",
    "**Probability Matrix $P$**\n",
    "\n",
    "Using states $S_1 \\ldots S_5$ and actions $A_1, A_2, A_3$, consider each transition:\n",
    "\n",
    "| State Transition        | Action: Up ($A_1$) | Action: Right ($A_2$) | Action: Left ($A_3$) |\n",
    "|-------------------------|------------------------|-----------------------------|----------------------------|\n",
    "| **$S_1$ → $S_1$** | 1 (blocked)            | 0                            | 1 (blocked)                  |\n",
    "| **$S_1$ → $S_4$** | 0                      | 1                            | 0                            |\n",
    "| **$S_2$ → $S_2$** | 1 (blocked)            | 1 (blocked)                  | 0                            |\n",
    "| **$S_2$ → $S_4$** | 0                      | 0                            | 1                            |\n",
    "| **$S_3$ → $S_3$** | 1 (blocked)            | 0                            | 0                            |\n",
    "| **$S_3$ → $S_4$** | 0                      | 1                            | 1                            |\n",
    "| **$S_4$ → $S_3$** | 0                      | $\\frac{2}{3}$ (probabilistic)| $\\frac{2}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_2$** | 0                      | $\\frac{1}{3}$ (probabilistic)| 0                            |\n",
    "| **$S_4$ → $S_1$** | 0                      | 0                            | $\\frac{1}{3}$ (probabilistic)|\n",
    "| **$S_4$ → $S_5$** | 1 (terminal)           | 0                            | 0                            |\n",
    "  \n",
    "Above:\n",
    "- **Blocked transitions** (e.g., \"Up\" when no upward movement is possible) have a **probability of 1** for staying in the same state.\n",
    "- Probabilistic transitions from $S_4$ to $S_1$, $S_2$, or $S_3$ when moving left or right.\n",
    "- Moving **up** from $S_4$ always leads to the terminal state $S_5$ with probability **1**.\n",
    "\n",
    "**Reward Function $R$**\n",
    "\n",
    "- If the robot takes the **Up** action in a sensor state where **Up is allowed** (i.e., there's no wall above), in $\\frac{2}{3}$ times it receives **+1** (entering green cells), and in $\\frac{1}{3}$ times, it receives **-1** (red cell).\n",
    "- For all other valid moves (where no immediate green state is reached), the robot receives **0 reward**.\n",
    "- If the robot tries an invalid action (e.g., moving into a wall), it stays in the same state and still receives a reward of **0**.\n",
    "  \n",
    "So, the reward function can be expressed as:\n",
    "\n",
    "$$R(s' | s, a) = \n",
    "     \\begin{cases}\n",
    "       +1 & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}, and a is \\text{Up}, with probability of } \\frac{2}{3} \\\\\n",
    "       -1 & \\text{If s = \\{right: 1, up: 1, left: 1, down: 0\\}, and a is \\text{Up}, with probability of } \\frac{1}{3} \\\\\n",
    "       0 & \\text{Otherwise}\n",
    "     \\end{cases}$$\n",
    "\n",
    "This MDP can indeed be efficiently solved using **dynamic programming** methods or value iteration once set up this way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "\n",
    "In scenario 3, implement **MDP** in the Python environment and determine the optimal policy using **value iteration** and **policy iteration**. If the obtained optimal policy is applied to the robot, what decision will it make in each part of the grid environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from hands_on.src.value_iteration import ValueIteration\n",
    "from hands_on.src.tabular_value_function import TabularValueFunction\n",
    "from hands_on.src.value_policy import ValuePolicy\n",
    "\n",
    "from hands_on.src.policy_iteration import PolicyIteration\n",
    "from hands_on.src.tabular_policy import TabularPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, transition_func, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_func = transition_func\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transition_func[state][action]\n",
    "    \n",
    "    def get_reward(self, state, action, new_state):\n",
    "        if state == \"S4\" and action == \"Up\":\n",
    "            return random.choices([1, -1], weights=[2, 1], k=1)[0]\n",
    "        return 0\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDP components\n",
    "states = ['S1', 'S2', 'S3', 'S4', 'S5']  # S5 is the terminal state\n",
    "actions = ['Up', 'Right', 'Left']\n",
    "discount_factor = 0.9   # Lambda = 0.9\n",
    "\n",
    "# Set the transition probability matrix P(s' | s, a)\n",
    "# Transitions represented as P[s_current][a] = list of possible (s_next, prob).\n",
    "transition_func = {\n",
    "    'S1': {'Up': [('S1', 1)], 'Right': [('S4', 1)], 'Left': [('S1', 1)]},       # S1 -> right leads to S4\n",
    "    'S2': {'Up': [('S2', 1)], 'Right': [('S2', 1)], 'Left': [('S4', 1)]},       # S2 -> left leads to S4\n",
    "    'S3': {'Up': [('S3', 1)], 'Right': [('S4', 1)], 'Left': [('S4', 1)]},       # S3 -> left/right leads to S4\n",
    "    'S4': {'Up': [('S5', 1)], 'Right': [('S2', 0.33), ('S3', 0.67)],            # S4 -> Up to terminal\n",
    "                              'Left': [('S1', 0.33), ('S3', 0.67)]},\n",
    "    'S5': {'Up': [('S5', 1)], 'Right': [('S5', 1)], 'Left': [('S5', 1)]}        # S5 is terminal (absorbing)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(states, actions, transition_func, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9, 0.9, 0.9, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "vi_values = TabularValueFunction()\n",
    "\n",
    "max_iter = 1000\n",
    "\n",
    "ValueIteration(mdp, vi_values).value_iteration(max_iterations=max_iter)\n",
    "print(vi_values.get_values(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_policy = ValuePolicy(mdp, vi_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: Right\n",
      "S2: Left\n",
      "S3: Right\n",
      "S4: Up\n",
      "S5: Right\n"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    print(f\"{state}: {vi_policy.select_action(state, actions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pics/P3S3ia.png](pics/P3S3ia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_policy = TabularPolicy(default_action=\"Left\")\n",
    "\n",
    "pi_max_iter = 1000\n",
    "\n",
    "PolicyIteration(mdp, pi_policy).policy_iteration(max_iterations=pi_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: Right\n",
      "S2: Left\n",
      "S3: Right\n",
      "S4: Up\n",
      "S5: Right\n"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    print(f\"{state}: {dict(pi_policy.policy_table)[state]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pics/P3S3ia.png](pics/P3S3ia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "\n",
    "Consider scenario 3 in a situation where the robot is aware of its location on the map. Once again, determine the optimal policy using the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, reward_func, transition_func, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.reward_func = reward_func\n",
    "        self.transition_func = transition_func\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transition_func[state][action]\n",
    "    \n",
    "    def get_reward(self, state, action, new_state):\n",
    "        return self.reward_func[state][action][new_state]\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDP components\n",
    "states = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'T1', 'T3', 'T5'] \n",
    "actions = ['Up', 'Right', 'Left']\n",
    "discount_factor = 0.9   # Lambda = 0.9\n",
    "\n",
    "# Reward function R(s, a) -> `terminal (+1/-1)` and `0` elsewhere (unused)\n",
    "reward_func = {\n",
    "    'S1': {'Up': {'S1': 0}, 'Right': {'S2': 0}, 'Left': {'S1': 0}},       \n",
    "    'S2': {'Up': {'T1': +1}, 'Right': {'S3': 0}, 'Left': {'S1': 0}},       \n",
    "    'S3': {'Up': {'S3': 0}, 'Right': {'S4': 0}, 'Left': {'S2': 0}},       \n",
    "    'S4': {'Up': {'T3': -1}, 'Right': {'S5': 0}, 'Left': {'S3': 0}},\n",
    "    'S5': {'Up': {'S5': 0}, 'Right': {'S6': 0}, 'Left': {'S4': 0}},        \n",
    "    'S6': {'Up': {'T5': +1}, 'Right': {'S7': 0}, 'Left': {'S5': 0}},\n",
    "    'S7': {'Up': {'S7': 0}, 'Right': {'S7': 0}, 'Left': {'S6': 0}},\n",
    "    'T1': {'Up': {'T1': 0}, 'Right': {'T1': 0}, 'Left': {'T1': 0}},\n",
    "    'T3': {'Up': {'T3': 0}, 'Right': {'T3': 0}, 'Left': {'T3': 0}},\n",
    "    'T5': {'Up': {'T5': 0}, 'Right': {'T5': 0}, 'Left': {'T5': 0}}\n",
    "}\n",
    "\n",
    "# Set the transition probability matrix P(s' | s, a)\n",
    "# Transitions represented as P[s_current][a] = list of possible (s_next, prob).\n",
    "transition_func = {\n",
    "    'S1': {'Up': [('S1', 1)], 'Right': [('S2', 1)], 'Left': [('S1', 1)]},       \n",
    "    'S2': {'Up': [('T1', 1)], 'Right': [('S3', 1)], 'Left': [('S1', 1)]},       \n",
    "    'S3': {'Up': [('S3', 1)], 'Right': [('S4', 1)], 'Left': [('S2', 1)]},       \n",
    "    'S4': {'Up': [('T3', 1)], 'Right': [('S5', 1)], 'Left': [('S3', 1)]},\n",
    "    'S5': {'Up': [('S5', 1)], 'Right': [('S6', 1)], 'Left': [('S4', 1)]},        \n",
    "    'S6': {'Up': [('T5', 1)], 'Right': [('S7', 1)], 'Left': [('S5', 1)]},\n",
    "    'S7': {'Up': [('S7', 1)], 'Right': [('S7', 1)], 'Left': [('S6', 1)]},\n",
    "    'T1': {'Up': [('T1', 1)], 'Right': [('T1', 1)], 'Left': [('T1', 1)]},\n",
    "    'T3': {'Up': [('T3', 1)], 'Right': [('T3', 1)], 'Left': [('T3', 1)]},\n",
    "    'T5': {'Up': [('T5', 1)], 'Right': [('T5', 1)], 'Left': [('T5', 1)]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(states, actions, reward_func, transition_func, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9, 1.0, 0.9, 0.81, 0.9, 1.0, 0.9, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "vi_values = TabularValueFunction()\n",
    "\n",
    "max_iter = 1000\n",
    "\n",
    "ValueIteration(mdp, vi_values).value_iteration(max_iterations=max_iter)\n",
    "print(vi_values.get_values(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_policy = ValuePolicy(mdp, vi_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: Right\n",
      "S2: Up\n",
      "S3: Left\n",
      "S4: Right\n",
      "S5: Right\n",
      "S6: Up\n",
      "S7: Left\n",
      "T1: Right\n",
      "T3: Right\n",
      "T5: Left\n"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    print(f\"{state}: {vi_policy.select_action(state, actions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pics/P3S3ib.png](pics/P3S3ib.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_policy = TabularPolicy(default_action=\"Up\")\n",
    "\"\"\"\n",
    "# The following loop is equivalent to:\n",
    "PolicyIteration(gridworld, policy).policy_iteration(max_iterations=100)\n",
    "\"\"\"\n",
    "max_iter = 1000\n",
    "\n",
    "PolicyIteration(mdp, pi_policy).policy_iteration(max_iterations=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: Right\n",
      "S2: Up\n",
      "S3: Left\n",
      "S4: Left\n",
      "S5: Right\n",
      "S6: Up\n",
      "S7: Left\n",
      "T1: Right\n",
      "T3: Right\n",
      "T5: Right\n"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    print(f\"{state}: {dict(pi_policy.policy_table)[state]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pics/P3S3ib2.png](pics/P3S3ib2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "In this question, we examine the effect of episode length (Horizon) on the agent’s policy. Consider a robot that is tasked with managing stock shares. (Assume this problem can be represented as an MDP.)\n",
    "\n",
    "Let $ s $ represent the number of shares the robot currently has (an integer always between [0,10]). At each moment, the robot has two options: to sell (if possible, $ s $ decreases by one unit) or to buy (if possible, $ s $ increases by one unit).\n",
    "\n",
    "- If $ s > 0 $ and the agent sells, it receives a reward of +1 for the sale, and the stock level changes to $ s - 1 $. If $ s = 0 $, nothing happens.\n",
    "- If $ s < 9 $ and the agent buys, it receives no reward, and the stock level changes to $ s + 1 $.\n",
    "- The stock owner wants the inventory to be fully stocked at the end of the day; therefore, if the stock level reaches the maximum value of $ s = 10 $, the agent receives a reward of +100.\n",
    "- The state $ s = 10 $ is also a terminal state, and the problem ends if it is reached.\n",
    "\n",
    "The reward function, denoted by $ r(s, a, s') $, is summarized as follows:\n",
    "\n",
    "- $ r(s, \\text{sell}, s - 1) = 1 $ for $ s > 0 $\n",
    "- $ r(0, \\text{sell}, 0) = 0 $\n",
    "- $ r(s, \\text{buy}, s + 1) = 0 $ for $ s < 9 $\n",
    "- $ r(9, \\text{buy}, 10) = 100 $, indicating that moving from $ s = 9 $ to $ s = 10 $ gives a reward of +100, reaching the maximum stock level.\n",
    "\n",
    "It is assumed that the stock level always starts from $ s = 3 $ at the beginning of the day. We will examine how the agent’s optimal policy changes by setting a limited horizon $H$ for the problem. Recall that the horizon $ H $ refers to a limit on the number of time steps in which the agent can interact with the MDP before the episode ends, regardless of whether a terminal state has been reached. We will analyze the characteristics of the optimal policy (the policy that maximizes the episode’s reward) as the horizon $ H $ changes. (For the finite horizon, the discount factor is $ \\gamma = 1 $).\n",
    "\n",
    "![pics/P4.png](pics/P4.png)\n",
    "\n",
    "For example, assume $ H = 4 $. The agent can sell for three steps, moving from $ s = 3 $ to $ s = 2 $, then $ s = 1 $, and finally $ s = 0 $, receiving rewards of +1, +1, and +1 for each sell action. In the fourth step, the inventory is empty, so it can either sell or buy, but it will not receive any reward in either case. Then, the problem ends due to the time limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a \n",
    "Starting from the initial state $ s = 3 $, is it possible to choose a value of $ H $ such that the optimal policy includes both buying and selling steps during the execution? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the optimal policy can include both buying and selling steps during the execution for specific values of $H$. For instance, for $H = 5$ or $H = 6$, the agent maximizes its reward by selling first, then buying to increase stock, and then selling again. \n",
    "\n",
    "#### Assumptions:\n",
    "1. **Discount Factor $\\gamma = 1$**: Since this is a **finite horizon MDP**, there's no discounting across time steps, meaning every reward is equally important, regardless of when it occurs during the episode. Thus, the agent prioritizes maximizing the cumulative reward across all steps of the episode.\n",
    "   \n",
    "2. **Markov Decision Process (MDP) Assumption**: In MDPs, the decisions made in any given state depend only on the current state and not on the sequence of actions that led to that state. Therefore, any strategy like trying to strategize on past actions to inform future decisions (e.g., selling first then buying later to reach the S10 at the end of the episode) is not standard MDP behavior.\n",
    "\n",
    "#### Setting $H$:\n",
    "Given the structure of the problem, especially starting from $s = 3$, we need to evaluate whether a mix of buy and sell actions can be optimal within a specific horizon. Selling gives immediate rewards, while buying leads towards the terminal state $s = 10$ for a large delayed reward of +100.\n",
    "\n",
    "The total reward maximization consists of:\n",
    "- **Getting Immediate Rewards** by selling when $s > 0$.\n",
    "- **Getting the Large Delayed Reward** by eventually reaching $s = 10$, but only if it can be done within the horizon time limit.\n",
    "\n",
    "Let’s explore $H$ = 5 and 6, and how the agent’s optimal policy will behave.\n",
    "\n",
    "#### Horizon $H = 5$\n",
    "- The agent can sell for 3 steps:\n",
    "    - $s = 3$ -> $s = 2$ \\[+1 reward\\].\n",
    "    - $s = 2$ -> $s = 1$ \\[+1 reward\\].\n",
    "    - $s = 1$ -> $s = 0$ \\[+1 reward\\].\n",
    "- At this point, 2 time steps are remaining. The agent can:\n",
    "    - **Buy** at $s = 0$, gaining no immediate reward but transitioning to $s = 1$.\n",
    "    - **Sell** back down to $s = 0$, gaining +1 reward.\n",
    "\n",
    "- **Total reward**: 1 (three sells) + 1 (one sell after a buy).\n",
    "  \n",
    "Here, we see the agent engaging in a sell-buy-sell strategy. The optimal policy now includes at least **one buy** action amidst the sell actions, as it is slightly more beneficial to buy and gain a future sell reward than doing nothing in the last two steps.\n",
    "\n",
    "#### Horizon $H = 6$\n",
    "- The agent can sell for 3 steps:\n",
    "    - $s = 3$ -> $s = 2$ \\[+1 reward\\].\n",
    "    - $s = 2$ -> $s = 1$ \\[+1 reward\\].\n",
    "    - $s = 1$ -> $s = 0$ \\[+1 reward\\].\n",
    "- At this point, the agent has 3 time steps remaining. It can:\n",
    "    - **Buy** at $s = 0$, gaining no immediate reward (moving to $s = 1$).\n",
    "    - **Sell** at $s = 1$, gaining +1 reward (moving to $s = 0$).\n",
    "    - **Buy** at $s = 0$, gaining no immediate reward (moving to $s = 1$).\n",
    "- **Total reward**: 4 (three sells plus one additional sell).\n",
    "  \n",
    "Here, the agent is alternating buy and sell actions in the remaining steps. The agent executes a **sell-buy-sell-buy** strategy by alternating between buying and selling once hitting $s = 0$, and this provides a mix of actions.\n",
    "\n",
    "Thus, for these $H$ values, the optimal policy involves buying and selling during the same episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "Starting from the initial state $ s = 3 $, for what values of $ H $ does the optimal policy lead to a fully stocked inventory? In other words, provide a range for $ H $.\n",
    "\n",
    "*Note 1:* We consider the inventory fully stocked when the buy action is chosen in state $ s = 9 $, causing a transition to $ s = 10 $. This includes the last time step in the horizon as well.\n",
    "\n",
    "*Note 2:* By performing only buy actions, the agent can reach $ s = 10 $ from $ s = 3 $ in $ H = 7 $ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of horizons $H$ for which the agent reaches a fully stocked inventory (i.e., reaches $s = 10$ by buying at $s = 9$) is:\n",
    "    \n",
    "$$7 \\leq H \\leq 3 + 98 \\times 2 - 1 = 198$$\n",
    "\n",
    "#### For Small Horizions $1 \\leq H \\leq 6$:\n",
    "\n",
    "- **For $H = 1,2,3$**:\n",
    "    - It's **impossible** to reach a fully stocked inventory (i.e., $s = 10$) because the agent can only take a limited number of actions starting from $s = 3$. Even if it bought shares at every step, reaching $s = 10$ from $s = 3$ requires 7 time steps (as mentioned in Note 2).\n",
    "    - The **optimal policy** for these short horizons will be to sell as much as possible to collect small, immediate rewards:\n",
    "        - $H = 1$: Sell once from $s = 3$ to $s = 2$ for +1 reward.\n",
    "        - $H = 2$: Sell twice, from $s = 3$ to $s = 1$ for +2 rewards.\n",
    "        - $H = 3$: Sell three times, from $s = 3$ to $s = 0$ for +3 rewards.\n",
    "    - **Total reward**: In each case, the agent maximizes the immediate small reward by selling.\n",
    "\n",
    "- **For $H = 4$**:\n",
    "    - The agent can sell for the first 3 steps, reaching $s = 0$. The last action can be either selling (achieving nothing since inventory is empty) or buying (achieving nothing since no reward is given for buying below $s = 9$). Hence, no fully stocked inventory is achieved. $s = 10$ is unreachable.\n",
    "    - **Total reward**: +3 from selling.\n",
    "    \n",
    "- **For $H = 5, 6$**:\n",
    "    - As discussed in the previous part, for $H = 5$ and $H = 6$, the optimal policy includes both **buying and selling** steps. The agent sells multiple times to collect immediate rewards and then alternates between buying and selling when $s = 0$ to gather a few extra rewards. However, the agent **does not attempt to fully stock the inventory** (i.e., reach $s = 10$) as buying all the way to $s = 10$ is suboptimal in this case.\n",
    "    - **No fully stocked inventory** in these horizons.\n",
    "\n",
    "#### For Horizon $H = 7$:\n",
    "- **For $H = 7$**, the agent can increase the stock from $s = 3$ to $s = 10$ exclusively by performing **buy actions**, as mentioned in the problem. This is the minimal number of steps the agent needs to reach the fully stocked state $s = 10$ from $s = 3$.\n",
    "    - After buying 7 times in succession:\n",
    "        - $s = 3$ -> $s = 4$ -> $s = 5$ -> $s = 6$ -> $s = 7$ -> $s = 8$ -> $s = 9$ -> $s = 10$.\n",
    "    - The agent receives the large **+100 reward** upon reaching $s = 10$.\n",
    "    - **Total reward**: +100 from reaching $s = 10$.\n",
    "\n",
    "#### For Larger Horizons $198 \\geq H \\geq 8$:\n",
    "\n",
    "- For $H = 8$ (and above):\n",
    "    - The agent can still fully stock the inventory and receive the **+100 reward** by performing the same 7 buy actions mentioned for $H = 7$. Afterward, since the agent reaches the terminal state at $s = 10$, any remaining time steps are irrelevant because the episode terminates.\n",
    "    - **Total reward** remains +100, as reaching $s = 10$ leads to episode termination.\n",
    "\n",
    "#### For Very Large Horizons $H \\geq 199$:\n",
    "- For extremely large horizons or more specifically, $H \\geq 3 + 98 \\ast 2$, the agent collects more total rewards by **selling first** and alternating between buys and sells rather than filling the inventory:\n",
    "    - **Optimal policy**: The agent will sell for the first 3 steps (hence reaching $s = 0$). Then, it will alternate between buying and selling repeatedly to maximize the small, cumulative rewards of +1 for each sell:\n",
    "        - Sell to collect +3 from states $s = 3 \\to 2 \\to 1 \\to 0$.\n",
    "        - Then, perform a sequence of **sell-buy** alternations every 2 time steps, collecting an additional +1 reward for each sell.\n",
    "    - **Total reward**: At least $3 + 98 = 101$, exceeding the +100 reward from fully stocking the inventory. Thus, for such high $H$, it is better **not** to fill the inventory and instead maximize rewards from alternating actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c\n",
    "Now, consider the infinite-horizon setting with a discount factor $ \\gamma $. In other words, there is no time limit, and the problem only ends if a terminal state is reached. Suppose $ \\gamma = 0 $; what action does the optimal policy take when $ s = 3 $? What action does the optimal policy take when $ s = 9 $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\gamma = 0$, the agent only cares about rewards in the current step and ignores all possible future outcomes. So, **at $s = 3$**, the best action is to **sell** a share because this provides an immediate reward of +1. additionally, **at $s = 9$** the best action is to **buy** a share to reach $s = 10$, which gives the large immediate reward of +100.\n",
    "\n",
    "#### What does $\\gamma$ = 0 mean?\n",
    "\n",
    "The discount factor $\\gamma$ tells us how the agent values **future rewards**. If $\\gamma = 0$, the agent **does not care about future rewards at all**; it only cares about the **immediate rewards**. This means the agent will choose actions that maximize the current step's reward without considering what might happen next.\n",
    "\n",
    "In this scenario, with $\\gamma = 0$, **only the immediate reward matters** for the agent at every step. The agent isn't concerned about long-term outcomes, such as reaching the fully stocked inventory $s = 10$ and receiving the +100 reward at the end. Instead, it focuses solely on the action that gives the highest reward in the current step.\n",
    "\n",
    "#### What happens at $s = 3$?\n",
    "At $s = 3$, the agent has two options:\n",
    "1. **Sell**: Selling one share will give the agent an immediate reward of **+1** and move it to state $s = 2$.\n",
    "2. **Buy**: Buying one share will give the agent **0 immediate reward** and move it to state $s = 4$.\n",
    "\n",
    "Since $\\gamma = 0$, the agent only cares about **immediate reward**, not future rewards. Therefore, the agent will choose the action that gives the best **immediate** return. **Selling** at $s = 3$ provides an immediate reward of **+1**, while buying provides **0 immediate reward**.\n",
    "\n",
    "Thus, **the optimal action at $s = 3$** is to **sell** a share in order to get the immediate reward of +1.\n",
    "\n",
    "#### What happens at $s = 9$?\n",
    "Similarly, at $s = 9$, the agent has two options:\n",
    "1. **Sell**: Selling one share will give the agent an immediate reward of **+1** and move it to state $s = 8$.\n",
    "2. **Buy**: Buying one share will move the agent to state $s = 10$, which is the terminal state, and will immediately give a reward of **+100**.\n",
    "\n",
    "Since $\\gamma = 0$ means the agent only cares about **immediate reward**, the choice here is clear: **Buying** in $s = 9$ leads to an immediate reward of **+100**, which is much higher than the alternative of **+1** received from selling.\n",
    "\n",
    "Thus, **the optimal action at $s = 9$** is to **buy** the last share, transitioning to $s = 10$ and earning the large immediate reward of +100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d\n",
    "In the infinite-horizon setting with a discount factor $ \\gamma $, is it possible to choose a constant $ \\gamma \\in (0, 1] $ such that the optimal policy, starting from $ s = 3 $, never fully stocks the inventory? If so, find a range of $ \\gamma $ that meets this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider two policies:\n",
    "\n",
    "1. **Policy for filling the inventory (buying until $s = 10$)**:\n",
    "    - Starting from $s = 3$, the agent buys until it reaches $s = 10$ to get the large reward of +100.\n",
    "    - The cumulative discounted reward for reaching $s = 10$ is:\n",
    "      $$G_{\\text{stock}} = 100 \\cdot \\gamma^6$$\n",
    "    - This is because the reward of +100 is received after 7 steps of buying (zero rewards along the way), so the total gain is discounted by $\\gamma^6$.\n",
    "\n",
    "2. **Policy for not filling the inventory (selling first, then alternating between buying and selling)**:\n",
    "    - In this case, the agent first sells down to $s = 0$, collects small rewards of +1, and then alternates between buying and selling to collect additional rewards.\n",
    "    - The cumulative discounted reward for selling and not restocking the inventory is:\n",
    "      $$G_{\\text{no\\_stock}} = 1 + \\gamma + \\gamma^2 + 0 + \\gamma^4 + 0 + \\gamma^6 + \\cdots$$\n",
    "    - The important thing to notice here is that rewards are collected every other step, and this forms a geometric series. Therefore, using the summation formula for a geometric series, the total reward from selling and alternating is:\n",
    "      $$G_{\\text{no\\_stock}} = \\gamma + \\frac{1}{1 - \\gamma^2}$$\n",
    "\n",
    "#### Condition for avoiding fully stocking the inventory:\n",
    "We want to find the values of $\\gamma$ such that the reward from not stocking the inventory is larger than the reward from fully stocking the inventory:\n",
    "$$G_{\\text{no\\_stock}} > G_{\\text{stock}}$$\n",
    "This results in the inequality:\n",
    "$$\\gamma + \\frac{1}{1 - \\gamma^2} > 100 \\cdot \\gamma^6$$\n",
    "Multiply both sides by $(1 - \\gamma^2)$ to eliminate the denominator:\n",
    "$$(\\gamma - \\gamma^3) + 1 > 100 \\cdot \\gamma^6 \\cdot (1 - \\gamma^2)$$\n",
    "Next, expand the terms:\n",
    "$$1 - \\gamma^3 + \\gamma > 100 \\cdot \\gamma^6 - 100 \\cdot \\gamma^8$$\n",
    "Rearrange the terms to move everything to one side:\n",
    "$$100 \\cdot \\gamma^8 - 100 \\cdot \\gamma^6 - \\gamma^3 + \\gamma + 1 > 0$$\n",
    "This is the expression we need to solve.\n",
    "\n",
    "#### Solving the inequality:\n",
    "\n",
    "![pics/P4d.png](pics/P4d.png)\n",
    "\n",
    "The graph shows the function:\n",
    "$$f(x) = 100 \\cdot x^8 - 100 \\cdot x^6 - x^3 + x + 1$$\n",
    "By plotting the graph (as shown in the image), we find that the root of the inequality is approximately:\n",
    "$$\\gamma \\approx 0.51554$$\n",
    "Thus, the range of $\\gamma$ such that the optimal policy **never fully stocks the inventory** is:\n",
    "$$\\gamma \\in (0, 0.51554)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, reward_func, transition_func, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.reward_func = reward_func\n",
    "        self.transition_func = transition_func\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transition_func[state][action]\n",
    "    \n",
    "    def get_reward(self, state, action, new_state):\n",
    "        return self.reward_func[state][action][new_state]\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['S0', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10']\n",
    "actions = ['buy', 'sell']\n",
    "\n",
    "# Reward function R(s, a) -> `terminal (+1/-1)` and `0` elsewhere\n",
    "reward_func = {\n",
    "    'S0': {'buy': {'S1': 0}, 'sell': {'S0': 0}},\n",
    "    'S1': {'buy': {'S2': 0}, 'sell': {'S0': 1}},\n",
    "    'S2': {'buy': {'S3': 0}, 'sell': {'S1': 1}},\n",
    "    'S3': {'buy': {'S4': 0}, 'sell': {'S2': 1}},\n",
    "    'S4': {'buy': {'S5': 0}, 'sell': {'S3': 1}},\n",
    "    'S5': {'buy': {'S6': 0}, 'sell': {'S4': 1}},\n",
    "    'S6': {'buy': {'S7': 0}, 'sell': {'S5': 1}},\n",
    "    'S7': {'buy': {'S8': 0}, 'sell': {'S6': 1}},\n",
    "    'S8': {'buy': {'S9': 0}, 'sell': {'S7': 1}},\n",
    "    'S9': {'buy': {'S10': 100}, 'sell': {'S8': 1}},\n",
    "    'S10': {'buy': {'S10': 0}, 'sell': {'S10': 0}}\n",
    "}\n",
    "\n",
    "# Set the transition probability matrix P(s' | s, a)\n",
    "# Transitions represented as P[s_current][a] = list of possible (s_next, prob).\n",
    "transition_func = {\n",
    "    'S0': {'buy': [('S1', 1)], 'sell': [('S0', 1)]},\n",
    "    'S1': {'buy': [('S2', 1)], 'sell': [('S0', 1)]},\n",
    "    'S2': {'buy': [('S3', 1)], 'sell': [('S1', 1)]},\n",
    "    'S3': {'buy': [('S4', 1)], 'sell': [('S2', 1)]},\n",
    "    'S4': {'buy': [('S5', 1)], 'sell': [('S3', 1)]},\n",
    "    'S5': {'buy': [('S6', 1)], 'sell': [('S4', 1)]},\n",
    "    'S6': {'buy': [('S7', 1)], 'sell': [('S5', 1)]},\n",
    "    'S7': {'buy': [('S8', 1)], 'sell': [('S6', 1)]},\n",
    "    'S8': {'buy': [('S9', 1)], 'sell': [('S7', 1)]},\n",
    "    'S9': {'buy': [('S10', 1)], 'sell': [('S8', 1)]},\n",
    "    'S10': {'buy': [('S10', 1)], 'sell': [('S10', 1)]}\n",
    "}\n",
    "\n",
    "# Starting state is S3\n",
    "start_state = 'S3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(states, actions, reward_func, transition_func, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_value_iteration(mdp, start_state, num_episodes, horizon):\n",
    "    # Initialize value function V(s) for all states (initially set to 0)\n",
    "    V = {state: 0.0 for state in mdp.get_states()}\n",
    "    \n",
    "    # Initialize returns list for all states to keep track of all values observed\n",
    "    returns = {state: [] for state in mdp.get_states()}\n",
    "    \n",
    "    # Monte Carlo simulation for a fixed number of episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Generate an episode starting from the start_state\n",
    "        state = start_state\n",
    "        episode_history = []  # To store the (state, action, reward) for each transition in episode\n",
    "        \n",
    "        # Simulate an episode of up to 'horizon'\n",
    "        for t in range(horizon):\n",
    "            # Randomly select an action -- assuming exploring start (radom policy)\n",
    "            action = random.choice(mdp.get_actions(state))\n",
    "            \n",
    "            # Get transitions and rewards for that action in the current state\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            new_state, prob = transitions[0]  # Assuming deterministic with prob = 1 for our model\n",
    "            \n",
    "            reward = mdp.get_reward(state, action, new_state)\n",
    "            \n",
    "            # Append this step to the episode history\n",
    "            episode_history.append((state, action, reward))\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = new_state\n",
    "            \n",
    "            # If we encounter terminal state S10, break\n",
    "            if state == 'S10':\n",
    "                break\n",
    "\n",
    "        first_visited_states = set()  # Track visited states in this episode\n",
    "\n",
    "        for (state, action, reward) in episode_history:\n",
    "            # Skip if we have already visited this state in the episode\n",
    "            if state in first_visited_states:\n",
    "                continue\n",
    "            first_visited_states.add(state)\n",
    "        \n",
    "        # Now we have the entire episode, calculate the return and update values\n",
    "        G = 0  # Cumulative discounted reward\n",
    "        for step in reversed(episode_history):\n",
    "            state, action, reward = step\n",
    "            G = reward + mdp.get_discount_factor() * G  # Update G with rewards and discount\n",
    "\n",
    "            if state not in first_visited_states:  # First-visit condition\n",
    "                returns[state].append(G)  # Keep track of returns for this state\n",
    "\n",
    "                # Update value function by averaging all returns for the state\n",
    "                V[state] = np.mean(returns[state])\n",
    "    \n",
    "    return V\n",
    "\n",
    "def compute_optimal_policy(mdp, value_function):\n",
    "    policy = {}\n",
    "    \n",
    "    # For each state, determine the best action based on the value function\n",
    "    for state in mdp.get_states():\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        # Iterate over all possible actions in the current state\n",
    "        for action in mdp.get_actions(state):\n",
    "            action_value = 0  # Expected value of taking this action\n",
    "            \n",
    "            # Loop over possible transitions for action in this state\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            for new_state, prob in transitions:\n",
    "                reward = mdp.get_reward(state, action, new_state)\n",
    "                action_value += prob * (reward + mdp.get_discount_factor() * value_function[new_state])\n",
    "            \n",
    "            # Track the action that gives the highest expected return\n",
    "            if action_value > best_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        \n",
    "        # Save the best action for this state\n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function V(s) after Monte Carlo iterations:\n",
      "V(S0) = 0.00\n",
      "V(S1) = 0.00\n",
      "V(S2) = 0.00\n",
      "V(S3) = 0.00\n",
      "V(S4) = 0.00\n",
      "V(S5) = 0.00\n",
      "V(S6) = 0.00\n",
      "V(S7) = 0.00\n",
      "V(S8) = 0.00\n",
      "V(S9) = 0.00\n",
      "V(S10) = 0.00\n",
      "Optimal Policy for each state:\n",
      "State S0: Take action 'buy'\n",
      "State S1: Take action 'sell'\n",
      "State S2: Take action 'sell'\n",
      "State S3: Take action 'sell'\n",
      "State S4: Take action 'sell'\n",
      "State S5: Take action 'sell'\n",
      "State S6: Take action 'sell'\n",
      "State S7: Take action 'sell'\n",
      "State S8: Take action 'sell'\n",
      "State S9: Take action 'buy'\n",
      "State S10: Take action 'buy'\n"
     ]
    }
   ],
   "source": [
    "# Horizon (number of steps per episode simulation)\n",
    "horizon = 7\n",
    "\n",
    "# Number of episodes to simulate\n",
    "num_episodes = 5000\n",
    "\n",
    "# Starting state is S3\n",
    "start_state = 'S3'\n",
    "\n",
    "# Run Monte Carlo Value Iteration\n",
    "values = monte_carlo_value_iteration(mdp, start_state, num_episodes, horizon)\n",
    "\n",
    "# Print Value function\n",
    "print(\"Estimated Value Function V(s) after Monte Carlo iterations:\")\n",
    "for state, value in values.items():\n",
    "    print(f\"V({state}) = {value:.2f}\")\n",
    "\n",
    "# Compute optimal policy based on value function\n",
    "optimal_policy = compute_optimal_policy(mdp, values)\n",
    "\n",
    "# Print the optimal policy for each state\n",
    "print(\"Optimal Policy for each state:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State {state}: Take action '{action}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_evaluation(mdp, policy, num_episodes, start_state, horizon, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Runs Monte Carlo policy evaluation for a fixed policy and returns the updated value function.\n",
    "    \"\"\"\n",
    "    V = {state: 0.0 for state in mdp.get_states()}  # Initialize value function\n",
    "    returns = {state: [] for state in mdp.get_states()}  # Track the returns seen\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = random.choice(mdp.get_states())  # Random starting state (could be set as needed)\n",
    "        # state = start_state  # Fixed starting state for all episodes\n",
    "        \n",
    "        episode_history = []  # To store the (state, action, reward) for each transition in episode\n",
    "\n",
    "        # Simulate an episode according to the current policy\n",
    "        for t in range(horizon):\n",
    "            # action = policy[state]  # Always follow the given policy\n",
    "            # action = random.choice(mdp.get_actions(state))\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                # Exploration: Choose a random action\n",
    "                action = random.choice(mdp.get_actions(state))\n",
    "            else:\n",
    "                # Exploitation: Follow the given policy\n",
    "                action = policy[state]\n",
    "            \n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            new_state, prob = transitions[0]  # Assuming deterministic transitions\n",
    "            \n",
    "            reward = mdp.get_reward(state, action, new_state)\n",
    "            episode_history.append((state, action, reward))  # Log the (state, action, reward)\n",
    "            \n",
    "            state = new_state\n",
    "            if state == 'S10':  # Terminal state\n",
    "                break\n",
    "        \n",
    "        # Calculate returns in a backward manner\n",
    "        G = 0  # Start with zero return\n",
    "        i = 0\n",
    "\n",
    "        # for step in reversed(episode_history):\n",
    "        for step in episode_history[::-1]:\n",
    "            state, action, reward = step\n",
    "            G = reward + mdp.get_discount_factor() * G  # Update return G\n",
    "\n",
    "            if state not in episode_history[::-1][i+1:]:  # First-visit condition\n",
    "                returns[state].append(G)  # Append the new return for this state\n",
    "                V[state] = np.mean(returns[state])  # Update the value function via returns averaging\n",
    "\n",
    "            i += 1\n",
    "    \n",
    "    return V\n",
    "\n",
    "def monte_carlo_policy_improvement(mdp, value_function):\n",
    "    \"\"\"\n",
    "    Performs policy improvement based on the given value function.\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    # Iterate over states to create the improved policy\n",
    "    for state in mdp.get_states():\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for action in mdp.get_actions(state):\n",
    "            action_value = 0\n",
    "            \n",
    "            # Calculate the expected value of taking this action in this state\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            for new_state, prob in transitions:\n",
    "                reward = mdp.get_reward(state, action, new_state)\n",
    "                action_value += prob * (reward + mdp.get_discount_factor() * value_function[new_state])\n",
    "            \n",
    "            if action_value > best_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        \n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def monte_carlo_policy_iteration(mdp, num_episodes, start_state, horizon):\n",
    "    \"\"\"\n",
    "    Monte Carlo Policy Iteration: iteratively performs policy evaluation and improvement.\n",
    "    \"\"\"\n",
    "    # Initialize the policy randomly -- we start with a random policy\n",
    "    policy = {state: random.choice(mdp.get_actions(state)) for state in mdp.get_states()}\n",
    "    \n",
    "    is_policy_stable = False\n",
    "    iteration = 0\n",
    "    \n",
    "    while not is_policy_stable:\n",
    "        iteration += 1\n",
    "        # print(f\"\\nPolicy Iteration {iteration}\")\n",
    "        \n",
    "        # Step 1: Policy Evaluation -> Estimate value function for the current policy\n",
    "        value_function = monte_carlo_policy_evaluation(mdp, policy, num_episodes, start_state, horizon)\n",
    "        # print(\"Value Function:\", value_function)\n",
    "        \n",
    "        # Step 2: Policy Improvement -> Improve the policy based on the current value function\n",
    "        new_policy = monte_carlo_policy_improvement(mdp, value_function)\n",
    "        # print(\"Policy:\", new_policy)\n",
    "\n",
    "        # Print final optimal policy and value function after convergence\n",
    "        # for state, action in policy.items():\n",
    "        #     print(f\"{state}: '{action}'\", end=', ')\n",
    "        # print()\n",
    "\n",
    "        # for state, value in value_function.items():\n",
    "        #     print(f\"{state} = {value:.2f}\", end=', ')\n",
    "        # print()\n",
    "\n",
    "        # Check if the policy has stabilized (i.e., no changes from the old policy)\n",
    "        if new_policy == policy:\n",
    "            is_policy_stable = True\n",
    "        else:\n",
    "            policy = new_policy  # Update the policy and continue\n",
    "    \n",
    "    return policy, value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m  \u001b[38;5;66;03m# Feel free to adjust this number for deeper training\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Perform Monte Carlo Policy Iteration\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m optimal_policy, optimal_value_function \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo_policy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print final optimal policy and value function after convergence\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOptimal Policy after Policy Iteration:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[117], line 96\u001b[0m, in \u001b[0;36mmonte_carlo_policy_iteration\u001b[1;34m(mdp, num_episodes, start_state, horizon)\u001b[0m\n\u001b[0;32m     92\u001b[0m iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# print(f\"\\nPolicy Iteration {iteration}\")\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Step 1: Policy Evaluation -> Estimate value function for the current policy\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m value_function \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo_policy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# print(\"Value Function:\", value_function)\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Step 2: Policy Improvement -> Improve the policy based on the current value function\u001b[39;00m\n\u001b[0;32m    100\u001b[0m new_policy \u001b[38;5;241m=\u001b[39m monte_carlo_policy_improvement(mdp, value_function)\n",
      "Cell \u001b[1;32mIn[117], line 47\u001b[0m, in \u001b[0;36mmonte_carlo_policy_evaluation\u001b[1;34m(mdp, policy, num_episodes, start_state, horizon, epsilon)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m episode_history[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:  \u001b[38;5;66;03m# First-visit condition\u001b[39;00m\n\u001b[0;32m     46\u001b[0m             returns[state]\u001b[38;5;241m.\u001b[39mappend(G)  \u001b[38;5;66;03m# Append the new return for this state\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m             V[state] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update the value function via returns averaging\u001b[39;00m\n\u001b[0;32m     49\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m V\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mjid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3465\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:188\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float16_result \u001b[38;5;129;01mand\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret)\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float16_result:\n\u001b[0;32m    190\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret \u001b[38;5;241m/\u001b[39m rcount)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Horizon (number of steps per episode simulation)\n",
    "horizon = 5\n",
    "\n",
    "# Number of episodes to evaluate the policy\n",
    "num_episodes = 500  # Feel free to adjust this number for deeper training\n",
    "\n",
    "# Perform Monte Carlo Policy Iteration\n",
    "optimal_policy, optimal_value_function = monte_carlo_policy_iteration(mdp, num_episodes, start_state, horizon)\n",
    "\n",
    "# Print final optimal policy and value function after convergence\n",
    "print(\"\\nOptimal Policy after Policy Iteration:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State {state}: Take action '{action}'\")\n",
    "\n",
    "print(\"\\nOptimal Value Function after Policy Iteration:\")\n",
    "for state, value in optimal_value_function.items():\n",
    "    print(f\"V({state}) = {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite Horizon Value Function and Policy:\n",
      "Time step 0:\n",
      "  S0: Value = 100.00, Best Action = buy\n",
      "  S1: Value = 100.00, Best Action = buy\n",
      "  S2: Value = 101.00, Best Action = buy\n",
      "  S3: Value = 101.00, Best Action = buy\n",
      "  S4: Value = 102.00, Best Action = buy\n",
      "  S5: Value = 102.00, Best Action = buy\n",
      "  S6: Value = 103.00, Best Action = buy\n",
      "  S7: Value = 103.00, Best Action = buy\n",
      "  S8: Value = 104.00, Best Action = buy\n",
      "  S9: Value = 104.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 1:\n",
      "  S0: Value = 4.00, Best Action = buy\n",
      "  S1: Value = 100.00, Best Action = buy\n",
      "  S2: Value = 100.00, Best Action = buy\n",
      "  S3: Value = 101.00, Best Action = buy\n",
      "  S4: Value = 101.00, Best Action = buy\n",
      "  S5: Value = 102.00, Best Action = buy\n",
      "  S6: Value = 102.00, Best Action = buy\n",
      "  S7: Value = 103.00, Best Action = buy\n",
      "  S8: Value = 103.00, Best Action = buy\n",
      "  S9: Value = 104.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 2:\n",
      "  S0: Value = 4.00, Best Action = buy\n",
      "  S1: Value = 4.00, Best Action = buy\n",
      "  S2: Value = 100.00, Best Action = buy\n",
      "  S3: Value = 100.00, Best Action = buy\n",
      "  S4: Value = 101.00, Best Action = buy\n",
      "  S5: Value = 101.00, Best Action = buy\n",
      "  S6: Value = 102.00, Best Action = buy\n",
      "  S7: Value = 102.00, Best Action = buy\n",
      "  S8: Value = 103.00, Best Action = buy\n",
      "  S9: Value = 103.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 3:\n",
      "  S0: Value = 3.00, Best Action = buy\n",
      "  S1: Value = 4.00, Best Action = buy\n",
      "  S2: Value = 4.00, Best Action = buy\n",
      "  S3: Value = 100.00, Best Action = buy\n",
      "  S4: Value = 100.00, Best Action = buy\n",
      "  S5: Value = 101.00, Best Action = buy\n",
      "  S6: Value = 101.00, Best Action = buy\n",
      "  S7: Value = 102.00, Best Action = buy\n",
      "  S8: Value = 102.00, Best Action = buy\n",
      "  S9: Value = 103.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 4:\n",
      "  S0: Value = 3.00, Best Action = buy\n",
      "  S1: Value = 3.00, Best Action = buy\n",
      "  S2: Value = 4.00, Best Action = buy\n",
      "  S3: Value = 4.00, Best Action = buy\n",
      "  S4: Value = 100.00, Best Action = buy\n",
      "  S5: Value = 100.00, Best Action = buy\n",
      "  S6: Value = 101.00, Best Action = buy\n",
      "  S7: Value = 101.00, Best Action = buy\n",
      "  S8: Value = 102.00, Best Action = buy\n",
      "  S9: Value = 102.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 5:\n",
      "  S0: Value = 2.00, Best Action = buy\n",
      "  S1: Value = 3.00, Best Action = buy\n",
      "  S2: Value = 3.00, Best Action = buy\n",
      "  S3: Value = 4.00, Best Action = buy\n",
      "  S4: Value = 4.00, Best Action = buy\n",
      "  S5: Value = 100.00, Best Action = buy\n",
      "  S6: Value = 100.00, Best Action = buy\n",
      "  S7: Value = 101.00, Best Action = buy\n",
      "  S8: Value = 101.00, Best Action = buy\n",
      "  S9: Value = 102.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 6:\n",
      "  S0: Value = 2.00, Best Action = buy\n",
      "  S1: Value = 2.00, Best Action = buy\n",
      "  S2: Value = 3.00, Best Action = buy\n",
      "  S3: Value = 3.00, Best Action = buy\n",
      "  S4: Value = 4.00, Best Action = sell\n",
      "  S5: Value = 4.00, Best Action = sell\n",
      "  S6: Value = 100.00, Best Action = buy\n",
      "  S7: Value = 100.00, Best Action = buy\n",
      "  S8: Value = 101.00, Best Action = buy\n",
      "  S9: Value = 101.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 7:\n",
      "  S0: Value = 1.00, Best Action = buy\n",
      "  S1: Value = 2.00, Best Action = buy\n",
      "  S2: Value = 2.00, Best Action = buy\n",
      "  S3: Value = 3.00, Best Action = sell\n",
      "  S4: Value = 3.00, Best Action = sell\n",
      "  S5: Value = 3.00, Best Action = sell\n",
      "  S6: Value = 3.00, Best Action = sell\n",
      "  S7: Value = 100.00, Best Action = buy\n",
      "  S8: Value = 100.00, Best Action = buy\n",
      "  S9: Value = 101.00, Best Action = sell\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 8:\n",
      "  S0: Value = 1.00, Best Action = buy\n",
      "  S1: Value = 1.00, Best Action = buy\n",
      "  S2: Value = 2.00, Best Action = sell\n",
      "  S3: Value = 2.00, Best Action = sell\n",
      "  S4: Value = 2.00, Best Action = sell\n",
      "  S5: Value = 2.00, Best Action = sell\n",
      "  S6: Value = 2.00, Best Action = sell\n",
      "  S7: Value = 2.00, Best Action = sell\n",
      "  S8: Value = 100.00, Best Action = buy\n",
      "  S9: Value = 100.00, Best Action = buy\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 9:\n",
      "  S0: Value = 0.00, Best Action = buy\n",
      "  S1: Value = 1.00, Best Action = sell\n",
      "  S2: Value = 1.00, Best Action = sell\n",
      "  S3: Value = 1.00, Best Action = sell\n",
      "  S4: Value = 1.00, Best Action = sell\n",
      "  S5: Value = 1.00, Best Action = sell\n",
      "  S6: Value = 1.00, Best Action = sell\n",
      "  S7: Value = 1.00, Best Action = sell\n",
      "  S8: Value = 1.00, Best Action = sell\n",
      "  S9: Value = 100.00, Best Action = buy\n",
      "  S10: Value = 0.00, Best Action = None\n",
      "------\n",
      "Time step 10:\n",
      "  S0: Value = 0.00, Best Action = -\n",
      "  S1: Value = 0.00, Best Action = -\n",
      "  S2: Value = 0.00, Best Action = -\n",
      "  S3: Value = 0.00, Best Action = -\n",
      "  S4: Value = 0.00, Best Action = -\n",
      "  S5: Value = 0.00, Best Action = -\n",
      "  S6: Value = 0.00, Best Action = -\n",
      "  S7: Value = 0.00, Best Action = -\n",
      "  S8: Value = 0.00, Best Action = -\n",
      "  S9: Value = 0.00, Best Action = -\n",
      "  S10: Value = 0.00, Best Action = -\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "def finite_horizon_value_iteration_with_policy(mdp, T):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.actions  # Same actions for all states\n",
    "    \n",
    "    # Initialize value functions for each time step t (V_t(s))\n",
    "    V = {t: {s: 0 for s in states} for t in range(T + 1)}  # V_T = 0 is terminal condition\n",
    "    policy = {t: {s: None for s in states} for t in range(T)}  # Initialize policy for each time t\n",
    "    \n",
    "    # Step backward from t = T-1 to t = 0\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        for state in states:\n",
    "            # Skip terminal state S10\n",
    "            if state == 'S10':\n",
    "                continue\n",
    "            \n",
    "            best_action = None\n",
    "            max_action_value = float('-inf')\n",
    "            \n",
    "            # Loop through all actions\n",
    "            for action in actions:\n",
    "                expected_value = 0\n",
    "                transitions = mdp.get_transitions(state, action)\n",
    "                \n",
    "                # Evaluate each transition based on next state's value function at time t+1\n",
    "                for next_state, prob in transitions:\n",
    "                    reward = mdp.get_reward(state, action, next_state)\n",
    "                    expected_value += prob * (reward + V[t + 1][next_state])\n",
    "                \n",
    "                # Find the best action for this state at time t\n",
    "                if expected_value > max_action_value:\n",
    "                    max_action_value = expected_value\n",
    "                    best_action = action\n",
    "            \n",
    "            # Update value function and policy\n",
    "            V[t][state] = max_action_value\n",
    "            policy[t][state] = best_action\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "# Finite horizon example with T = 5\n",
    "T = 10\n",
    "V_finite, policy_finite = finite_horizon_value_iteration_with_policy(mdp, T)\n",
    "\n",
    "print(\"Finite Horizon Value Function and Policy:\")\n",
    "for t in range(T + 1):\n",
    "    print(f\"Time step {t}:\")\n",
    "    for s in V_finite[t]:\n",
    "        action = policy_finite[t].get(s, '-') if t < T else '-'\n",
    "        print(f\"  {s}: Value = {V_finite[t][s]:.2f}, Best Action = {action}\")\n",
    "    print(\"------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
